{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"GenAI Bench \u00b6 Unified, accurate, and beautiful LLM Benchmarking What is GenAI Bench? \u00b6 Genai-bench is a powerful benchmark tool designed for comprehensive token-level performance evaluation of large language model (LLM) serving systems. It provides detailed insights into model serving performance, offering both a user-friendly CLI and a live UI for real-time progress monitoring. Live UI Dashboard \u00b6 GenAI Bench includes a real-time dashboard that provides live monitoring of your benchmarks: Key Features \u00b6 \ud83d\udee0\ufe0f CLI Tool : Validates user inputs and initiates benchmarks seamlessly. \ud83d\udcca Live UI Dashboard : Displays current progress, logs, and real-time metrics. \ud83d\udcdd Rich Logs : Automatically flushed to both terminal and file upon experiment completion. \ud83d\udcc8 Experiment Analyzer : Generates comprehensive Excel reports with pricing and raw metrics data, plus flexible plot configurations (default 2x4 grid) that visualize key performance metrics including throughput, latency (TTFT, E2E, TPOT), error rates, and RPS across different traffic scenarios and concurrency levels. Supports custom plot layouts and multi-line comparisons. Quick Start \u00b6 Get started with GenAI Bench in minutes: # Install from PyPI pip install genai-bench # Run your first benchmark genai-bench benchmark --help For detailed installation and usage instructions, see our Getting Started Guide . Supported Tasks \u00b6 GenAI Bench supports multiple benchmark types: Task Description Use Case text-to-text Benchmarks generating text output from text input Chat, QA text-to-embeddings Benchmarks generating embeddings from text input Semantic search image-text-to-text Benchmarks generating text from images and text prompts Visual question answering image-to-embeddings Benchmarks generating embeddings from images Image similarity Documentation Sections \u00b6 \ud83d\ude80 Getting Started \u00b6 Quick Start - Get up and running in minutes Installation - Detailed installation guide Configuration - Configure your environment \ud83d\udcd6 User Guide \u00b6 Overview - Understanding GenAI Bench concepts CLI Reference - Complete command-line interface guide Tasks & Benchmarks - Running different types of benchmarks Analysis - Understanding your results \ud83d\udca1 Examples \u00b6 Basic Benchmarks - Simple benchmark examples \ud83d\udd27 Development \u00b6 Contributing - How to contribute to GenAI Bench Architecture - Understanding the codebase API Reference - Developer documentation Community \u00b6 GitHub : sgl-project/genai-bench PyPI : genai-bench Issues : GitHub Issues License \u00b6 This project is licensed under the MIT License - see the LICENSE file for details.","title":"Home"},{"location":"#genai-bench","text":"Unified, accurate, and beautiful LLM Benchmarking","title":"GenAI Bench"},{"location":"#what-is-genai-bench","text":"Genai-bench is a powerful benchmark tool designed for comprehensive token-level performance evaluation of large language model (LLM) serving systems. It provides detailed insights into model serving performance, offering both a user-friendly CLI and a live UI for real-time progress monitoring.","title":"What is GenAI Bench?"},{"location":"#live-ui-dashboard","text":"GenAI Bench includes a real-time dashboard that provides live monitoring of your benchmarks:","title":"Live UI Dashboard"},{"location":"#key-features","text":"\ud83d\udee0\ufe0f CLI Tool : Validates user inputs and initiates benchmarks seamlessly. \ud83d\udcca Live UI Dashboard : Displays current progress, logs, and real-time metrics. \ud83d\udcdd Rich Logs : Automatically flushed to both terminal and file upon experiment completion. \ud83d\udcc8 Experiment Analyzer : Generates comprehensive Excel reports with pricing and raw metrics data, plus flexible plot configurations (default 2x4 grid) that visualize key performance metrics including throughput, latency (TTFT, E2E, TPOT), error rates, and RPS across different traffic scenarios and concurrency levels. Supports custom plot layouts and multi-line comparisons.","title":"Key Features"},{"location":"#quick-start","text":"Get started with GenAI Bench in minutes: # Install from PyPI pip install genai-bench # Run your first benchmark genai-bench benchmark --help For detailed installation and usage instructions, see our Getting Started Guide .","title":"Quick Start"},{"location":"#supported-tasks","text":"GenAI Bench supports multiple benchmark types: Task Description Use Case text-to-text Benchmarks generating text output from text input Chat, QA text-to-embeddings Benchmarks generating embeddings from text input Semantic search image-text-to-text Benchmarks generating text from images and text prompts Visual question answering image-to-embeddings Benchmarks generating embeddings from images Image similarity","title":"Supported Tasks"},{"location":"#documentation-sections","text":"","title":"Documentation Sections"},{"location":"#getting-started","text":"Quick Start - Get up and running in minutes Installation - Detailed installation guide Configuration - Configure your environment","title":"\ud83d\ude80 Getting Started"},{"location":"#user-guide","text":"Overview - Understanding GenAI Bench concepts CLI Reference - Complete command-line interface guide Tasks & Benchmarks - Running different types of benchmarks Analysis - Understanding your results","title":"\ud83d\udcd6 User Guide"},{"location":"#examples","text":"Basic Benchmarks - Simple benchmark examples","title":"\ud83d\udca1 Examples"},{"location":"#development","text":"Contributing - How to contribute to GenAI Bench Architecture - Understanding the codebase API Reference - Developer documentation","title":"\ud83d\udd27 Development"},{"location":"#community","text":"GitHub : sgl-project/genai-bench PyPI : genai-bench Issues : GitHub Issues","title":"Community"},{"location":"#license","text":"This project is licensed under the MIT License - see the LICENSE file for details.","title":"License"},{"location":"api/overview/","text":"API Overview \u00b6 This section provides comprehensive documentation for GenAI Bench's APIs, interfaces, and extension points. Whether you're integrating GenAI Bench into your workflow or extending its functionality, this documentation will help you understand the available interfaces. API Categories \u00b6 GenAI Bench provides several types of APIs: CLI API - Command-line interface for running benchmarks (see CLI Reference ) Python API - Programmatic interface for Python applications REST API - HTTP API for remote access (when using web UI) Extension APIs - Interfaces for extending functionality Core Concepts \u00b6 Benchmark Configuration \u00b6 All APIs work with a common configuration structure: from dataclasses import dataclass from typing import Optional , Dict , Any @dataclass class BenchmarkConfig : # API Configuration api_backend : str api_base : str api_key : str api_model_name : str # Task Configuration task : str dataset_name : str # Load Configuration num_users : int max_time_per_run : int max_requests_per_run : Optional [ int ] = None spawn_rate : Optional [ float ] = None # Output Configuration output_dir : str = \"./results\" ui : bool = False # Advanced Configuration additional_sampling_params : Optional [ Dict [ str , Any ]] = None server_info : Optional [ Dict [ str , Any ]] = None Task Types \u00b6 GenAI Bench supports several task types: from enum import Enum class TaskType ( Enum ): TEXT_TO_TEXT = \"text-to-text\" TEXT_TO_EMBEDDINGS = \"text-to-embeddings\" IMAGE_TEXT_TO_TEXT = \"image-text-to-text\" IMAGE_TO_EMBEDDINGS = \"image-to-embeddings\" Metrics Structure \u00b6 All APIs return metrics in a consistent format: @dataclass class BenchmarkMetrics : # Request-level metrics request_metrics : List [ RequestMetric ] # Aggregated metrics aggregated_metrics : AggregatedMetrics # System metrics system_metrics : SystemMetrics # Metadata benchmark_info : BenchmarkInfo Quick Start Examples \u00b6 Python API \u00b6 from genai_bench import Benchmark # Create benchmark configuration config = BenchmarkConfig ( api_backend = \"openai\" , api_base = \"http://localhost:8082\" , api_key = \"your-api-key\" , api_model_name = \"llama-2-7b\" , task = \"text-to-text\" , dataset_name = \"sonnet.txt\" , num_users = 2 , max_time_per_run = 60 ) # Run benchmark benchmark = Benchmark ( config ) results = benchmark . run () # Access metrics print ( f \"Average latency: { results . aggregated_metrics . avg_latency } \" ) print ( f \"Throughput: { results . aggregated_metrics . throughput } \" ) CLI API \u00b6 # Basic benchmark genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --dataset-name \"sonnet.txt\" \\ --num-users 2 \\ --max-time-per-run 60 # Export results genai-bench excel \\ --experiment-dir ./experiments/latest \\ --output-file results.xlsx # Generate plots genai-bench plot \\ --experiment-dirs ./experiments/latest \\ --output-file results.png REST API \u00b6 # Start benchmark via REST API curl -X POST http://localhost:8080/api/v1/benchmarks \\ -H \"Content-Type: application/json\" \\ -d '{ \"config\": { \"api_backend\": \"openai\", \"api_base\": \"http://localhost:8082\", \"api_key\": \"your-api-key\", \"api_model_name\": \"llama-2-7b\", \"task\": \"text-to-text\", \"dataset_name\": \"sonnet.txt\", \"num_users\": 2, \"max_time_per_run\": 60 } }' # Get benchmark status curl http://localhost:8080/api/v1/benchmarks/ { benchmark_id } /status # Get results curl http://localhost:8080/api/v1/benchmarks/ { benchmark_id } /results API Stability \u00b6 Stability Levels \u00b6 Stable : APIs that are guaranteed to be backward compatible Beta : APIs that may change but with deprecation notices Alpha : Experimental APIs that may change without notice Current Stability \u00b6 API Component Stability Level Notes CLI Commands Stable Core commands are stable Python Core API Stable Main interfaces are stable Configuration Schema Stable Core configuration is stable Metrics Format Beta May add new metrics REST API Beta Under active development Extension APIs Alpha Subject to change Versioning \u00b6 GenAI Bench follows semantic versioning (MAJOR.MINOR.PATCH): MAJOR : Breaking changes to stable APIs MINOR : New features, backward compatible PATCH : Bug fixes, backward compatible API Versioning \u00b6 CLI : Version specified via --version flag Python API : Version available via genai_bench.__version__ REST API : Version in URL path ( /api/v1/ ) Authentication \u00b6 API Keys \u00b6 GenAI Bench supports various authentication methods: # Environment variable (recommended) import os config . api_key = os . getenv ( \"OPENAI_API_KEY\" ) # Direct assignment config . api_key = \"your-api-key\" # File-based with open ( \"api_key.txt\" ) as f : config . api_key = f . read () . strip () Security Best Practices \u00b6 Never hardcode API keys in source code Use environment variables for sensitive data Rotate keys regularly for production systems Limit key permissions where possible Error Handling \u00b6 Common Error Types \u00b6 from genai_bench.exceptions import ( BenchmarkError , ConfigurationError , APIError , DatasetError ) try : results = benchmark . run () except ConfigurationError as e : print ( f \"Configuration error: { e } \" ) except APIError as e : print ( f \"API error: { e } \" ) except DatasetError as e : print ( f \"Dataset error: { e } \" ) except BenchmarkError as e : print ( f \"Benchmark error: { e } \" ) Error Response Format \u00b6 { \"error\" : { \"type\" : \"ConfigurationError\" , \"message\" : \"Invalid API endpoint\" , \"details\" : { \"field\" : \"api_base\" , \"value\" : \"invalid-url\" } } } Integration Examples \u00b6 CI/CD Integration \u00b6 # GitHub Actions example name : Performance Benchmark on : [ push ] jobs : benchmark : runs-on : ubuntu-latest steps : - uses : actions/checkout@v4 - name : Run Benchmark run : | pip install genai-bench genai-bench benchmark \\ --api-backend openai \\ --api-base \"${{ secrets.API_BASE }}\" \\ --api-key \"${{ secrets.API_KEY }}\" \\ --api-model-name \"test-model\" \\ --task text-to-text \\ --num-users 2 \\ --max-time-per-run 60 Docker Integration \u00b6 FROM python:3.11-slim RUN pip install genai-bench COPY benchmark-config.yaml /app/ WORKDIR /app CMD [ \"genai-bench\" , \"benchmark\" , \"--config\" , \"benchmark-config.yaml\" ] Kubernetes Integration \u00b6 apiVersion : batch/v1 kind : Job metadata : name : genai-bench-job spec : template : spec : containers : - name : benchmark image : genai-bench:latest env : - name : API_KEY valueFrom : secretKeyRef : name : api-secrets key : openai-key command : [ \"genai-bench\" , \"benchmark\" ] args : [ \"--api-backend\" , \"openai\" , \"--api-base\" , \"http://model-service:8080\" , \"--task\" , \"text-to-text\" , \"--num-users\" , \"10\" , \"--max-time-per-run\" , \"300\" ] restartPolicy : Never Performance Considerations \u00b6 Resource Usage \u00b6 Memory : Scales with number of concurrent users and dataset size CPU : Depends on tokenization and metrics processing Network : Proportional to request rate and payload size Optimization Tips \u00b6 Use appropriate concurrency for your target system Monitor resource usage during benchmarks Batch requests when possible Cache tokenizers for repeated use Troubleshooting \u00b6 Common Issues \u00b6 Connection timeouts : Check network connectivity and API endpoint Authentication errors : Verify API keys and permissions Rate limiting : Reduce concurrency or add delays Memory issues : Reduce dataset size or concurrency Debug Mode \u00b6 # Enable debug logging genai-bench benchmark --log-level DEBUG # Save detailed logs genai-bench benchmark --log-file benchmark.log Getting Help \u00b6 Check the User Guide for detailed documentation Review Examples for practical use cases Open an issue for bugs or questions Next Steps \u00b6 Explore the CLI Reference for command-line usage Read about Tasks and Benchmarks for different benchmark types Check out Examples for practical scenarios","title":"Overview"},{"location":"api/overview/#api-overview","text":"This section provides comprehensive documentation for GenAI Bench's APIs, interfaces, and extension points. Whether you're integrating GenAI Bench into your workflow or extending its functionality, this documentation will help you understand the available interfaces.","title":"API Overview"},{"location":"api/overview/#api-categories","text":"GenAI Bench provides several types of APIs: CLI API - Command-line interface for running benchmarks (see CLI Reference ) Python API - Programmatic interface for Python applications REST API - HTTP API for remote access (when using web UI) Extension APIs - Interfaces for extending functionality","title":"API Categories"},{"location":"api/overview/#core-concepts","text":"","title":"Core Concepts"},{"location":"api/overview/#benchmark-configuration","text":"All APIs work with a common configuration structure: from dataclasses import dataclass from typing import Optional , Dict , Any @dataclass class BenchmarkConfig : # API Configuration api_backend : str api_base : str api_key : str api_model_name : str # Task Configuration task : str dataset_name : str # Load Configuration num_users : int max_time_per_run : int max_requests_per_run : Optional [ int ] = None spawn_rate : Optional [ float ] = None # Output Configuration output_dir : str = \"./results\" ui : bool = False # Advanced Configuration additional_sampling_params : Optional [ Dict [ str , Any ]] = None server_info : Optional [ Dict [ str , Any ]] = None","title":"Benchmark Configuration"},{"location":"api/overview/#task-types","text":"GenAI Bench supports several task types: from enum import Enum class TaskType ( Enum ): TEXT_TO_TEXT = \"text-to-text\" TEXT_TO_EMBEDDINGS = \"text-to-embeddings\" IMAGE_TEXT_TO_TEXT = \"image-text-to-text\" IMAGE_TO_EMBEDDINGS = \"image-to-embeddings\"","title":"Task Types"},{"location":"api/overview/#metrics-structure","text":"All APIs return metrics in a consistent format: @dataclass class BenchmarkMetrics : # Request-level metrics request_metrics : List [ RequestMetric ] # Aggregated metrics aggregated_metrics : AggregatedMetrics # System metrics system_metrics : SystemMetrics # Metadata benchmark_info : BenchmarkInfo","title":"Metrics Structure"},{"location":"api/overview/#quick-start-examples","text":"","title":"Quick Start Examples"},{"location":"api/overview/#python-api","text":"from genai_bench import Benchmark # Create benchmark configuration config = BenchmarkConfig ( api_backend = \"openai\" , api_base = \"http://localhost:8082\" , api_key = \"your-api-key\" , api_model_name = \"llama-2-7b\" , task = \"text-to-text\" , dataset_name = \"sonnet.txt\" , num_users = 2 , max_time_per_run = 60 ) # Run benchmark benchmark = Benchmark ( config ) results = benchmark . run () # Access metrics print ( f \"Average latency: { results . aggregated_metrics . avg_latency } \" ) print ( f \"Throughput: { results . aggregated_metrics . throughput } \" )","title":"Python API"},{"location":"api/overview/#cli-api","text":"# Basic benchmark genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --dataset-name \"sonnet.txt\" \\ --num-users 2 \\ --max-time-per-run 60 # Export results genai-bench excel \\ --experiment-dir ./experiments/latest \\ --output-file results.xlsx # Generate plots genai-bench plot \\ --experiment-dirs ./experiments/latest \\ --output-file results.png","title":"CLI API"},{"location":"api/overview/#rest-api","text":"# Start benchmark via REST API curl -X POST http://localhost:8080/api/v1/benchmarks \\ -H \"Content-Type: application/json\" \\ -d '{ \"config\": { \"api_backend\": \"openai\", \"api_base\": \"http://localhost:8082\", \"api_key\": \"your-api-key\", \"api_model_name\": \"llama-2-7b\", \"task\": \"text-to-text\", \"dataset_name\": \"sonnet.txt\", \"num_users\": 2, \"max_time_per_run\": 60 } }' # Get benchmark status curl http://localhost:8080/api/v1/benchmarks/ { benchmark_id } /status # Get results curl http://localhost:8080/api/v1/benchmarks/ { benchmark_id } /results","title":"REST API"},{"location":"api/overview/#api-stability","text":"","title":"API Stability"},{"location":"api/overview/#stability-levels","text":"Stable : APIs that are guaranteed to be backward compatible Beta : APIs that may change but with deprecation notices Alpha : Experimental APIs that may change without notice","title":"Stability Levels"},{"location":"api/overview/#current-stability","text":"API Component Stability Level Notes CLI Commands Stable Core commands are stable Python Core API Stable Main interfaces are stable Configuration Schema Stable Core configuration is stable Metrics Format Beta May add new metrics REST API Beta Under active development Extension APIs Alpha Subject to change","title":"Current Stability"},{"location":"api/overview/#versioning","text":"GenAI Bench follows semantic versioning (MAJOR.MINOR.PATCH): MAJOR : Breaking changes to stable APIs MINOR : New features, backward compatible PATCH : Bug fixes, backward compatible","title":"Versioning"},{"location":"api/overview/#api-versioning","text":"CLI : Version specified via --version flag Python API : Version available via genai_bench.__version__ REST API : Version in URL path ( /api/v1/ )","title":"API Versioning"},{"location":"api/overview/#authentication","text":"","title":"Authentication"},{"location":"api/overview/#api-keys","text":"GenAI Bench supports various authentication methods: # Environment variable (recommended) import os config . api_key = os . getenv ( \"OPENAI_API_KEY\" ) # Direct assignment config . api_key = \"your-api-key\" # File-based with open ( \"api_key.txt\" ) as f : config . api_key = f . read () . strip ()","title":"API Keys"},{"location":"api/overview/#security-best-practices","text":"Never hardcode API keys in source code Use environment variables for sensitive data Rotate keys regularly for production systems Limit key permissions where possible","title":"Security Best Practices"},{"location":"api/overview/#error-handling","text":"","title":"Error Handling"},{"location":"api/overview/#common-error-types","text":"from genai_bench.exceptions import ( BenchmarkError , ConfigurationError , APIError , DatasetError ) try : results = benchmark . run () except ConfigurationError as e : print ( f \"Configuration error: { e } \" ) except APIError as e : print ( f \"API error: { e } \" ) except DatasetError as e : print ( f \"Dataset error: { e } \" ) except BenchmarkError as e : print ( f \"Benchmark error: { e } \" )","title":"Common Error Types"},{"location":"api/overview/#error-response-format","text":"{ \"error\" : { \"type\" : \"ConfigurationError\" , \"message\" : \"Invalid API endpoint\" , \"details\" : { \"field\" : \"api_base\" , \"value\" : \"invalid-url\" } } }","title":"Error Response Format"},{"location":"api/overview/#integration-examples","text":"","title":"Integration Examples"},{"location":"api/overview/#cicd-integration","text":"# GitHub Actions example name : Performance Benchmark on : [ push ] jobs : benchmark : runs-on : ubuntu-latest steps : - uses : actions/checkout@v4 - name : Run Benchmark run : | pip install genai-bench genai-bench benchmark \\ --api-backend openai \\ --api-base \"${{ secrets.API_BASE }}\" \\ --api-key \"${{ secrets.API_KEY }}\" \\ --api-model-name \"test-model\" \\ --task text-to-text \\ --num-users 2 \\ --max-time-per-run 60","title":"CI/CD Integration"},{"location":"api/overview/#docker-integration","text":"FROM python:3.11-slim RUN pip install genai-bench COPY benchmark-config.yaml /app/ WORKDIR /app CMD [ \"genai-bench\" , \"benchmark\" , \"--config\" , \"benchmark-config.yaml\" ]","title":"Docker Integration"},{"location":"api/overview/#kubernetes-integration","text":"apiVersion : batch/v1 kind : Job metadata : name : genai-bench-job spec : template : spec : containers : - name : benchmark image : genai-bench:latest env : - name : API_KEY valueFrom : secretKeyRef : name : api-secrets key : openai-key command : [ \"genai-bench\" , \"benchmark\" ] args : [ \"--api-backend\" , \"openai\" , \"--api-base\" , \"http://model-service:8080\" , \"--task\" , \"text-to-text\" , \"--num-users\" , \"10\" , \"--max-time-per-run\" , \"300\" ] restartPolicy : Never","title":"Kubernetes Integration"},{"location":"api/overview/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"api/overview/#resource-usage","text":"Memory : Scales with number of concurrent users and dataset size CPU : Depends on tokenization and metrics processing Network : Proportional to request rate and payload size","title":"Resource Usage"},{"location":"api/overview/#optimization-tips","text":"Use appropriate concurrency for your target system Monitor resource usage during benchmarks Batch requests when possible Cache tokenizers for repeated use","title":"Optimization Tips"},{"location":"api/overview/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"api/overview/#common-issues","text":"Connection timeouts : Check network connectivity and API endpoint Authentication errors : Verify API keys and permissions Rate limiting : Reduce concurrency or add delays Memory issues : Reduce dataset size or concurrency","title":"Common Issues"},{"location":"api/overview/#debug-mode","text":"# Enable debug logging genai-bench benchmark --log-level DEBUG # Save detailed logs genai-bench benchmark --log-file benchmark.log","title":"Debug Mode"},{"location":"api/overview/#getting-help","text":"Check the User Guide for detailed documentation Review Examples for practical use cases Open an issue for bugs or questions","title":"Getting Help"},{"location":"api/overview/#next-steps","text":"Explore the CLI Reference for command-line usage Read about Tasks and Benchmarks for different benchmark types Check out Examples for practical scenarios","title":"Next Steps"},{"location":"development/architecture/","text":"Architecture Guide \u00b6 This guide provides an overview of GenAI Bench's architecture, design principles, and key components to help developers understand the system and contribute effectively. Overview \u00b6 GenAI Bench is a comprehensive benchmarking framework for Generative AI APIs that focuses on performance, scalability, and ease of use. The architecture is designed to be modular, extensible, and capable of handling various types of AI workloads. Design Principles \u00b6 1. Modularity \u00b6 Separation of concerns : Each component has a specific responsibility Pluggable architecture : Easy to add new backends, metrics, or tasks Clean interfaces : Well-defined APIs between components 2. Scalability \u00b6 Concurrent execution : Support for multiple users and requests Distributed testing : Ability to run across multiple machines Resource efficiency : Optimized for both small and large-scale testing 3. Extensibility \u00b6 Backend agnostic : Support for multiple API providers Task flexibility : Easy to add new task types Metric customization : Extensible metrics collection 4. Reliability \u00b6 Error handling : Robust error recovery and reporting Data integrity : Accurate metrics collection and storage Reproducibility : Consistent results across runs System Architecture \u00b6 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 GenAI Bench \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 CLI Interface \u2502 Web UI \u2502 Configuration Management \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u2502 \u2502 \u2502 Benchmark \u2502 Task \u2502 Traffic Scenario \u2502 \u2502 Orchestrator \u2502 Manager \u2502 Management \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u2502 \u2502 \u2502 User Pool \u2502 Dataset \u2502 Metrics Collection \u2502 \u2502 Management \u2502 Loader \u2502 & Analysis \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u2502 \u2502 \u2502 API Backend \u2502 Auth \u2502 Result Storage & \u2502 \u2502 Abstraction \u2502 Manager \u2502 Reporting \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 External APIs \u2502 \u2502 (OpenAI, Cohere,\u2502 \u2502 vLLM, etc.) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Core Components \u00b6 1. CLI Interface ( genai_bench/cli/ ) \u00b6 The command-line interface is the primary entry point for users. Key Files: - main.py : Main CLI entry point - benchmark.py : Benchmark command implementation - export.py : Export functionality - plot.py : Plotting commands Responsibilities: - Parse command-line arguments - Validate configuration - Orchestrate benchmark execution - Handle user interactions 2. Benchmark Orchestrator ( genai_bench/ ) \u00b6 The core orchestration logic that coordinates all components. Key Files: - benchmark.py : Main benchmark orchestration - locust_benchmark.py : Locust-based load testing Responsibilities: - Initialize and configure components - Manage benchmark lifecycle - Coordinate user simulation - Handle distributed execution 3. Task Management ( genai_bench/ ) \u00b6 Handles different types of AI tasks and their execution. Task Types: - Text-to-text generation - Text-to-embeddings - Image-text-to-text (vision) - Image-to-embeddings Responsibilities: - Task definition and validation - Request formatting - Response processing - Task-specific metrics 4. User Pool Management ( genai_bench/user/ ) \u00b6 Simulates concurrent users making API requests. Key Files: - base_user.py : Base user class - openai_user.py : OpenAI API user - cohere_user.py : Cohere API user Responsibilities: - User simulation - Request execution - Error handling - Response collection 5. API Backend Abstraction ( genai_bench/user/ ) \u00b6 Provides a unified interface for different API providers. Supported Backends: - OpenAI-compatible APIs - Cohere API - OCI Cohere API - Custom backends Responsibilities: - API client management - Request formatting - Response parsing - Error handling 6. Metrics Collection ( genai_bench/metrics/ ) \u00b6 Collects, processes, and analyzes performance metrics. Key Files: - metrics.py : Core metrics definitions - analysis.py : Analysis utilities Metric Types: - Latency metrics (TTFT, E2E) - Throughput metrics - Token metrics - Error rates - Custom metrics Responsibilities: - Real-time metrics collection - Statistical analysis - Aggregation and summarization - Export formatting 7. Dataset Management ( genai_bench/sampling/ ) \u00b6 Handles dataset loading and tokenization. Key Files: - tokenizer.py : Tokenization utilities - dataset_loader.py : Dataset loading logic Dataset Types: - Built-in datasets (sonnet.txt, etc.) - Custom text datasets - Vision datasets - Hugging Face datasets Responsibilities: - Dataset loading and validation - Tokenization and preprocessing - Sampling strategies - Format conversion 8. Traffic Scenarios ( genai_bench/scenarios/ ) \u00b6 Defines different traffic patterns for testing. Scenario Types: - Constant load - Burst traffic - Ramp-up patterns - Custom scenarios Responsibilities: - Traffic pattern definition - User spawn rate control - Load distribution - Timing coordination 9. Authentication ( genai_bench/auth/ ) \u00b6 Manages authentication for different API providers. Key Files: - auth.py : Authentication utilities - Provider-specific auth modules Responsibilities: - API key management - Token refresh - Authentication validation - Security handling 10. Analysis & Reporting ( genai_bench/analysis/ ) \u00b6 Processes results and generates reports. Key Files: - analysis.py : Analysis utilities - plotting.py : Visualization tools - export.py : Export functionality Responsibilities: - Result processing - Statistical analysis - Report generation - Visualization creation 11. Web UI ( genai_bench/ui/ ) \u00b6 Provides a web interface for monitoring and control. Responsibilities: - Real-time monitoring - Interactive control - Result visualization - Configuration management 12. Distributed Execution ( genai_bench/distributed/ ) \u00b6 Enables distributed benchmarking across multiple machines. Responsibilities: - Cluster coordination - Load distribution - Result aggregation - Fault tolerance Data Flow \u00b6 1. Benchmark Initialization \u00b6 Configuration \u2192 Validation \u2192 Component Setup \u2192 Dataset Loading 2. Benchmark Execution \u00b6 User Spawn \u2192 Request Generation \u2192 API Calls \u2192 Response Collection \u2192 Metrics Recording 3. Result Processing \u00b6 Raw Metrics \u2192 Aggregation \u2192 Analysis \u2192 Report Generation \u2192 Export Key Design Patterns \u00b6 1. Strategy Pattern \u00b6 Used for pluggable backends and metrics: class APIBackend ( ABC ): @abstractmethod def make_request ( self , request : Request ) -> Response : pass class OpenAIBackend ( APIBackend ): def make_request ( self , request : Request ) -> Response : # OpenAI-specific implementation pass 2. Observer Pattern \u00b6 Used for metrics collection: class MetricsCollector : def __init__ ( self ): self . observers = [] def add_observer ( self , observer ): self . observers . append ( observer ) def notify ( self , event ): for observer in self . observers : observer . handle ( event ) 3. Factory Pattern \u00b6 Used for creating users and backends: class UserFactory : @staticmethod def create_user ( backend_type : str , config : Config ) -> BaseUser : if backend_type == \"openai\" : return OpenAIUser ( config ) elif backend_type == \"cohere\" : return CohereUser ( config ) # ... 4. Command Pattern \u00b6 Used for CLI command handling: class BenchmarkCommand : def __init__ ( self , args ): self . args = args def execute ( self ): # Execute benchmark logic pass Configuration Management \u00b6 Configuration Sources \u00b6 Command-line arguments : Highest priority Environment variables : Medium priority Configuration files : Lowest priority Defaults : Fallback values Configuration Flow \u00b6 CLI Args \u2192 Env Vars \u2192 Config Files \u2192 Defaults \u2192 Validation \u2192 Final Config Error Handling Strategy \u00b6 Error Types \u00b6 Configuration errors : Invalid settings Network errors : API connectivity issues Authentication errors : Invalid credentials Rate limiting : API quota exceeded Timeout errors : Request timeouts Data errors : Invalid datasets or responses Error Handling Approach \u00b6 Graceful degradation : Continue when possible Detailed logging : Comprehensive error information User feedback : Clear error messages Recovery mechanisms : Retry logic and fallbacks Performance Considerations \u00b6 Optimization Areas \u00b6 Concurrent execution : Async/await patterns Memory management : Efficient data structures Network optimization : Connection pooling CPU optimization : Efficient algorithms I/O optimization : Batched operations Monitoring \u00b6 Resource usage : CPU, memory, network Performance metrics : Latency, throughput Error rates : Success/failure ratios System health : Component status Security Considerations \u00b6 Security Measures \u00b6 API key protection : Secure storage and transmission Input validation : Sanitize all inputs Network security : HTTPS/TLS enforcement Access control : Authentication and authorization Data privacy : Secure data handling Testing Architecture \u00b6 Test Types \u00b6 Unit tests : Component-level testing Integration tests : Component interaction testing End-to-end tests : Full workflow testing Performance tests : Benchmark validation Security tests : Vulnerability testing Test Structure \u00b6 tests/ \u251c\u2500\u2500 unit/ # Unit tests \u251c\u2500\u2500 integration/ # Integration tests \u251c\u2500\u2500 e2e/ # End-to-end tests \u251c\u2500\u2500 fixtures/ # Test data \u2514\u2500\u2500 conftest.py # Pytest configuration Deployment Architecture \u00b6 Deployment Options \u00b6 Local installation : Single machine setup Docker containers : Containerized deployment Kubernetes : Orchestrated deployment Cloud platforms : Managed services Scalability Patterns \u00b6 Horizontal scaling : Multiple instances Vertical scaling : Resource increase Load balancing : Request distribution Caching : Result caching Extension Points \u00b6 Adding New Backends \u00b6 Implement BaseUser interface Add backend-specific configuration Register in factory Add tests and documentation Adding New Metrics \u00b6 Implement metric collection logic Add to metrics registry Update analysis tools Add visualization support Adding New Tasks \u00b6 Define task interface Implement request/response handling Add task-specific metrics Update documentation Future Architecture Considerations \u00b6 Planned Improvements \u00b6 Plugin system : Dynamic component loading Configuration UI : Web-based configuration Real-time streaming : Live metrics streaming ML integration : Automated analysis Cloud native : Kubernetes operators Scalability Roadmap \u00b6 Microservices : Service decomposition Event streaming : Async communication Data pipeline : Streaming analytics Multi-region : Global deployment Best Practices for Developers \u00b6 Code Organization \u00b6 Single responsibility : One purpose per module Clear interfaces : Well-defined APIs Dependency injection : Loose coupling Error handling : Comprehensive coverage Performance \u00b6 Async patterns : Non-blocking operations Resource pooling : Efficient resource use Caching : Appropriate caching strategies Profiling : Regular performance analysis Maintainability \u00b6 Documentation : Comprehensive docs Testing : High test coverage Logging : Structured logging Monitoring : Observability Troubleshooting \u00b6 Common Issues \u00b6 Import errors : Module path issues Configuration errors : Invalid settings Network issues : Connectivity problems Performance issues : Resource constraints Debugging Tools \u00b6 Logging : Debug-level logging Profiling : Performance profiling Monitoring : System monitoring Testing : Isolated testing Next Steps \u00b6 Read the Contributing Guide for development setup Explore the codebase following this architectural overview Start with small contributions to understand the system better","title":"Architecture"},{"location":"development/architecture/#architecture-guide","text":"This guide provides an overview of GenAI Bench's architecture, design principles, and key components to help developers understand the system and contribute effectively.","title":"Architecture Guide"},{"location":"development/architecture/#overview","text":"GenAI Bench is a comprehensive benchmarking framework for Generative AI APIs that focuses on performance, scalability, and ease of use. The architecture is designed to be modular, extensible, and capable of handling various types of AI workloads.","title":"Overview"},{"location":"development/architecture/#design-principles","text":"","title":"Design Principles"},{"location":"development/architecture/#1-modularity","text":"Separation of concerns : Each component has a specific responsibility Pluggable architecture : Easy to add new backends, metrics, or tasks Clean interfaces : Well-defined APIs between components","title":"1. Modularity"},{"location":"development/architecture/#2-scalability","text":"Concurrent execution : Support for multiple users and requests Distributed testing : Ability to run across multiple machines Resource efficiency : Optimized for both small and large-scale testing","title":"2. Scalability"},{"location":"development/architecture/#3-extensibility","text":"Backend agnostic : Support for multiple API providers Task flexibility : Easy to add new task types Metric customization : Extensible metrics collection","title":"3. Extensibility"},{"location":"development/architecture/#4-reliability","text":"Error handling : Robust error recovery and reporting Data integrity : Accurate metrics collection and storage Reproducibility : Consistent results across runs","title":"4. Reliability"},{"location":"development/architecture/#system-architecture","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 GenAI Bench \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 CLI Interface \u2502 Web UI \u2502 Configuration Management \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u2502 \u2502 \u2502 Benchmark \u2502 Task \u2502 Traffic Scenario \u2502 \u2502 Orchestrator \u2502 Manager \u2502 Management \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u2502 \u2502 \u2502 User Pool \u2502 Dataset \u2502 Metrics Collection \u2502 \u2502 Management \u2502 Loader \u2502 & Analysis \u2502 \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 \u2502 \u2502 \u2502 API Backend \u2502 Auth \u2502 Result Storage & \u2502 \u2502 Abstraction \u2502 Manager \u2502 Reporting \u2502 \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 External APIs \u2502 \u2502 (OpenAI, Cohere,\u2502 \u2502 vLLM, etc.) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"System Architecture"},{"location":"development/architecture/#core-components","text":"","title":"Core Components"},{"location":"development/architecture/#1-cli-interface-genai_benchcli","text":"The command-line interface is the primary entry point for users. Key Files: - main.py : Main CLI entry point - benchmark.py : Benchmark command implementation - export.py : Export functionality - plot.py : Plotting commands Responsibilities: - Parse command-line arguments - Validate configuration - Orchestrate benchmark execution - Handle user interactions","title":"1. CLI Interface (genai_bench/cli/)"},{"location":"development/architecture/#2-benchmark-orchestrator-genai_bench","text":"The core orchestration logic that coordinates all components. Key Files: - benchmark.py : Main benchmark orchestration - locust_benchmark.py : Locust-based load testing Responsibilities: - Initialize and configure components - Manage benchmark lifecycle - Coordinate user simulation - Handle distributed execution","title":"2. Benchmark Orchestrator (genai_bench/)"},{"location":"development/architecture/#3-task-management-genai_bench","text":"Handles different types of AI tasks and their execution. Task Types: - Text-to-text generation - Text-to-embeddings - Image-text-to-text (vision) - Image-to-embeddings Responsibilities: - Task definition and validation - Request formatting - Response processing - Task-specific metrics","title":"3. Task Management (genai_bench/)"},{"location":"development/architecture/#4-user-pool-management-genai_benchuser","text":"Simulates concurrent users making API requests. Key Files: - base_user.py : Base user class - openai_user.py : OpenAI API user - cohere_user.py : Cohere API user Responsibilities: - User simulation - Request execution - Error handling - Response collection","title":"4. User Pool Management (genai_bench/user/)"},{"location":"development/architecture/#5-api-backend-abstraction-genai_benchuser","text":"Provides a unified interface for different API providers. Supported Backends: - OpenAI-compatible APIs - Cohere API - OCI Cohere API - Custom backends Responsibilities: - API client management - Request formatting - Response parsing - Error handling","title":"5. API Backend Abstraction (genai_bench/user/)"},{"location":"development/architecture/#6-metrics-collection-genai_benchmetrics","text":"Collects, processes, and analyzes performance metrics. Key Files: - metrics.py : Core metrics definitions - analysis.py : Analysis utilities Metric Types: - Latency metrics (TTFT, E2E) - Throughput metrics - Token metrics - Error rates - Custom metrics Responsibilities: - Real-time metrics collection - Statistical analysis - Aggregation and summarization - Export formatting","title":"6. Metrics Collection (genai_bench/metrics/)"},{"location":"development/architecture/#7-dataset-management-genai_benchsampling","text":"Handles dataset loading and tokenization. Key Files: - tokenizer.py : Tokenization utilities - dataset_loader.py : Dataset loading logic Dataset Types: - Built-in datasets (sonnet.txt, etc.) - Custom text datasets - Vision datasets - Hugging Face datasets Responsibilities: - Dataset loading and validation - Tokenization and preprocessing - Sampling strategies - Format conversion","title":"7. Dataset Management (genai_bench/sampling/)"},{"location":"development/architecture/#8-traffic-scenarios-genai_benchscenarios","text":"Defines different traffic patterns for testing. Scenario Types: - Constant load - Burst traffic - Ramp-up patterns - Custom scenarios Responsibilities: - Traffic pattern definition - User spawn rate control - Load distribution - Timing coordination","title":"8. Traffic Scenarios (genai_bench/scenarios/)"},{"location":"development/architecture/#9-authentication-genai_benchauth","text":"Manages authentication for different API providers. Key Files: - auth.py : Authentication utilities - Provider-specific auth modules Responsibilities: - API key management - Token refresh - Authentication validation - Security handling","title":"9. Authentication (genai_bench/auth/)"},{"location":"development/architecture/#10-analysis-reporting-genai_benchanalysis","text":"Processes results and generates reports. Key Files: - analysis.py : Analysis utilities - plotting.py : Visualization tools - export.py : Export functionality Responsibilities: - Result processing - Statistical analysis - Report generation - Visualization creation","title":"10. Analysis &amp; Reporting (genai_bench/analysis/)"},{"location":"development/architecture/#11-web-ui-genai_benchui","text":"Provides a web interface for monitoring and control. Responsibilities: - Real-time monitoring - Interactive control - Result visualization - Configuration management","title":"11. Web UI (genai_bench/ui/)"},{"location":"development/architecture/#12-distributed-execution-genai_benchdistributed","text":"Enables distributed benchmarking across multiple machines. Responsibilities: - Cluster coordination - Load distribution - Result aggregation - Fault tolerance","title":"12. Distributed Execution (genai_bench/distributed/)"},{"location":"development/architecture/#data-flow","text":"","title":"Data Flow"},{"location":"development/architecture/#1-benchmark-initialization","text":"Configuration \u2192 Validation \u2192 Component Setup \u2192 Dataset Loading","title":"1. Benchmark Initialization"},{"location":"development/architecture/#2-benchmark-execution","text":"User Spawn \u2192 Request Generation \u2192 API Calls \u2192 Response Collection \u2192 Metrics Recording","title":"2. Benchmark Execution"},{"location":"development/architecture/#3-result-processing","text":"Raw Metrics \u2192 Aggregation \u2192 Analysis \u2192 Report Generation \u2192 Export","title":"3. Result Processing"},{"location":"development/architecture/#key-design-patterns","text":"","title":"Key Design Patterns"},{"location":"development/architecture/#1-strategy-pattern","text":"Used for pluggable backends and metrics: class APIBackend ( ABC ): @abstractmethod def make_request ( self , request : Request ) -> Response : pass class OpenAIBackend ( APIBackend ): def make_request ( self , request : Request ) -> Response : # OpenAI-specific implementation pass","title":"1. Strategy Pattern"},{"location":"development/architecture/#2-observer-pattern","text":"Used for metrics collection: class MetricsCollector : def __init__ ( self ): self . observers = [] def add_observer ( self , observer ): self . observers . append ( observer ) def notify ( self , event ): for observer in self . observers : observer . handle ( event )","title":"2. Observer Pattern"},{"location":"development/architecture/#3-factory-pattern","text":"Used for creating users and backends: class UserFactory : @staticmethod def create_user ( backend_type : str , config : Config ) -> BaseUser : if backend_type == \"openai\" : return OpenAIUser ( config ) elif backend_type == \"cohere\" : return CohereUser ( config ) # ...","title":"3. Factory Pattern"},{"location":"development/architecture/#4-command-pattern","text":"Used for CLI command handling: class BenchmarkCommand : def __init__ ( self , args ): self . args = args def execute ( self ): # Execute benchmark logic pass","title":"4. Command Pattern"},{"location":"development/architecture/#configuration-management","text":"","title":"Configuration Management"},{"location":"development/architecture/#configuration-sources","text":"Command-line arguments : Highest priority Environment variables : Medium priority Configuration files : Lowest priority Defaults : Fallback values","title":"Configuration Sources"},{"location":"development/architecture/#configuration-flow","text":"CLI Args \u2192 Env Vars \u2192 Config Files \u2192 Defaults \u2192 Validation \u2192 Final Config","title":"Configuration Flow"},{"location":"development/architecture/#error-handling-strategy","text":"","title":"Error Handling Strategy"},{"location":"development/architecture/#error-types","text":"Configuration errors : Invalid settings Network errors : API connectivity issues Authentication errors : Invalid credentials Rate limiting : API quota exceeded Timeout errors : Request timeouts Data errors : Invalid datasets or responses","title":"Error Types"},{"location":"development/architecture/#error-handling-approach","text":"Graceful degradation : Continue when possible Detailed logging : Comprehensive error information User feedback : Clear error messages Recovery mechanisms : Retry logic and fallbacks","title":"Error Handling Approach"},{"location":"development/architecture/#performance-considerations","text":"","title":"Performance Considerations"},{"location":"development/architecture/#optimization-areas","text":"Concurrent execution : Async/await patterns Memory management : Efficient data structures Network optimization : Connection pooling CPU optimization : Efficient algorithms I/O optimization : Batched operations","title":"Optimization Areas"},{"location":"development/architecture/#monitoring","text":"Resource usage : CPU, memory, network Performance metrics : Latency, throughput Error rates : Success/failure ratios System health : Component status","title":"Monitoring"},{"location":"development/architecture/#security-considerations","text":"","title":"Security Considerations"},{"location":"development/architecture/#security-measures","text":"API key protection : Secure storage and transmission Input validation : Sanitize all inputs Network security : HTTPS/TLS enforcement Access control : Authentication and authorization Data privacy : Secure data handling","title":"Security Measures"},{"location":"development/architecture/#testing-architecture","text":"","title":"Testing Architecture"},{"location":"development/architecture/#test-types","text":"Unit tests : Component-level testing Integration tests : Component interaction testing End-to-end tests : Full workflow testing Performance tests : Benchmark validation Security tests : Vulnerability testing","title":"Test Types"},{"location":"development/architecture/#test-structure","text":"tests/ \u251c\u2500\u2500 unit/ # Unit tests \u251c\u2500\u2500 integration/ # Integration tests \u251c\u2500\u2500 e2e/ # End-to-end tests \u251c\u2500\u2500 fixtures/ # Test data \u2514\u2500\u2500 conftest.py # Pytest configuration","title":"Test Structure"},{"location":"development/architecture/#deployment-architecture","text":"","title":"Deployment Architecture"},{"location":"development/architecture/#deployment-options","text":"Local installation : Single machine setup Docker containers : Containerized deployment Kubernetes : Orchestrated deployment Cloud platforms : Managed services","title":"Deployment Options"},{"location":"development/architecture/#scalability-patterns","text":"Horizontal scaling : Multiple instances Vertical scaling : Resource increase Load balancing : Request distribution Caching : Result caching","title":"Scalability Patterns"},{"location":"development/architecture/#extension-points","text":"","title":"Extension Points"},{"location":"development/architecture/#adding-new-backends","text":"Implement BaseUser interface Add backend-specific configuration Register in factory Add tests and documentation","title":"Adding New Backends"},{"location":"development/architecture/#adding-new-metrics","text":"Implement metric collection logic Add to metrics registry Update analysis tools Add visualization support","title":"Adding New Metrics"},{"location":"development/architecture/#adding-new-tasks","text":"Define task interface Implement request/response handling Add task-specific metrics Update documentation","title":"Adding New Tasks"},{"location":"development/architecture/#future-architecture-considerations","text":"","title":"Future Architecture Considerations"},{"location":"development/architecture/#planned-improvements","text":"Plugin system : Dynamic component loading Configuration UI : Web-based configuration Real-time streaming : Live metrics streaming ML integration : Automated analysis Cloud native : Kubernetes operators","title":"Planned Improvements"},{"location":"development/architecture/#scalability-roadmap","text":"Microservices : Service decomposition Event streaming : Async communication Data pipeline : Streaming analytics Multi-region : Global deployment","title":"Scalability Roadmap"},{"location":"development/architecture/#best-practices-for-developers","text":"","title":"Best Practices for Developers"},{"location":"development/architecture/#code-organization","text":"Single responsibility : One purpose per module Clear interfaces : Well-defined APIs Dependency injection : Loose coupling Error handling : Comprehensive coverage","title":"Code Organization"},{"location":"development/architecture/#performance","text":"Async patterns : Non-blocking operations Resource pooling : Efficient resource use Caching : Appropriate caching strategies Profiling : Regular performance analysis","title":"Performance"},{"location":"development/architecture/#maintainability","text":"Documentation : Comprehensive docs Testing : High test coverage Logging : Structured logging Monitoring : Observability","title":"Maintainability"},{"location":"development/architecture/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"development/architecture/#common-issues","text":"Import errors : Module path issues Configuration errors : Invalid settings Network issues : Connectivity problems Performance issues : Resource constraints","title":"Common Issues"},{"location":"development/architecture/#debugging-tools","text":"Logging : Debug-level logging Profiling : Performance profiling Monitoring : System monitoring Testing : Isolated testing","title":"Debugging Tools"},{"location":"development/architecture/#next-steps","text":"Read the Contributing Guide for development setup Explore the codebase following this architectural overview Start with small contributions to understand the system better","title":"Next Steps"},{"location":"development/contributing/","text":"Contributing to GenAI Bench \u00b6 Thank you for your interest in contributing to GenAI Bench! This guide will help you get started with development and understand our contribution process. Development Setup \u00b6 Prerequisites \u00b6 Python 3.11 or 3.12 Git Make (optional, for using Makefile commands) Local Development Environment \u00b6 Clone the repository : bash git clone https://github.com/sgl-project/genai-bench.git cd genai-bench Set up virtual environment : bash make uv source .venv/bin/activate Install in editable mode : bash make install Install development dependencies : bash pip install -e \".[dev]\" Set up pre-commit hooks : bash pre-commit install Verify Setup \u00b6 Test that everything is working: # Run tests make test # Check code quality make lint # Run type checking make type-check Project Structure \u00b6 genai-bench/ \u251c\u2500\u2500 genai_bench/ # Main package \u2502 \u251c\u2500\u2500 cli/ # Command-line interface \u2502 \u251c\u2500\u2500 metrics/ # Metrics collection and analysis \u2502 \u251c\u2500\u2500 sampling/ # Data sampling and tokenization \u2502 \u251c\u2500\u2500 scenarios/ # Traffic scenarios \u2502 \u251c\u2500\u2500 ui/ # Web UI components \u2502 \u251c\u2500\u2500 user/ # API user implementations \u2502 \u251c\u2500\u2500 auth/ # Authentication modules \u2502 \u251c\u2500\u2500 analysis/ # Result analysis tools \u2502 \u251c\u2500\u2500 distributed/ # Distributed benchmarking \u2502 \u2514\u2500\u2500 oci_object_storage/ # OCI integration \u251c\u2500\u2500 tests/ # Test suite \u251c\u2500\u2500 examples/ # Example scripts and configs \u251c\u2500\u2500 docs/ # Documentation \u251c\u2500\u2500 pyproject.toml # Project configuration \u251c\u2500\u2500 Makefile # Development commands \u2514\u2500\u2500 README.md # Project overview Development Workflow \u00b6 1. Create a Feature Branch \u00b6 git checkout -b feature/your-feature-name 2. Make Your Changes \u00b6 Follow our coding standards and make your changes. 3. Run Tests \u00b6 # Run all tests make test # Run specific test file pytest tests/test_specific_module.py # Run with coverage make test-cov 4. Check Code Quality \u00b6 # Run all quality checks make lint # Run specific checks make ruff make black make isort make mypy 5. Commit Your Changes \u00b6 git add . git commit -m \"feat: add new feature description\" 6. Push and Create Pull Request \u00b6 git push origin feature/your-feature-name Then create a pull request on GitHub. Coding Standards \u00b6 Python Style Guide \u00b6 We follow PEP 8 with some modifications: Line length : 88 characters (Black default) Import sorting : Use isort with Black profile Type hints : Required for all public functions Docstrings : Use Google style docstrings Code Formatting \u00b6 We use automated tools for code formatting: # Format code make format # Check formatting make check-format Type Checking \u00b6 We use MyPy for static type checking: # Run type checking make type-check # Type check specific file mypy genai_bench/your_module.py Linting \u00b6 We use Ruff for linting: # Run linter make lint # Fix auto-fixable issues make lint-fix Testing \u00b6 Writing Tests \u00b6 Place tests in the tests/ directory Use pytest as the testing framework Follow the naming convention: test_*.py Use descriptive test names Test Structure \u00b6 import pytest from genai_bench.your_module import your_function def test_your_function_basic (): \"\"\"Test basic functionality.\"\"\" result = your_function ( \"input\" ) assert result == \"expected_output\" def test_your_function_edge_case (): \"\"\"Test edge case handling.\"\"\" with pytest . raises ( ValueError ): your_function ( \"\" ) @pytest . fixture def sample_data (): \"\"\"Provide test data.\"\"\" return { \"key\" : \"value\" } def test_your_function_with_fixture ( sample_data ): \"\"\"Test with fixture data.\"\"\" result = your_function ( sample_data ) assert result is not None Running Tests \u00b6 # Run all tests pytest # Run with verbose output pytest -v # Run specific test pytest tests/test_specific.py::test_function # Run with coverage pytest --cov = genai_bench # Run integration tests pytest tests/integration/ Documentation \u00b6 Code Documentation \u00b6 Use Google style docstrings for all public functions Include type hints for all parameters and return values Document exceptions that may be raised Example: def process_data ( data : str , config : Dict [ str , Any ]) -> List [ str ]: \"\"\"Process input data according to configuration. Args: data: Input string to process. config: Configuration dictionary. Returns: List of processed strings. Raises: ValueError: If data is empty or invalid. ConfigError: If configuration is invalid. \"\"\" if not data : raise ValueError ( \"Data cannot be empty\" ) # Implementation here return processed_data Documentation Updates \u00b6 When adding new features: Update relevant documentation files in docs/ Add examples if applicable Update API documentation Ensure all links work correctly Pull Request Process \u00b6 Before Submitting \u00b6 Ensure tests pass : Run make test Check code quality : Run make lint Update documentation : Add/update relevant docs Add tests : Include tests for new functionality Pull Request Template \u00b6 Use this template when creating a PR: ## Description Brief description of the changes. ## Type of Change - [ ] Bug fix (non-breaking change which fixes an issue) - [ ] New feature (non-breaking change which adds functionality) - [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected) - [ ] Documentation update ## Testing - [ ] I have added tests that prove my fix is effective or that my feature works - [ ] All existing tests pass - [ ] I have tested the changes manually ## Checklist - [ ] My code follows the style guidelines of this project - [ ] I have performed a self-review of my own code - [ ] I have commented my code, particularly in hard-to-understand areas - [ ] I have made corresponding changes to the documentation - [ ] My changes generate no new warnings - [ ] I have added tests that prove my fix is effective or that my feature works - [ ] New and existing unit tests pass locally with my changes Review Process \u00b6 Automated checks must pass Code review by maintainers Documentation review if applicable Testing verification if needed Areas for Contribution \u00b6 High Priority \u00b6 Performance improvements : Optimize existing functionality Bug fixes : Fix reported issues Documentation : Improve existing docs or add missing sections Test coverage : Add tests for uncovered code Medium Priority \u00b6 New features : Add requested functionality API improvements : Enhance existing APIs UI enhancements : Improve web interface Integration : Add support for new backends Low Priority \u00b6 Code refactoring : Improve code structure Tooling : Enhance development tools Examples : Add more example configurations Getting Help \u00b6 Communication Channels \u00b6 GitHub Issues : For bug reports and feature requests GitHub Discussions : For questions and general discussion Pull Requests : For code contributions Before Asking for Help \u00b6 Check existing issues : Search for similar problems Read documentation : Check relevant docs Try debugging : Use debug mode and logs Provide context : Include error messages and environment details Creating Good Issues \u00b6 When reporting bugs or requesting features: Use clear titles : Descriptive and specific Provide context : Environment, version, steps to reproduce Include logs : Error messages and relevant output Add examples : Code snippets or configuration files Release Process \u00b6 Version Management \u00b6 We use semantic versioning (MAJOR.MINOR.PATCH): MAJOR : Breaking changes MINOR : New features, backward compatible PATCH : Bug fixes, backward compatible Release Checklist \u00b6 Before a release: Update version : Update version in pyproject.toml Update changelog : Document changes Run full test suite : Ensure all tests pass Update documentation : Ensure docs are current Create release notes : Summarize changes Code of Conduct \u00b6 Our Standards \u00b6 Be respectful and inclusive Focus on constructive feedback Help others learn and grow Respect different viewpoints Enforcement \u00b6 Report violations to maintainers Maintainers will address issues promptly Violations may result in temporary or permanent exclusion License \u00b6 By contributing to GenAI Bench, you agree that your contributions will be licensed under the MIT License. Acknowledgments \u00b6 Thank you for contributing to GenAI Bench! Your contributions help make this project better for everyone. Next Steps \u00b6 Read the Architecture Guide to understand the codebase Start with small contributions to understand the system better Join the community discussions for questions and ideas","title":"Contributing"},{"location":"development/contributing/#contributing-to-genai-bench","text":"Thank you for your interest in contributing to GenAI Bench! This guide will help you get started with development and understand our contribution process.","title":"Contributing to GenAI Bench"},{"location":"development/contributing/#development-setup","text":"","title":"Development Setup"},{"location":"development/contributing/#prerequisites","text":"Python 3.11 or 3.12 Git Make (optional, for using Makefile commands)","title":"Prerequisites"},{"location":"development/contributing/#local-development-environment","text":"Clone the repository : bash git clone https://github.com/sgl-project/genai-bench.git cd genai-bench Set up virtual environment : bash make uv source .venv/bin/activate Install in editable mode : bash make install Install development dependencies : bash pip install -e \".[dev]\" Set up pre-commit hooks : bash pre-commit install","title":"Local Development Environment"},{"location":"development/contributing/#verify-setup","text":"Test that everything is working: # Run tests make test # Check code quality make lint # Run type checking make type-check","title":"Verify Setup"},{"location":"development/contributing/#project-structure","text":"genai-bench/ \u251c\u2500\u2500 genai_bench/ # Main package \u2502 \u251c\u2500\u2500 cli/ # Command-line interface \u2502 \u251c\u2500\u2500 metrics/ # Metrics collection and analysis \u2502 \u251c\u2500\u2500 sampling/ # Data sampling and tokenization \u2502 \u251c\u2500\u2500 scenarios/ # Traffic scenarios \u2502 \u251c\u2500\u2500 ui/ # Web UI components \u2502 \u251c\u2500\u2500 user/ # API user implementations \u2502 \u251c\u2500\u2500 auth/ # Authentication modules \u2502 \u251c\u2500\u2500 analysis/ # Result analysis tools \u2502 \u251c\u2500\u2500 distributed/ # Distributed benchmarking \u2502 \u2514\u2500\u2500 oci_object_storage/ # OCI integration \u251c\u2500\u2500 tests/ # Test suite \u251c\u2500\u2500 examples/ # Example scripts and configs \u251c\u2500\u2500 docs/ # Documentation \u251c\u2500\u2500 pyproject.toml # Project configuration \u251c\u2500\u2500 Makefile # Development commands \u2514\u2500\u2500 README.md # Project overview","title":"Project Structure"},{"location":"development/contributing/#development-workflow","text":"","title":"Development Workflow"},{"location":"development/contributing/#1-create-a-feature-branch","text":"git checkout -b feature/your-feature-name","title":"1. Create a Feature Branch"},{"location":"development/contributing/#2-make-your-changes","text":"Follow our coding standards and make your changes.","title":"2. Make Your Changes"},{"location":"development/contributing/#3-run-tests","text":"# Run all tests make test # Run specific test file pytest tests/test_specific_module.py # Run with coverage make test-cov","title":"3. Run Tests"},{"location":"development/contributing/#4-check-code-quality","text":"# Run all quality checks make lint # Run specific checks make ruff make black make isort make mypy","title":"4. Check Code Quality"},{"location":"development/contributing/#5-commit-your-changes","text":"git add . git commit -m \"feat: add new feature description\"","title":"5. Commit Your Changes"},{"location":"development/contributing/#6-push-and-create-pull-request","text":"git push origin feature/your-feature-name Then create a pull request on GitHub.","title":"6. Push and Create Pull Request"},{"location":"development/contributing/#coding-standards","text":"","title":"Coding Standards"},{"location":"development/contributing/#python-style-guide","text":"We follow PEP 8 with some modifications: Line length : 88 characters (Black default) Import sorting : Use isort with Black profile Type hints : Required for all public functions Docstrings : Use Google style docstrings","title":"Python Style Guide"},{"location":"development/contributing/#code-formatting","text":"We use automated tools for code formatting: # Format code make format # Check formatting make check-format","title":"Code Formatting"},{"location":"development/contributing/#type-checking","text":"We use MyPy for static type checking: # Run type checking make type-check # Type check specific file mypy genai_bench/your_module.py","title":"Type Checking"},{"location":"development/contributing/#linting","text":"We use Ruff for linting: # Run linter make lint # Fix auto-fixable issues make lint-fix","title":"Linting"},{"location":"development/contributing/#testing","text":"","title":"Testing"},{"location":"development/contributing/#writing-tests","text":"Place tests in the tests/ directory Use pytest as the testing framework Follow the naming convention: test_*.py Use descriptive test names","title":"Writing Tests"},{"location":"development/contributing/#test-structure","text":"import pytest from genai_bench.your_module import your_function def test_your_function_basic (): \"\"\"Test basic functionality.\"\"\" result = your_function ( \"input\" ) assert result == \"expected_output\" def test_your_function_edge_case (): \"\"\"Test edge case handling.\"\"\" with pytest . raises ( ValueError ): your_function ( \"\" ) @pytest . fixture def sample_data (): \"\"\"Provide test data.\"\"\" return { \"key\" : \"value\" } def test_your_function_with_fixture ( sample_data ): \"\"\"Test with fixture data.\"\"\" result = your_function ( sample_data ) assert result is not None","title":"Test Structure"},{"location":"development/contributing/#running-tests","text":"# Run all tests pytest # Run with verbose output pytest -v # Run specific test pytest tests/test_specific.py::test_function # Run with coverage pytest --cov = genai_bench # Run integration tests pytest tests/integration/","title":"Running Tests"},{"location":"development/contributing/#documentation","text":"","title":"Documentation"},{"location":"development/contributing/#code-documentation","text":"Use Google style docstrings for all public functions Include type hints for all parameters and return values Document exceptions that may be raised Example: def process_data ( data : str , config : Dict [ str , Any ]) -> List [ str ]: \"\"\"Process input data according to configuration. Args: data: Input string to process. config: Configuration dictionary. Returns: List of processed strings. Raises: ValueError: If data is empty or invalid. ConfigError: If configuration is invalid. \"\"\" if not data : raise ValueError ( \"Data cannot be empty\" ) # Implementation here return processed_data","title":"Code Documentation"},{"location":"development/contributing/#documentation-updates","text":"When adding new features: Update relevant documentation files in docs/ Add examples if applicable Update API documentation Ensure all links work correctly","title":"Documentation Updates"},{"location":"development/contributing/#pull-request-process","text":"","title":"Pull Request Process"},{"location":"development/contributing/#before-submitting","text":"Ensure tests pass : Run make test Check code quality : Run make lint Update documentation : Add/update relevant docs Add tests : Include tests for new functionality","title":"Before Submitting"},{"location":"development/contributing/#pull-request-template","text":"Use this template when creating a PR: ## Description Brief description of the changes. ## Type of Change - [ ] Bug fix (non-breaking change which fixes an issue) - [ ] New feature (non-breaking change which adds functionality) - [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected) - [ ] Documentation update ## Testing - [ ] I have added tests that prove my fix is effective or that my feature works - [ ] All existing tests pass - [ ] I have tested the changes manually ## Checklist - [ ] My code follows the style guidelines of this project - [ ] I have performed a self-review of my own code - [ ] I have commented my code, particularly in hard-to-understand areas - [ ] I have made corresponding changes to the documentation - [ ] My changes generate no new warnings - [ ] I have added tests that prove my fix is effective or that my feature works - [ ] New and existing unit tests pass locally with my changes","title":"Pull Request Template"},{"location":"development/contributing/#review-process","text":"Automated checks must pass Code review by maintainers Documentation review if applicable Testing verification if needed","title":"Review Process"},{"location":"development/contributing/#areas-for-contribution","text":"","title":"Areas for Contribution"},{"location":"development/contributing/#high-priority","text":"Performance improvements : Optimize existing functionality Bug fixes : Fix reported issues Documentation : Improve existing docs or add missing sections Test coverage : Add tests for uncovered code","title":"High Priority"},{"location":"development/contributing/#medium-priority","text":"New features : Add requested functionality API improvements : Enhance existing APIs UI enhancements : Improve web interface Integration : Add support for new backends","title":"Medium Priority"},{"location":"development/contributing/#low-priority","text":"Code refactoring : Improve code structure Tooling : Enhance development tools Examples : Add more example configurations","title":"Low Priority"},{"location":"development/contributing/#getting-help","text":"","title":"Getting Help"},{"location":"development/contributing/#communication-channels","text":"GitHub Issues : For bug reports and feature requests GitHub Discussions : For questions and general discussion Pull Requests : For code contributions","title":"Communication Channels"},{"location":"development/contributing/#before-asking-for-help","text":"Check existing issues : Search for similar problems Read documentation : Check relevant docs Try debugging : Use debug mode and logs Provide context : Include error messages and environment details","title":"Before Asking for Help"},{"location":"development/contributing/#creating-good-issues","text":"When reporting bugs or requesting features: Use clear titles : Descriptive and specific Provide context : Environment, version, steps to reproduce Include logs : Error messages and relevant output Add examples : Code snippets or configuration files","title":"Creating Good Issues"},{"location":"development/contributing/#release-process","text":"","title":"Release Process"},{"location":"development/contributing/#version-management","text":"We use semantic versioning (MAJOR.MINOR.PATCH): MAJOR : Breaking changes MINOR : New features, backward compatible PATCH : Bug fixes, backward compatible","title":"Version Management"},{"location":"development/contributing/#release-checklist","text":"Before a release: Update version : Update version in pyproject.toml Update changelog : Document changes Run full test suite : Ensure all tests pass Update documentation : Ensure docs are current Create release notes : Summarize changes","title":"Release Checklist"},{"location":"development/contributing/#code-of-conduct","text":"","title":"Code of Conduct"},{"location":"development/contributing/#our-standards","text":"Be respectful and inclusive Focus on constructive feedback Help others learn and grow Respect different viewpoints","title":"Our Standards"},{"location":"development/contributing/#enforcement","text":"Report violations to maintainers Maintainers will address issues promptly Violations may result in temporary or permanent exclusion","title":"Enforcement"},{"location":"development/contributing/#license","text":"By contributing to GenAI Bench, you agree that your contributions will be licensed under the MIT License.","title":"License"},{"location":"development/contributing/#acknowledgments","text":"Thank you for contributing to GenAI Bench! Your contributions help make this project better for everyone.","title":"Acknowledgments"},{"location":"development/contributing/#next-steps","text":"Read the Architecture Guide to understand the codebase Start with small contributions to understand the system better Join the community discussions for questions and ideas","title":"Next Steps"},{"location":"examples/basic-benchmarks/","text":"Basic Benchmarks \u00b6 This guide provides practical examples of basic benchmarks you can run with GenAI Bench to get started quickly. Quick Start Examples \u00b6 Example 1: Simple Text-to-Text Benchmark \u00b6 Run a basic chat completion benchmark: genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --num-users 2 \\ --max-time-per-run 60 \\ --dataset-name \"sonnet.txt\" What this does: - Connects to a local vLLM server - Runs a 2-user benchmark for 60 seconds - Uses the built-in sonnet dataset - Generates text completions Example 2: Embeddings Benchmark \u00b6 Test embedding generation performance: genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"text-embedding-ada-002\" \\ --task text-to-embeddings \\ --num-users 5 \\ --max-time-per-run 120 \\ --dataset-name \"sonnet.txt\" What this does: - Tests embedding generation - Uses 5 concurrent users - Runs for 2 minutes - Measures embedding latency and throughput Example 3: Vision Benchmark \u00b6 Test image-text understanding: genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"llava-1.5-7b\" \\ --task image-text-to-text \\ --num-users 1 \\ --max-time-per-run 180 \\ --dataset-name \"vision_dataset\" What this does: - Tests vision-language model - Uses single user (vision models are typically slower) - Runs for 3 minutes - Processes images with text prompts Production-Ready Examples \u00b6 Example 4: Load Testing \u00b6 Test system performance under load: genai-bench benchmark \\ --api-backend openai \\ --api-base \"https://your-production-api.com\" \\ --api-key \"your-production-key\" \\ --api-model-name \"gpt-4\" \\ --task text-to-text \\ --num-users 20 \\ --spawn-rate 5 \\ --max-time-per-run 600 \\ --max-requests-per-run 2000 \\ --dataset-name \"production_prompts.txt\" \\ --ui \\ --output-dir \"./production-benchmark\" What this does: - Tests production API endpoint - Gradually ramps up to 20 users - Runs for 10 minutes - Limits to 2000 requests - Enables UI dashboard - Saves results to specific directory Example 5: Model Comparison \u00b6 Compare multiple models: # Benchmark Model A genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --num-users 10 \\ --max-time-per-run 300 \\ --output-dir \"./experiments/llama-2-7b\" # Benchmark Model B genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"mistral-7b\" \\ --task text-to-text \\ --num-users 10 \\ --max-time-per-run 300 \\ --output-dir \"./experiments/mistral-7b\" # Generate comparison genai-bench plot \\ --experiment-dirs \"./experiments/llama-2-7b,./experiments/mistral-7b\" \\ --output-file \"model_comparison.png\" \\ --group-by \"api_model_name\" Example 6: Traffic Pattern Testing \u00b6 Test different traffic scenarios: # Constant load genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --traffic-scenario constant \\ --num-users 10 \\ --max-time-per-run 300 \\ --output-dir \"./experiments/constant\" # Burst traffic genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --traffic-scenario burst \\ --num-users 50 \\ --spawn-rate 10 \\ --max-time-per-run 300 \\ --output-dir \"./experiments/burst\" # Ramp-up traffic genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --traffic-scenario ramp \\ --num-users 20 \\ --spawn-rate 1 \\ --max-time-per-run 300 \\ --output-dir \"./experiments/ramp\" API-Specific Examples \u00b6 Example 7: OpenAI API \u00b6 Test against OpenAI's hosted API: genai-bench benchmark \\ --api-backend openai \\ --api-base \"https://api.openai.com/v1\" \\ --api-key \"sk-your-openai-key\" \\ --api-model-name \"gpt-3.5-turbo\" \\ --task text-to-text \\ --num-users 5 \\ --max-time-per-run 300 \\ --temperature 0 .7 \\ --max-tokens 100 Example 8: Cohere API \u00b6 Test against Cohere's API: genai-bench benchmark \\ --api-backend cohere \\ --api-base \"https://api.cohere.ai\" \\ --api-key \"your-cohere-key\" \\ --api-model-name \"command-r-plus\" \\ --task text-to-text \\ --num-users 5 \\ --max-time-per-run 300 Example 9: OCI Cohere \u00b6 Test against Oracle Cloud Infrastructure Cohere: genai-bench benchmark \\ --api-backend oci_cohere \\ --api-base \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\" \\ --api-key \"your-oci-key\" \\ --api-model-name \"cohere.command-r-plus\" \\ --task text-to-text \\ --num-users 5 \\ --max-time-per-run 300 Custom Dataset Examples \u00b6 Example 10: Custom Text Dataset \u00b6 Use your own prompts: # Create custom dataset cat > my_prompts.txt << EOF What is machine learning? Explain the benefits of cloud computing. Write a short story about a robot. How does blockchain technology work? EOF # Run benchmark with custom dataset genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --dataset-path \"./my_prompts.txt\" \\ --num-users 3 \\ --max-time-per-run 180 Example 11: Hugging Face Dataset \u00b6 Use a dataset from Hugging Face: genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --dataset-name \"squad\" \\ --dataset-split \"train\" \\ --dataset-column \"question\" \\ --num-users 5 \\ --max-time-per-run 300 Advanced Examples \u00b6 Example 12: Parameter Tuning \u00b6 Test different generation parameters: # Conservative generation genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --temperature 0 .1 \\ --max-tokens 50 \\ --top-p 0 .9 \\ --num-users 5 \\ --max-time-per-run 300 \\ --output-dir \"./experiments/conservative\" # Creative generation genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --temperature 0 .9 \\ --max-tokens 100 \\ --top-p 0 .95 \\ --num-users 5 \\ --max-time-per-run 300 \\ --output-dir \"./experiments/creative\" Example 13: Tokenizer Configuration \u00b6 Use specific tokenizer for accurate token counting: genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --model-tokenizer \"meta-llama/Llama-2-7b-chat-hf\" \\ --num-users 5 \\ --max-time-per-run 300 Example 14: Server Information \u00b6 Include server details for analysis: genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --server-engine \"vLLM\" \\ --server-gpu-type \"H100\" \\ --server-model-size \"7B\" \\ --server-batch-size 32 \\ --num-users 10 \\ --max-time-per-run 300 Analysis Examples \u00b6 Example 15: Generate Excel Report \u00b6 Create detailed Excel report: genai-bench excel \\ --experiment-dir \"./experiments/my-benchmark\" \\ --output-file \"detailed_report.xlsx\" \\ --include-raw-data \\ --include-pricing Example 16: Custom Plots \u00b6 Generate custom visualizations: # Create plot configuration cat > plot_config.yaml << EOF layout: rows: 2 cols: 2 plots: - title: \"Throughput vs Concurrency\" type: \"line\" x_axis: \"num_users\" y_axis: \"mean_total_tokens_throughput\" - title: \"Latency vs Concurrency\" type: \"line\" x_axis: \"num_users\" y_axis: \"mean_e2e_latency\" - title: \"Error Rate\" type: \"bar\" x_axis: \"num_users\" y_axis: \"error_rate\" - title: \"Token Distribution\" type: \"histogram\" x_axis: \"num_output_tokens\" bins: 20 EOF # Generate plots genai-bench plot \\ --experiment-dirs \"./experiments/model1,./experiments/model2\" \\ --output-file \"comparison.png\" \\ --plot-config \"plot_config.yaml\" \\ --group-by \"api_model_name\" Troubleshooting Examples \u00b6 Example 17: Debug Mode \u00b6 Run with detailed logging: genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --log-level DEBUG \\ --num-users 1 \\ --max-time-per-run 60 Example 18: Test Connection \u00b6 Test API connectivity: # Test with minimal configuration genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"test\" \\ --task text-to-text \\ --num-users 1 \\ --max-time-per-run 10 \\ --max-requests-per-run 1 Best Practices \u00b6 Before Running Benchmarks \u00b6 Test API Connection : Ensure your API endpoint is accessible Check Resources : Verify sufficient CPU, memory, and network capacity Start Small : Begin with low concurrency and short duration Monitor System : Watch system resources during benchmarks During Benchmarks \u00b6 Use UI Dashboard : Monitor real-time progress at http://localhost:8089 Check Logs : Monitor logs for errors or warnings Resource Monitoring : Watch CPU, memory, and network usage Error Analysis : Pay attention to error rates and types After Benchmarks \u00b6 Generate Reports : Create Excel reports for detailed analysis Create Plots : Visualize results for better understanding Compare Results : Compare with previous benchmarks Document Findings : Record configuration and results Next Steps \u00b6 Read the User Guide for detailed explanations Learn about Tasks and Benchmarks for different task types Check out Results Analysis for understanding your metrics Explore the CLI Reference for complete command documentation","title":"Basic Benchmarks"},{"location":"examples/basic-benchmarks/#basic-benchmarks","text":"This guide provides practical examples of basic benchmarks you can run with GenAI Bench to get started quickly.","title":"Basic Benchmarks"},{"location":"examples/basic-benchmarks/#quick-start-examples","text":"","title":"Quick Start Examples"},{"location":"examples/basic-benchmarks/#example-1-simple-text-to-text-benchmark","text":"Run a basic chat completion benchmark: genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --num-users 2 \\ --max-time-per-run 60 \\ --dataset-name \"sonnet.txt\" What this does: - Connects to a local vLLM server - Runs a 2-user benchmark for 60 seconds - Uses the built-in sonnet dataset - Generates text completions","title":"Example 1: Simple Text-to-Text Benchmark"},{"location":"examples/basic-benchmarks/#example-2-embeddings-benchmark","text":"Test embedding generation performance: genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"text-embedding-ada-002\" \\ --task text-to-embeddings \\ --num-users 5 \\ --max-time-per-run 120 \\ --dataset-name \"sonnet.txt\" What this does: - Tests embedding generation - Uses 5 concurrent users - Runs for 2 minutes - Measures embedding latency and throughput","title":"Example 2: Embeddings Benchmark"},{"location":"examples/basic-benchmarks/#example-3-vision-benchmark","text":"Test image-text understanding: genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"llava-1.5-7b\" \\ --task image-text-to-text \\ --num-users 1 \\ --max-time-per-run 180 \\ --dataset-name \"vision_dataset\" What this does: - Tests vision-language model - Uses single user (vision models are typically slower) - Runs for 3 minutes - Processes images with text prompts","title":"Example 3: Vision Benchmark"},{"location":"examples/basic-benchmarks/#production-ready-examples","text":"","title":"Production-Ready Examples"},{"location":"examples/basic-benchmarks/#example-4-load-testing","text":"Test system performance under load: genai-bench benchmark \\ --api-backend openai \\ --api-base \"https://your-production-api.com\" \\ --api-key \"your-production-key\" \\ --api-model-name \"gpt-4\" \\ --task text-to-text \\ --num-users 20 \\ --spawn-rate 5 \\ --max-time-per-run 600 \\ --max-requests-per-run 2000 \\ --dataset-name \"production_prompts.txt\" \\ --ui \\ --output-dir \"./production-benchmark\" What this does: - Tests production API endpoint - Gradually ramps up to 20 users - Runs for 10 minutes - Limits to 2000 requests - Enables UI dashboard - Saves results to specific directory","title":"Example 4: Load Testing"},{"location":"examples/basic-benchmarks/#example-5-model-comparison","text":"Compare multiple models: # Benchmark Model A genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --num-users 10 \\ --max-time-per-run 300 \\ --output-dir \"./experiments/llama-2-7b\" # Benchmark Model B genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"mistral-7b\" \\ --task text-to-text \\ --num-users 10 \\ --max-time-per-run 300 \\ --output-dir \"./experiments/mistral-7b\" # Generate comparison genai-bench plot \\ --experiment-dirs \"./experiments/llama-2-7b,./experiments/mistral-7b\" \\ --output-file \"model_comparison.png\" \\ --group-by \"api_model_name\"","title":"Example 5: Model Comparison"},{"location":"examples/basic-benchmarks/#example-6-traffic-pattern-testing","text":"Test different traffic scenarios: # Constant load genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --traffic-scenario constant \\ --num-users 10 \\ --max-time-per-run 300 \\ --output-dir \"./experiments/constant\" # Burst traffic genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --traffic-scenario burst \\ --num-users 50 \\ --spawn-rate 10 \\ --max-time-per-run 300 \\ --output-dir \"./experiments/burst\" # Ramp-up traffic genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --traffic-scenario ramp \\ --num-users 20 \\ --spawn-rate 1 \\ --max-time-per-run 300 \\ --output-dir \"./experiments/ramp\"","title":"Example 6: Traffic Pattern Testing"},{"location":"examples/basic-benchmarks/#api-specific-examples","text":"","title":"API-Specific Examples"},{"location":"examples/basic-benchmarks/#example-7-openai-api","text":"Test against OpenAI's hosted API: genai-bench benchmark \\ --api-backend openai \\ --api-base \"https://api.openai.com/v1\" \\ --api-key \"sk-your-openai-key\" \\ --api-model-name \"gpt-3.5-turbo\" \\ --task text-to-text \\ --num-users 5 \\ --max-time-per-run 300 \\ --temperature 0 .7 \\ --max-tokens 100","title":"Example 7: OpenAI API"},{"location":"examples/basic-benchmarks/#example-8-cohere-api","text":"Test against Cohere's API: genai-bench benchmark \\ --api-backend cohere \\ --api-base \"https://api.cohere.ai\" \\ --api-key \"your-cohere-key\" \\ --api-model-name \"command-r-plus\" \\ --task text-to-text \\ --num-users 5 \\ --max-time-per-run 300","title":"Example 8: Cohere API"},{"location":"examples/basic-benchmarks/#example-9-oci-cohere","text":"Test against Oracle Cloud Infrastructure Cohere: genai-bench benchmark \\ --api-backend oci_cohere \\ --api-base \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\" \\ --api-key \"your-oci-key\" \\ --api-model-name \"cohere.command-r-plus\" \\ --task text-to-text \\ --num-users 5 \\ --max-time-per-run 300","title":"Example 9: OCI Cohere"},{"location":"examples/basic-benchmarks/#custom-dataset-examples","text":"","title":"Custom Dataset Examples"},{"location":"examples/basic-benchmarks/#example-10-custom-text-dataset","text":"Use your own prompts: # Create custom dataset cat > my_prompts.txt << EOF What is machine learning? Explain the benefits of cloud computing. Write a short story about a robot. How does blockchain technology work? EOF # Run benchmark with custom dataset genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --dataset-path \"./my_prompts.txt\" \\ --num-users 3 \\ --max-time-per-run 180","title":"Example 10: Custom Text Dataset"},{"location":"examples/basic-benchmarks/#example-11-hugging-face-dataset","text":"Use a dataset from Hugging Face: genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --dataset-name \"squad\" \\ --dataset-split \"train\" \\ --dataset-column \"question\" \\ --num-users 5 \\ --max-time-per-run 300","title":"Example 11: Hugging Face Dataset"},{"location":"examples/basic-benchmarks/#advanced-examples","text":"","title":"Advanced Examples"},{"location":"examples/basic-benchmarks/#example-12-parameter-tuning","text":"Test different generation parameters: # Conservative generation genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --temperature 0 .1 \\ --max-tokens 50 \\ --top-p 0 .9 \\ --num-users 5 \\ --max-time-per-run 300 \\ --output-dir \"./experiments/conservative\" # Creative generation genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --temperature 0 .9 \\ --max-tokens 100 \\ --top-p 0 .95 \\ --num-users 5 \\ --max-time-per-run 300 \\ --output-dir \"./experiments/creative\"","title":"Example 12: Parameter Tuning"},{"location":"examples/basic-benchmarks/#example-13-tokenizer-configuration","text":"Use specific tokenizer for accurate token counting: genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --model-tokenizer \"meta-llama/Llama-2-7b-chat-hf\" \\ --num-users 5 \\ --max-time-per-run 300","title":"Example 13: Tokenizer Configuration"},{"location":"examples/basic-benchmarks/#example-14-server-information","text":"Include server details for analysis: genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --server-engine \"vLLM\" \\ --server-gpu-type \"H100\" \\ --server-model-size \"7B\" \\ --server-batch-size 32 \\ --num-users 10 \\ --max-time-per-run 300","title":"Example 14: Server Information"},{"location":"examples/basic-benchmarks/#analysis-examples","text":"","title":"Analysis Examples"},{"location":"examples/basic-benchmarks/#example-15-generate-excel-report","text":"Create detailed Excel report: genai-bench excel \\ --experiment-dir \"./experiments/my-benchmark\" \\ --output-file \"detailed_report.xlsx\" \\ --include-raw-data \\ --include-pricing","title":"Example 15: Generate Excel Report"},{"location":"examples/basic-benchmarks/#example-16-custom-plots","text":"Generate custom visualizations: # Create plot configuration cat > plot_config.yaml << EOF layout: rows: 2 cols: 2 plots: - title: \"Throughput vs Concurrency\" type: \"line\" x_axis: \"num_users\" y_axis: \"mean_total_tokens_throughput\" - title: \"Latency vs Concurrency\" type: \"line\" x_axis: \"num_users\" y_axis: \"mean_e2e_latency\" - title: \"Error Rate\" type: \"bar\" x_axis: \"num_users\" y_axis: \"error_rate\" - title: \"Token Distribution\" type: \"histogram\" x_axis: \"num_output_tokens\" bins: 20 EOF # Generate plots genai-bench plot \\ --experiment-dirs \"./experiments/model1,./experiments/model2\" \\ --output-file \"comparison.png\" \\ --plot-config \"plot_config.yaml\" \\ --group-by \"api_model_name\"","title":"Example 16: Custom Plots"},{"location":"examples/basic-benchmarks/#troubleshooting-examples","text":"","title":"Troubleshooting Examples"},{"location":"examples/basic-benchmarks/#example-17-debug-mode","text":"Run with detailed logging: genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --log-level DEBUG \\ --num-users 1 \\ --max-time-per-run 60","title":"Example 17: Debug Mode"},{"location":"examples/basic-benchmarks/#example-18-test-connection","text":"Test API connectivity: # Test with minimal configuration genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"test\" \\ --task text-to-text \\ --num-users 1 \\ --max-time-per-run 10 \\ --max-requests-per-run 1","title":"Example 18: Test Connection"},{"location":"examples/basic-benchmarks/#best-practices","text":"","title":"Best Practices"},{"location":"examples/basic-benchmarks/#before-running-benchmarks","text":"Test API Connection : Ensure your API endpoint is accessible Check Resources : Verify sufficient CPU, memory, and network capacity Start Small : Begin with low concurrency and short duration Monitor System : Watch system resources during benchmarks","title":"Before Running Benchmarks"},{"location":"examples/basic-benchmarks/#during-benchmarks","text":"Use UI Dashboard : Monitor real-time progress at http://localhost:8089 Check Logs : Monitor logs for errors or warnings Resource Monitoring : Watch CPU, memory, and network usage Error Analysis : Pay attention to error rates and types","title":"During Benchmarks"},{"location":"examples/basic-benchmarks/#after-benchmarks","text":"Generate Reports : Create Excel reports for detailed analysis Create Plots : Visualize results for better understanding Compare Results : Compare with previous benchmarks Document Findings : Record configuration and results","title":"After Benchmarks"},{"location":"examples/basic-benchmarks/#next-steps","text":"Read the User Guide for detailed explanations Learn about Tasks and Benchmarks for different task types Check out Results Analysis for understanding your metrics Explore the CLI Reference for complete command documentation","title":"Next Steps"},{"location":"getting-started/configuration/","text":"Configuration Guide \u00b6 This guide explains how to configure GenAI Bench for different environments, API backends, and use cases. Environment Configuration \u00b6 Python Environment \u00b6 Ensure you have the correct Python version and dependencies: # Check Python version python3 --version # Should be 3.11 or 3.12 # Install in virtual environment (recommended) python3 -m venv genai-bench-env source genai-bench-env/bin/activate pip install genai-bench Environment Variables \u00b6 Set these environment variables for optimal performance: # Hugging Face token (for tokenizer downloads) export HF_TOKEN = \"your-huggingface-token\" # Disable torch warnings (not needed for benchmarking) export TRANSFORMERS_VERBOSITY = error # Log level export GENAI_BENCH_LOG_LEVEL = INFO # API keys (set as needed) export OPENAI_API_KEY = \"your-openai-key\" export COHERE_API_KEY = \"your-cohere-key\" API Backend Configuration \u00b6 OpenAI-Compatible Backends \u00b6 Configure for OpenAI API, vLLM, or other OpenAI-compatible services: # Basic configuration --api-backend openai --api-base \"https://api.openai.com/v1\" --api-key \"sk-...\" # vLLM configuration --api-backend openai --api-base \"http://localhost:8082\" --api-key \"your-vllm-key\" --api-model-name \"llama-2-7b\" Configuration Options \u00b6 Option Description Example --api-base API endpoint URL https://api.openai.com/v1 --api-key Authentication key sk-... --api-model-name Model identifier gpt-3.5-turbo --api-version API version 2024-02-15 Cohere Backend \u00b6 Configure for Cohere API: --api-backend cohere --api-base \"https://api.cohere.ai\" --api-key \"your-cohere-key\" --api-model-name \"command-r-plus\" Cohere-Specific Options \u00b6 Option Description Example --cohere-embedding-model Embedding model embed-english-v3.0 --cohere-rerank-model Rerank model rerank-english-v2.0 OCI Cohere Backend \u00b6 Configure for Oracle Cloud Infrastructure Cohere: --api-backend oci_cohere --api-base \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\" --api-key \"your-oci-key\" --api-model-name \"cohere.command-r-plus\" OCI Configuration \u00b6 Set up OCI CLI : bash oci setup config Configure authentication : bash export OCI_CONFIG_FILE=\"~/.oci/config\" export OCI_KEY_FILE=\"~/.oci/oci_api_key.pem\" Model Configuration \u00b6 Model Selection \u00b6 Choose the appropriate model for your task: Text-to-Text Models \u00b6 # OpenAI models --api-model-name \"gpt-3.5-turbo\" --api-model-name \"gpt-4\" --api-model-name \"gpt-4-turbo\" # vLLM models --api-model-name \"llama-2-7b\" --api-model-name \"mistral-7b\" --api-model-name \"codellama-7b\" # Cohere models --api-model-name \"command-r-plus\" --api-model-name \"command-light\" Embedding Models \u00b6 # OpenAI embeddings --api-model-name \"text-embedding-ada-002\" --api-model-name \"text-embedding-3-small\" # Cohere embeddings --api-model-name \"embed-english-v3.0\" --api-model-name \"embed-multilingual-v3.0\" Vision Models \u00b6 # OpenAI vision models --api-model-name \"gpt-4-vision-preview\" # Other vision models --api-model-name \"llava-1.5-7b\" --api-model-name \"qwen-vl-7b\" Tokenizer Configuration \u00b6 For accurate token counting, specify the tokenizer: # Use model's tokenizer --model-tokenizer \"/path/to/tokenizer\" # Use Hugging Face tokenizer --model-tokenizer \"meta-llama/Llama-2-7b-chat-hf\" # Use specific tokenizer --model-tokenizer \"gpt2\" --model-tokenizer \"cl100k_base\" Dataset Configuration \u00b6 Built-in Datasets \u00b6 Use GenAI Bench's built-in datasets: # Text datasets --dataset-name \"sonnet.txt\" --dataset-name \"qa_dataset.txt\" # Vision datasets --dataset-name \"vision_dataset\" --dataset-name \"image_qa_dataset\" Custom Datasets \u00b6 Text Datasets \u00b6 Create a text file with one prompt per line: # prompts.txt What is the capital of France? Explain quantum computing in simple terms. Write a short poem about spring. --dataset-path \"/path/to/prompts.txt\" Vision Datasets \u00b6 Organize your vision dataset: vision_dataset/ \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 image1.jpg \u2502 \u251c\u2500\u2500 image2.png \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 prompts.txt \u2514\u2500\u2500 metadata.json --dataset-path \"/path/to/vision_dataset\" Hugging Face Datasets \u00b6 Use datasets from Hugging Face: --dataset-name \"squad\" --dataset-split \"train\" --dataset-column \"question\" Traffic Configuration \u00b6 Traffic Scenarios \u00b6 Choose the appropriate traffic pattern: Constant Load \u00b6 --traffic-scenario constant --num-users 10 --spawn-rate 2 Burst Traffic \u00b6 --traffic-scenario burst --num-users 50 --spawn-rate 10 --burst-duration 30 Ramp-up Traffic \u00b6 --traffic-scenario ramp --num-users 20 --spawn-rate 1 --ramp-duration 300 Load Parameters \u00b6 Configure load testing parameters: # Concurrency --num-users 10 # Number of concurrent users --spawn-rate 2 # Users spawned per second # Duration --max-time-per-run 300 # Maximum run time (seconds) --max-requests-per-run 1000 # Maximum requests per run # Limits --max-requests-per-user 100 # Requests per user --max-time-per-request 60 # Timeout per request Advanced Configuration \u00b6 Request Parameters \u00b6 Customize request behavior: Text-to-Text Parameters \u00b6 # Generation parameters --temperature 0 .7 --max-tokens 100 --top-p 0 .9 --frequency-penalty 0 .1 --presence-penalty 0 .1 # Stop sequences --stop-sequences \"END,STOP\" Embedding Parameters \u00b6 # Embedding options --embedding-dimensions 1536 --normalize-embeddings true --truncate-embeddings \"NONE\" Server Information \u00b6 Provide server details for analysis: # Server configuration --server-engine \"vLLM\" --server-gpu-type \"H100\" --server-model-size \"7B\" --server-batch-size 32 Monitoring Configuration \u00b6 Configure monitoring and logging: # UI dashboard --ui --ui-host \"0.0.0.0\" --ui-port 8089 # Logging --log-level INFO --log-file \"benchmark.log\" # Output --output-dir \"./experiments\" --save-raw-data true Configuration Files \u00b6 YAML Configuration \u00b6 Create a configuration file for complex setups: # config.yaml api : backend : openai base : \"http://localhost:8082\" key : \"your-api-key\" model : \"llama-2-7b\" task : type : text-to-text temperature : 0.7 max_tokens : 100 traffic : scenario : constant num_users : 10 spawn_rate : 2 max_time : 300 dataset : name : \"sonnet.txt\" tokenizer : \"meta-llama/Llama-2-7b-chat-hf\" monitoring : ui : true log_level : INFO output_dir : \"./experiments\" Use the configuration file: genai-bench benchmark --config config.yaml Environment-Specific Configs \u00b6 Create different configurations for different environments: Development Configuration \u00b6 # dev-config.yaml api : backend : openai base : \"http://localhost:8082\" model : \"llama-2-7b\" traffic : num_users : 2 max_time : 60 monitoring : log_level : DEBUG Production Configuration \u00b6 # prod-config.yaml api : backend : openai base : \"https://api.openai.com/v1\" model : \"gpt-4\" traffic : num_users : 50 max_time : 1800 monitoring : ui : false log_level : INFO Security Configuration \u00b6 API Key Management \u00b6 Secure your API keys: # Use environment variables (recommended) export OPENAI_API_KEY = \"your-key\" export COHERE_API_KEY = \"your-key\" # Use key files --api-key-file \"/path/to/api-key.txt\" # Use key management services --api-key-vault \"azure-keyvault://your-vault\" Network Security \u00b6 Configure network settings: # Proxy configuration --proxy \"http://proxy.company.com:8080\" --proxy-auth \"user:pass\" # SSL verification --verify-ssl true --ca-cert \"/path/to/ca-cert.pem\" # Timeout settings --connect-timeout 30 --read-timeout 60 Performance Tuning \u00b6 Resource Configuration \u00b6 Optimize for your hardware: # Memory settings --max-memory \"8GB\" --memory-fraction 0 .8 # CPU settings --num-workers 4 --worker-class \"gevent\" # Network settings --connection-pool-size 100 --max-connections 1000 Benchmark Optimization \u00b6 # Sampling optimization --sample-size 1000 --sample-strategy \"random\" # Metrics collection --metrics-interval 1 --detailed-metrics true # Result processing --compress-results true --save-intermediate true Troubleshooting Configuration \u00b6 Common Issues \u00b6 API Connection Issues \u00b6 # Test API connection curl -X POST \"http://localhost:8082/v1/chat/completions\" \\ -H \"Authorization: Bearer your-key\" \\ -H \"Content-Type: application/json\" \\ -d '{\"model\":\"test\",\"messages\":[{\"role\":\"user\",\"content\":\"test\"}]}' Tokenizer Issues \u00b6 # Test tokenizer python -c \" from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf') print(tokenizer.encode('test')) \" Dataset Issues \u00b6 # Validate dataset genai-bench validate-dataset --dataset-path \"/path/to/dataset\" Configuration Validation \u00b6 # Validate configuration genai-bench validate-config --config config.yaml # Test configuration genai-bench test-config --config config.yaml Best Practices \u00b6 Configuration Management \u00b6 Use Version Control : Track configuration changes Environment Separation : Use different configs for dev/prod Sensitive Data : Never commit API keys to version control Documentation : Document configuration decisions Performance Optimization \u00b6 Start Simple : Begin with basic configurations Gradual Scaling : Increase complexity step by step Monitor Resources : Watch CPU, memory, and network usage Test Thoroughly : Validate configurations before production use Security Considerations \u00b6 API Key Rotation : Regularly rotate API keys Network Security : Use secure connections and firewalls Access Control : Limit access to configuration files Audit Logging : Log configuration changes Next Steps \u00b6 Read the Quick Start Guide to run your first benchmark Explore Tasks and Benchmarks for specific scenarios Check out Examples for practical configurations","title":"Configuration"},{"location":"getting-started/configuration/#configuration-guide","text":"This guide explains how to configure GenAI Bench for different environments, API backends, and use cases.","title":"Configuration Guide"},{"location":"getting-started/configuration/#environment-configuration","text":"","title":"Environment Configuration"},{"location":"getting-started/configuration/#python-environment","text":"Ensure you have the correct Python version and dependencies: # Check Python version python3 --version # Should be 3.11 or 3.12 # Install in virtual environment (recommended) python3 -m venv genai-bench-env source genai-bench-env/bin/activate pip install genai-bench","title":"Python Environment"},{"location":"getting-started/configuration/#environment-variables","text":"Set these environment variables for optimal performance: # Hugging Face token (for tokenizer downloads) export HF_TOKEN = \"your-huggingface-token\" # Disable torch warnings (not needed for benchmarking) export TRANSFORMERS_VERBOSITY = error # Log level export GENAI_BENCH_LOG_LEVEL = INFO # API keys (set as needed) export OPENAI_API_KEY = \"your-openai-key\" export COHERE_API_KEY = \"your-cohere-key\"","title":"Environment Variables"},{"location":"getting-started/configuration/#api-backend-configuration","text":"","title":"API Backend Configuration"},{"location":"getting-started/configuration/#openai-compatible-backends","text":"Configure for OpenAI API, vLLM, or other OpenAI-compatible services: # Basic configuration --api-backend openai --api-base \"https://api.openai.com/v1\" --api-key \"sk-...\" # vLLM configuration --api-backend openai --api-base \"http://localhost:8082\" --api-key \"your-vllm-key\" --api-model-name \"llama-2-7b\"","title":"OpenAI-Compatible Backends"},{"location":"getting-started/configuration/#configuration-options","text":"Option Description Example --api-base API endpoint URL https://api.openai.com/v1 --api-key Authentication key sk-... --api-model-name Model identifier gpt-3.5-turbo --api-version API version 2024-02-15","title":"Configuration Options"},{"location":"getting-started/configuration/#cohere-backend","text":"Configure for Cohere API: --api-backend cohere --api-base \"https://api.cohere.ai\" --api-key \"your-cohere-key\" --api-model-name \"command-r-plus\"","title":"Cohere Backend"},{"location":"getting-started/configuration/#cohere-specific-options","text":"Option Description Example --cohere-embedding-model Embedding model embed-english-v3.0 --cohere-rerank-model Rerank model rerank-english-v2.0","title":"Cohere-Specific Options"},{"location":"getting-started/configuration/#oci-cohere-backend","text":"Configure for Oracle Cloud Infrastructure Cohere: --api-backend oci_cohere --api-base \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\" --api-key \"your-oci-key\" --api-model-name \"cohere.command-r-plus\"","title":"OCI Cohere Backend"},{"location":"getting-started/configuration/#oci-configuration","text":"Set up OCI CLI : bash oci setup config Configure authentication : bash export OCI_CONFIG_FILE=\"~/.oci/config\" export OCI_KEY_FILE=\"~/.oci/oci_api_key.pem\"","title":"OCI Configuration"},{"location":"getting-started/configuration/#model-configuration","text":"","title":"Model Configuration"},{"location":"getting-started/configuration/#model-selection","text":"Choose the appropriate model for your task:","title":"Model Selection"},{"location":"getting-started/configuration/#text-to-text-models","text":"# OpenAI models --api-model-name \"gpt-3.5-turbo\" --api-model-name \"gpt-4\" --api-model-name \"gpt-4-turbo\" # vLLM models --api-model-name \"llama-2-7b\" --api-model-name \"mistral-7b\" --api-model-name \"codellama-7b\" # Cohere models --api-model-name \"command-r-plus\" --api-model-name \"command-light\"","title":"Text-to-Text Models"},{"location":"getting-started/configuration/#embedding-models","text":"# OpenAI embeddings --api-model-name \"text-embedding-ada-002\" --api-model-name \"text-embedding-3-small\" # Cohere embeddings --api-model-name \"embed-english-v3.0\" --api-model-name \"embed-multilingual-v3.0\"","title":"Embedding Models"},{"location":"getting-started/configuration/#vision-models","text":"# OpenAI vision models --api-model-name \"gpt-4-vision-preview\" # Other vision models --api-model-name \"llava-1.5-7b\" --api-model-name \"qwen-vl-7b\"","title":"Vision Models"},{"location":"getting-started/configuration/#tokenizer-configuration","text":"For accurate token counting, specify the tokenizer: # Use model's tokenizer --model-tokenizer \"/path/to/tokenizer\" # Use Hugging Face tokenizer --model-tokenizer \"meta-llama/Llama-2-7b-chat-hf\" # Use specific tokenizer --model-tokenizer \"gpt2\" --model-tokenizer \"cl100k_base\"","title":"Tokenizer Configuration"},{"location":"getting-started/configuration/#dataset-configuration","text":"","title":"Dataset Configuration"},{"location":"getting-started/configuration/#built-in-datasets","text":"Use GenAI Bench's built-in datasets: # Text datasets --dataset-name \"sonnet.txt\" --dataset-name \"qa_dataset.txt\" # Vision datasets --dataset-name \"vision_dataset\" --dataset-name \"image_qa_dataset\"","title":"Built-in Datasets"},{"location":"getting-started/configuration/#custom-datasets","text":"","title":"Custom Datasets"},{"location":"getting-started/configuration/#text-datasets","text":"Create a text file with one prompt per line: # prompts.txt What is the capital of France? Explain quantum computing in simple terms. Write a short poem about spring. --dataset-path \"/path/to/prompts.txt\"","title":"Text Datasets"},{"location":"getting-started/configuration/#vision-datasets","text":"Organize your vision dataset: vision_dataset/ \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 image1.jpg \u2502 \u251c\u2500\u2500 image2.png \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 prompts.txt \u2514\u2500\u2500 metadata.json --dataset-path \"/path/to/vision_dataset\"","title":"Vision Datasets"},{"location":"getting-started/configuration/#hugging-face-datasets","text":"Use datasets from Hugging Face: --dataset-name \"squad\" --dataset-split \"train\" --dataset-column \"question\"","title":"Hugging Face Datasets"},{"location":"getting-started/configuration/#traffic-configuration","text":"","title":"Traffic Configuration"},{"location":"getting-started/configuration/#traffic-scenarios","text":"Choose the appropriate traffic pattern:","title":"Traffic Scenarios"},{"location":"getting-started/configuration/#constant-load","text":"--traffic-scenario constant --num-users 10 --spawn-rate 2","title":"Constant Load"},{"location":"getting-started/configuration/#burst-traffic","text":"--traffic-scenario burst --num-users 50 --spawn-rate 10 --burst-duration 30","title":"Burst Traffic"},{"location":"getting-started/configuration/#ramp-up-traffic","text":"--traffic-scenario ramp --num-users 20 --spawn-rate 1 --ramp-duration 300","title":"Ramp-up Traffic"},{"location":"getting-started/configuration/#load-parameters","text":"Configure load testing parameters: # Concurrency --num-users 10 # Number of concurrent users --spawn-rate 2 # Users spawned per second # Duration --max-time-per-run 300 # Maximum run time (seconds) --max-requests-per-run 1000 # Maximum requests per run # Limits --max-requests-per-user 100 # Requests per user --max-time-per-request 60 # Timeout per request","title":"Load Parameters"},{"location":"getting-started/configuration/#advanced-configuration","text":"","title":"Advanced Configuration"},{"location":"getting-started/configuration/#request-parameters","text":"Customize request behavior:","title":"Request Parameters"},{"location":"getting-started/configuration/#text-to-text-parameters","text":"# Generation parameters --temperature 0 .7 --max-tokens 100 --top-p 0 .9 --frequency-penalty 0 .1 --presence-penalty 0 .1 # Stop sequences --stop-sequences \"END,STOP\"","title":"Text-to-Text Parameters"},{"location":"getting-started/configuration/#embedding-parameters","text":"# Embedding options --embedding-dimensions 1536 --normalize-embeddings true --truncate-embeddings \"NONE\"","title":"Embedding Parameters"},{"location":"getting-started/configuration/#server-information","text":"Provide server details for analysis: # Server configuration --server-engine \"vLLM\" --server-gpu-type \"H100\" --server-model-size \"7B\" --server-batch-size 32","title":"Server Information"},{"location":"getting-started/configuration/#monitoring-configuration","text":"Configure monitoring and logging: # UI dashboard --ui --ui-host \"0.0.0.0\" --ui-port 8089 # Logging --log-level INFO --log-file \"benchmark.log\" # Output --output-dir \"./experiments\" --save-raw-data true","title":"Monitoring Configuration"},{"location":"getting-started/configuration/#configuration-files","text":"","title":"Configuration Files"},{"location":"getting-started/configuration/#yaml-configuration","text":"Create a configuration file for complex setups: # config.yaml api : backend : openai base : \"http://localhost:8082\" key : \"your-api-key\" model : \"llama-2-7b\" task : type : text-to-text temperature : 0.7 max_tokens : 100 traffic : scenario : constant num_users : 10 spawn_rate : 2 max_time : 300 dataset : name : \"sonnet.txt\" tokenizer : \"meta-llama/Llama-2-7b-chat-hf\" monitoring : ui : true log_level : INFO output_dir : \"./experiments\" Use the configuration file: genai-bench benchmark --config config.yaml","title":"YAML Configuration"},{"location":"getting-started/configuration/#environment-specific-configs","text":"Create different configurations for different environments:","title":"Environment-Specific Configs"},{"location":"getting-started/configuration/#development-configuration","text":"# dev-config.yaml api : backend : openai base : \"http://localhost:8082\" model : \"llama-2-7b\" traffic : num_users : 2 max_time : 60 monitoring : log_level : DEBUG","title":"Development Configuration"},{"location":"getting-started/configuration/#production-configuration","text":"# prod-config.yaml api : backend : openai base : \"https://api.openai.com/v1\" model : \"gpt-4\" traffic : num_users : 50 max_time : 1800 monitoring : ui : false log_level : INFO","title":"Production Configuration"},{"location":"getting-started/configuration/#security-configuration","text":"","title":"Security Configuration"},{"location":"getting-started/configuration/#api-key-management","text":"Secure your API keys: # Use environment variables (recommended) export OPENAI_API_KEY = \"your-key\" export COHERE_API_KEY = \"your-key\" # Use key files --api-key-file \"/path/to/api-key.txt\" # Use key management services --api-key-vault \"azure-keyvault://your-vault\"","title":"API Key Management"},{"location":"getting-started/configuration/#network-security","text":"Configure network settings: # Proxy configuration --proxy \"http://proxy.company.com:8080\" --proxy-auth \"user:pass\" # SSL verification --verify-ssl true --ca-cert \"/path/to/ca-cert.pem\" # Timeout settings --connect-timeout 30 --read-timeout 60","title":"Network Security"},{"location":"getting-started/configuration/#performance-tuning","text":"","title":"Performance Tuning"},{"location":"getting-started/configuration/#resource-configuration","text":"Optimize for your hardware: # Memory settings --max-memory \"8GB\" --memory-fraction 0 .8 # CPU settings --num-workers 4 --worker-class \"gevent\" # Network settings --connection-pool-size 100 --max-connections 1000","title":"Resource Configuration"},{"location":"getting-started/configuration/#benchmark-optimization","text":"# Sampling optimization --sample-size 1000 --sample-strategy \"random\" # Metrics collection --metrics-interval 1 --detailed-metrics true # Result processing --compress-results true --save-intermediate true","title":"Benchmark Optimization"},{"location":"getting-started/configuration/#troubleshooting-configuration","text":"","title":"Troubleshooting Configuration"},{"location":"getting-started/configuration/#common-issues","text":"","title":"Common Issues"},{"location":"getting-started/configuration/#api-connection-issues","text":"# Test API connection curl -X POST \"http://localhost:8082/v1/chat/completions\" \\ -H \"Authorization: Bearer your-key\" \\ -H \"Content-Type: application/json\" \\ -d '{\"model\":\"test\",\"messages\":[{\"role\":\"user\",\"content\":\"test\"}]}'","title":"API Connection Issues"},{"location":"getting-started/configuration/#tokenizer-issues","text":"# Test tokenizer python -c \" from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf') print(tokenizer.encode('test')) \"","title":"Tokenizer Issues"},{"location":"getting-started/configuration/#dataset-issues","text":"# Validate dataset genai-bench validate-dataset --dataset-path \"/path/to/dataset\"","title":"Dataset Issues"},{"location":"getting-started/configuration/#configuration-validation","text":"# Validate configuration genai-bench validate-config --config config.yaml # Test configuration genai-bench test-config --config config.yaml","title":"Configuration Validation"},{"location":"getting-started/configuration/#best-practices","text":"","title":"Best Practices"},{"location":"getting-started/configuration/#configuration-management","text":"Use Version Control : Track configuration changes Environment Separation : Use different configs for dev/prod Sensitive Data : Never commit API keys to version control Documentation : Document configuration decisions","title":"Configuration Management"},{"location":"getting-started/configuration/#performance-optimization","text":"Start Simple : Begin with basic configurations Gradual Scaling : Increase complexity step by step Monitor Resources : Watch CPU, memory, and network usage Test Thoroughly : Validate configurations before production use","title":"Performance Optimization"},{"location":"getting-started/configuration/#security-considerations","text":"API Key Rotation : Regularly rotate API keys Network Security : Use secure connections and firewalls Access Control : Limit access to configuration files Audit Logging : Log configuration changes","title":"Security Considerations"},{"location":"getting-started/configuration/#next-steps","text":"Read the Quick Start Guide to run your first benchmark Explore Tasks and Benchmarks for specific scenarios Check out Examples for practical configurations","title":"Next Steps"},{"location":"getting-started/installation/","text":"Installation Guide \u00b6 This guide covers all the ways to install GenAI Bench, from simple PyPI installation to full development setup. System Requirements \u00b6 Python Version \u00b6 Required : Python 3.11 or 3.12 Recommended : Python 3.12 Operating Systems \u00b6 Linux : Ubuntu 20.04+, CentOS 8+, RHEL 8+ macOS : 10.15+ (Catalina) Windows : Windows 10+ (with WSL2 recommended) Hardware Requirements \u00b6 Minimum : 4GB RAM, 2 CPU cores Recommended : 8GB+ RAM, 4+ CPU cores For large benchmarks : 16GB+ RAM, 8+ CPU cores Installation Methods \u00b6 Method 1: PyPI Installation (Recommended) \u00b6 The simplest way to install GenAI Bench: pip install genai-bench For a specific version: pip install genai-bench == 0 .1.75 Method 2: Development Installation \u00b6 For development or to use the latest features: Prerequisites \u00b6 Install Python 3.11+ ```bash # Ubuntu/Debian sudo apt update sudo apt install python3.11 python3.11-venv python3.11-pip # macOS (using Homebrew) brew install python@3.11 # Windows (download from python.org) ``` Install uv (recommended package manager) bash curl -LsSf https://astral.sh/uv/install.sh | sh Installation Steps \u00b6 # Clone the repository git clone https://github.com/sgl-project/genai-bench.git cd genai-bench # Create virtual environment and install dependencies make uv # Activate virtual environment source .venv/bin/activate # Install in editable mode make install Method 3: Docker Installation \u00b6 For containerized environments: # Pull the official image docker pull ghcr.io/sgl-project/genai-bench:latest # Run a benchmark docker run -it --rm ghcr.io/sgl-project/genai-bench:latest \\ genai-bench benchmark --help Verification \u00b6 After installation, verify that GenAI Bench is working: # Check version genai-bench --version # Check help genai-bench --help # Check benchmark command genai-bench benchmark --help Environment Setup \u00b6 Environment Variables \u00b6 Set these environment variables for optimal performance: # For Hugging Face tokenizer downloads export HF_TOKEN = \"your-huggingface-token\" # Disable torch warnings (not needed for benchmarking) export TRANSFORMERS_VERBOSITY = error # Optional: Set log level export GENAI_BENCH_LOG_LEVEL = INFO API Keys \u00b6 Depending on your backend, you may need API keys: # OpenAI-compatible APIs export OPENAI_API_KEY = \"your-api-key\" # Cohere API export COHERE_API_KEY = \"your-cohere-key\" # OCI Cohere export OCI_CONFIG_FILE = \"~/.oci/config\" Troubleshooting \u00b6 Common Issues \u00b6 Python Version Issues \u00b6 # Check Python version python3 --version # If you have multiple Python versions, use specific version python3.11 -m pip install genai-bench Permission Issues \u00b6 # Use user installation pip install --user genai-bench # Or use virtual environment python3 -m venv genai-bench-env source genai-bench-env/bin/activate pip install genai-bench Missing Dependencies \u00b6 # Update pip pip install --upgrade pip # Install with all dependencies pip install genai-bench [ dev ] Network Issues \u00b6 # Use alternative PyPI mirrors pip install -i https://pypi.tuna.tsinghua.edu.cn/simple/ genai-bench # Or use conda conda install -c conda-forge genai-bench Getting Help \u00b6 If you encounter issues: Check the GitHub Issues Search for similar problems Create a new issue with: Your operating system and Python version Installation method used Full error message Steps to reproduce Next Steps \u00b6 After successful installation: Read the Quick Start Guide to run your first benchmark Explore the User Guide for detailed usage Check out Examples for practical scenarios","title":"Installation"},{"location":"getting-started/installation/#installation-guide","text":"This guide covers all the ways to install GenAI Bench, from simple PyPI installation to full development setup.","title":"Installation Guide"},{"location":"getting-started/installation/#system-requirements","text":"","title":"System Requirements"},{"location":"getting-started/installation/#python-version","text":"Required : Python 3.11 or 3.12 Recommended : Python 3.12","title":"Python Version"},{"location":"getting-started/installation/#operating-systems","text":"Linux : Ubuntu 20.04+, CentOS 8+, RHEL 8+ macOS : 10.15+ (Catalina) Windows : Windows 10+ (with WSL2 recommended)","title":"Operating Systems"},{"location":"getting-started/installation/#hardware-requirements","text":"Minimum : 4GB RAM, 2 CPU cores Recommended : 8GB+ RAM, 4+ CPU cores For large benchmarks : 16GB+ RAM, 8+ CPU cores","title":"Hardware Requirements"},{"location":"getting-started/installation/#installation-methods","text":"","title":"Installation Methods"},{"location":"getting-started/installation/#method-1-pypi-installation-recommended","text":"The simplest way to install GenAI Bench: pip install genai-bench For a specific version: pip install genai-bench == 0 .1.75","title":"Method 1: PyPI Installation (Recommended)"},{"location":"getting-started/installation/#method-2-development-installation","text":"For development or to use the latest features:","title":"Method 2: Development Installation"},{"location":"getting-started/installation/#prerequisites","text":"Install Python 3.11+ ```bash # Ubuntu/Debian sudo apt update sudo apt install python3.11 python3.11-venv python3.11-pip # macOS (using Homebrew) brew install python@3.11 # Windows (download from python.org) ``` Install uv (recommended package manager) bash curl -LsSf https://astral.sh/uv/install.sh | sh","title":"Prerequisites"},{"location":"getting-started/installation/#installation-steps","text":"# Clone the repository git clone https://github.com/sgl-project/genai-bench.git cd genai-bench # Create virtual environment and install dependencies make uv # Activate virtual environment source .venv/bin/activate # Install in editable mode make install","title":"Installation Steps"},{"location":"getting-started/installation/#method-3-docker-installation","text":"For containerized environments: # Pull the official image docker pull ghcr.io/sgl-project/genai-bench:latest # Run a benchmark docker run -it --rm ghcr.io/sgl-project/genai-bench:latest \\ genai-bench benchmark --help","title":"Method 3: Docker Installation"},{"location":"getting-started/installation/#verification","text":"After installation, verify that GenAI Bench is working: # Check version genai-bench --version # Check help genai-bench --help # Check benchmark command genai-bench benchmark --help","title":"Verification"},{"location":"getting-started/installation/#environment-setup","text":"","title":"Environment Setup"},{"location":"getting-started/installation/#environment-variables","text":"Set these environment variables for optimal performance: # For Hugging Face tokenizer downloads export HF_TOKEN = \"your-huggingface-token\" # Disable torch warnings (not needed for benchmarking) export TRANSFORMERS_VERBOSITY = error # Optional: Set log level export GENAI_BENCH_LOG_LEVEL = INFO","title":"Environment Variables"},{"location":"getting-started/installation/#api-keys","text":"Depending on your backend, you may need API keys: # OpenAI-compatible APIs export OPENAI_API_KEY = \"your-api-key\" # Cohere API export COHERE_API_KEY = \"your-cohere-key\" # OCI Cohere export OCI_CONFIG_FILE = \"~/.oci/config\"","title":"API Keys"},{"location":"getting-started/installation/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"getting-started/installation/#common-issues","text":"","title":"Common Issues"},{"location":"getting-started/installation/#python-version-issues","text":"# Check Python version python3 --version # If you have multiple Python versions, use specific version python3.11 -m pip install genai-bench","title":"Python Version Issues"},{"location":"getting-started/installation/#permission-issues","text":"# Use user installation pip install --user genai-bench # Or use virtual environment python3 -m venv genai-bench-env source genai-bench-env/bin/activate pip install genai-bench","title":"Permission Issues"},{"location":"getting-started/installation/#missing-dependencies","text":"# Update pip pip install --upgrade pip # Install with all dependencies pip install genai-bench [ dev ]","title":"Missing Dependencies"},{"location":"getting-started/installation/#network-issues","text":"# Use alternative PyPI mirrors pip install -i https://pypi.tuna.tsinghua.edu.cn/simple/ genai-bench # Or use conda conda install -c conda-forge genai-bench","title":"Network Issues"},{"location":"getting-started/installation/#getting-help","text":"If you encounter issues: Check the GitHub Issues Search for similar problems Create a new issue with: Your operating system and Python version Installation method used Full error message Steps to reproduce","title":"Getting Help"},{"location":"getting-started/installation/#next-steps","text":"After successful installation: Read the Quick Start Guide to run your first benchmark Explore the User Guide for detailed usage Check out Examples for practical scenarios","title":"Next Steps"},{"location":"getting-started/quick-start/","text":"Quick Start Guide \u00b6 Get up and running with GenAI Bench in minutes! This guide will walk you through the essential steps to run your first benchmark. Prerequisites \u00b6 Python 3.11 or 3.12 An LLM serving endpoint (e.g., vLLM, OpenAI API, etc.) Basic familiarity with command-line tools Installation \u00b6 Option 1: Install from PyPI (Recommended) \u00b6 pip install genai-bench Option 2: Development Setup \u00b6 # Clone the repository git clone https://github.com/sgl-project/genai-bench.git cd genai-bench # Set up virtual environment make uv source .venv/bin/activate # Install in editable mode make install Your First Benchmark \u00b6 Let's run a simple text-to-text benchmark to get familiar with GenAI Bench: 1. Basic Chat Benchmark \u00b6 genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"your-model\" \\ --task text-to-text \\ --max-time-per-run 60 \\ --max-requests-per-run 100 2. Monitor Your Benchmark \u00b6 GenAI Bench provides a live UI dashboard. Open your browser and navigate to: http://localhost:8089 You'll see real-time progress, logs, and metrics as your benchmark runs. 3. Analyze Results \u00b6 After the benchmark completes, generate an Excel report: genai-bench excel --experiment-dir ./experiments/latest Key Concepts \u00b6 Tasks \u00b6 GenAI Bench supports different types of benchmarks: text-to-text : Generate text responses (chat, QA) text-to-embeddings : Generate embeddings from text image-text-to-text : Generate text from images + text image-to-embeddings : Generate embeddings from images API Backends \u00b6 Supported API backends: openai : OpenAI-compatible APIs (vLLM, OpenAI, etc.) cohere : Cohere API oci_cohere : Oracle Cloud Infrastructure Cohere Traffic Scenarios \u00b6 GenAI Bench supports various traffic patterns: constant : Steady request rate burst : Sudden traffic spikes ramp : Gradually increasing load Next Steps \u00b6 Read the Installation Guide for detailed setup instructions Explore Tasks and Benchmarks for advanced usage Check out Examples for more scenarios Learn about Results Analysis to understand your metrics Getting Help \u00b6 Use genai-bench --help for command-line help Check the User Guide for detailed documentation Open an issue for bugs or questions","title":"Quick Start"},{"location":"getting-started/quick-start/#quick-start-guide","text":"Get up and running with GenAI Bench in minutes! This guide will walk you through the essential steps to run your first benchmark.","title":"Quick Start Guide"},{"location":"getting-started/quick-start/#prerequisites","text":"Python 3.11 or 3.12 An LLM serving endpoint (e.g., vLLM, OpenAI API, etc.) Basic familiarity with command-line tools","title":"Prerequisites"},{"location":"getting-started/quick-start/#installation","text":"","title":"Installation"},{"location":"getting-started/quick-start/#option-1-install-from-pypi-recommended","text":"pip install genai-bench","title":"Option 1: Install from PyPI (Recommended)"},{"location":"getting-started/quick-start/#option-2-development-setup","text":"# Clone the repository git clone https://github.com/sgl-project/genai-bench.git cd genai-bench # Set up virtual environment make uv source .venv/bin/activate # Install in editable mode make install","title":"Option 2: Development Setup"},{"location":"getting-started/quick-start/#your-first-benchmark","text":"Let's run a simple text-to-text benchmark to get familiar with GenAI Bench:","title":"Your First Benchmark"},{"location":"getting-started/quick-start/#1-basic-chat-benchmark","text":"genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"your-model\" \\ --task text-to-text \\ --max-time-per-run 60 \\ --max-requests-per-run 100","title":"1. Basic Chat Benchmark"},{"location":"getting-started/quick-start/#2-monitor-your-benchmark","text":"GenAI Bench provides a live UI dashboard. Open your browser and navigate to: http://localhost:8089 You'll see real-time progress, logs, and metrics as your benchmark runs.","title":"2. Monitor Your Benchmark"},{"location":"getting-started/quick-start/#3-analyze-results","text":"After the benchmark completes, generate an Excel report: genai-bench excel --experiment-dir ./experiments/latest","title":"3. Analyze Results"},{"location":"getting-started/quick-start/#key-concepts","text":"","title":"Key Concepts"},{"location":"getting-started/quick-start/#tasks","text":"GenAI Bench supports different types of benchmarks: text-to-text : Generate text responses (chat, QA) text-to-embeddings : Generate embeddings from text image-text-to-text : Generate text from images + text image-to-embeddings : Generate embeddings from images","title":"Tasks"},{"location":"getting-started/quick-start/#api-backends","text":"Supported API backends: openai : OpenAI-compatible APIs (vLLM, OpenAI, etc.) cohere : Cohere API oci_cohere : Oracle Cloud Infrastructure Cohere","title":"API Backends"},{"location":"getting-started/quick-start/#traffic-scenarios","text":"GenAI Bench supports various traffic patterns: constant : Steady request rate burst : Sudden traffic spikes ramp : Gradually increasing load","title":"Traffic Scenarios"},{"location":"getting-started/quick-start/#next-steps","text":"Read the Installation Guide for detailed setup instructions Explore Tasks and Benchmarks for advanced usage Check out Examples for more scenarios Learn about Results Analysis to understand your metrics","title":"Next Steps"},{"location":"getting-started/quick-start/#getting-help","text":"Use genai-bench --help for command-line help Check the User Guide for detailed documentation Open an issue for bugs or questions","title":"Getting Help"},{"location":"user-guide/analysis/","text":"Results Analysis \u00b6 This guide explains how to analyze benchmark results, understand the metrics, and generate comprehensive reports. Understanding Benchmark Results \u00b6 After running a benchmark, GenAI Bench generates detailed results that help you understand your system's performance characteristics. Result Structure \u00b6 experiments/ \u2514\u2500\u2500 your-experiment/ \u251c\u2500\u2500 results.json # Raw benchmark data \u251c\u2500\u2500 metrics.json # Aggregated metrics \u251c\u2500\u2500 logs/ # Detailed logs \u2502 \u251c\u2500\u2500 benchmark.log \u2502 \u2514\u2500\u2500 errors.log \u2514\u2500\u2500 plots/ # Generated visualizations \u2514\u2500\u2500 performance.png Key Metrics Explained \u00b6 Single-Request Metrics \u00b6 These metrics capture performance for individual requests: Time Metrics \u00b6 Metric Description Formula Interpretation TTFT Time to First Token time_at_first_token - start_time Initial response time E2E Latency End-to-End latency end_time - start_time Total request time TPOT Time Per Output Token (e2e_latency - TTFT) / (num_output_tokens - 1) Generation speed Throughput Metrics \u00b6 Metric Description Formula Unit Input Throughput Input processing rate num_input_tokens / TTFT tokens/second Output Throughput Output generation rate (num_output_tokens - 1) / output_latency tokens/second Inference Speed Overall generation speed 1 / TPOT tokens/second Token Metrics \u00b6 Metric Description Unit Input Tokens Number of prompt tokens tokens Output Tokens Number of generated tokens tokens Total Tokens Total tokens processed tokens Aggregated Metrics \u00b6 These metrics summarize performance across multiple requests: System Performance \u00b6 Metric Description Formula Unit Mean Input Throughput Average input processing sum(input_tokens) / run_duration tokens/second Mean Output Throughput Average output generation sum(output_tokens) / run_duration tokens/second Total Tokens Throughput Overall processing rate sum(total_tokens) / run_duration tokens/second Requests Per Minute Request processing rate num_completed_requests / duration * 60 requests/minute Quality Metrics \u00b6 Metric Description Formula Unit Error Rate Percentage of failed requests num_error_requests / total_requests percentage Success Rate Percentage of successful requests num_completed_requests / total_requests percentage Total Chars Per Hour Character processing capacity total_tokens_throughput * chars_per_token * 3600 characters/hour Generating Reports \u00b6 Excel Reports \u00b6 Generate comprehensive Excel reports with all metrics: genai-bench excel \\ --experiment-dir \"./experiments/my-benchmark\" \\ --output-file \"detailed_report.xlsx\" \\ --include-raw-data \\ --include-pricing Excel Report Contents \u00b6 Summary Sheet : High-level metrics and statistics Raw Data : Individual request metrics Aggregated Metrics : System-level performance Pricing Analysis : Cost calculations Error Analysis : Failed request details Custom Plots \u00b6 Generate visualizations with custom configurations: genai-bench plot \\ --experiment-dirs \"./exp1,./exp2,./exp3\" \\ --output-file \"comparison.png\" \\ --plot-config \"my_config.yaml\" Plot Configuration \u00b6 Create a YAML configuration file: # plot_config.yaml layout : rows : 2 cols : 4 plots : - title : \"Throughput vs Concurrency\" type : \"line\" x_axis : \"num_users\" y_axis : \"mean_total_tokens_throughput\" color_by : \"server_engine\" - title : \"Latency vs Concurrency\" type : \"line\" x_axis : \"num_users\" y_axis : \"mean_e2e_latency\" color_by : \"server_engine\" - title : \"Error Rate vs Concurrency\" type : \"bar\" x_axis : \"num_users\" y_axis : \"error_rate\" - title : \"Token Distribution\" type : \"histogram\" x_axis : \"num_output_tokens\" bins : 20 Interpreting Results \u00b6 Performance Analysis \u00b6 Throughput Analysis \u00b6 Identify Peak Throughput : Find the maximum sustainable throughput Bottleneck Detection : Look for performance degradation points Scaling Efficiency : Analyze how performance scales with concurrency Latency Analysis \u00b6 Response Time Distribution : Understand latency percentiles TTFT vs E2E : Compare initial vs total response times Latency Stability : Check for latency spikes or inconsistencies Error Analysis \u00b6 Error Patterns : Identify common error types Error Rate Trends : Monitor error rates under different loads Recovery Behavior : Test system recovery after errors Comparative Analysis \u00b6 Model Comparison \u00b6 # Run benchmarks for different models for model in \"gpt-3.5-turbo\" \"gpt-4\" \"llama-2-7b\" ; do genai-bench benchmark \\ --api-model-name \" $model \" \\ --output-dir \"./experiments/ $model \" \\ --max-time-per-run 300 done # Generate comparison genai-bench plot \\ --experiment-dirs \"./experiments/gpt-3.5-turbo,./experiments/gpt-4,./experiments/llama-2-7b\" \\ --output-file \"model_comparison.png\" \\ --group-by \"api_model_name\" Configuration Comparison \u00b6 Compare different configurations: Concurrency Levels : Test different user counts Traffic Patterns : Compare constant vs burst vs ramp Model Parameters : Test different temperature, max_tokens settings Performance Optimization \u00b6 Identifying Bottlenecks \u00b6 CPU Bottlenecks : High CPU usage, low throughput Memory Bottlenecks : High memory usage, OOM errors Network Bottlenecks : High latency, connection timeouts GPU Bottlenecks : Low GPU utilization, long inference times Optimization Strategies \u00b6 Batch Processing : Increase batch sizes for better throughput Model Optimization : Use quantized or optimized models Infrastructure Scaling : Add more resources or instances Request Optimization : Optimize prompt length and parameters Advanced Analysis \u00b6 Statistical Analysis \u00b6 Percentile Analysis \u00b6 import pandas as pd # Load results df = pd . read_json ( \"experiments/my-benchmark/results.json\" ) # Calculate percentiles percentiles = df [ 'e2e_latency' ] . quantile ([ 0.5 , 0.9 , 0.95 , 0.99 ]) print ( f \"P50: { percentiles [ 0.5 ] : .3f } s\" ) print ( f \"P90: { percentiles [ 0.9 ] : .3f } s\" ) print ( f \"P95: { percentiles [ 0.95 ] : .3f } s\" ) print ( f \"P99: { percentiles [ 0.99 ] : .3f } s\" ) Trend Analysis \u00b6 # Analyze trends over time df [ 'timestamp' ] = pd . to_datetime ( df [ 'start_time' ]) hourly_stats = df . groupby ( df [ 'timestamp' ] . dt . hour )[ 'e2e_latency' ] . agg ([ 'mean' , 'std' ]) Custom Metrics \u00b6 Business Metrics \u00b6 Calculate business-relevant metrics: # Cost per request cost_per_request = ( input_tokens * input_cost + output_tokens * output_cost ) # Revenue per hour revenue_per_hour = requests_per_hour * price_per_request # ROI calculation roi = ( revenue_per_hour - cost_per_hour ) / cost_per_hour Quality Metrics \u00b6 # Response quality (if available) quality_score = calculate_quality_score ( responses ) # Consistency metrics latency_consistency = df [ 'e2e_latency' ] . std () / df [ 'e2e_latency' ] . mean () Reporting Best Practices \u00b6 Report Structure \u00b6 Executive Summary : High-level findings and recommendations Methodology : Benchmark setup and configuration Results : Detailed metrics and analysis Conclusions : Key insights and next steps Visualization Guidelines \u00b6 Use Appropriate Charts : Line charts for trends, bar charts for comparisons Include Error Bars : Show confidence intervals where applicable Consistent Formatting : Use consistent colors, fonts, and scales Clear Labels : Provide descriptive titles and axis labels Documentation \u00b6 Record Configuration : Document all benchmark parameters Note Environment : Record hardware, software, and network conditions Track Changes : Version control your benchmark configurations Share Context : Provide business context for technical results Troubleshooting Analysis \u00b6 Common Issues \u00b6 Missing Data \u00b6 Error: No results found in experiment directory Solution : Check that the benchmark completed successfully and generated results. Inconsistent Results \u00b6 Warning: High variance in latency measurements Solution : Check for network issues, system load, or configuration problems. Plot Generation Errors \u00b6 Error: Invalid plot configuration Solution : Validate your YAML configuration file syntax and structure. Getting Help \u00b6 Check the CLI Reference for command options Review Examples for analysis patterns Open an issue for bugs Next Steps \u00b6 Learn about Traffic Scenarios for load testing Explore Examples for practical analysis Check out API Reference for programmatic access","title":"Results Analysis"},{"location":"user-guide/analysis/#results-analysis","text":"This guide explains how to analyze benchmark results, understand the metrics, and generate comprehensive reports.","title":"Results Analysis"},{"location":"user-guide/analysis/#understanding-benchmark-results","text":"After running a benchmark, GenAI Bench generates detailed results that help you understand your system's performance characteristics.","title":"Understanding Benchmark Results"},{"location":"user-guide/analysis/#result-structure","text":"experiments/ \u2514\u2500\u2500 your-experiment/ \u251c\u2500\u2500 results.json # Raw benchmark data \u251c\u2500\u2500 metrics.json # Aggregated metrics \u251c\u2500\u2500 logs/ # Detailed logs \u2502 \u251c\u2500\u2500 benchmark.log \u2502 \u2514\u2500\u2500 errors.log \u2514\u2500\u2500 plots/ # Generated visualizations \u2514\u2500\u2500 performance.png","title":"Result Structure"},{"location":"user-guide/analysis/#key-metrics-explained","text":"","title":"Key Metrics Explained"},{"location":"user-guide/analysis/#single-request-metrics","text":"These metrics capture performance for individual requests:","title":"Single-Request Metrics"},{"location":"user-guide/analysis/#time-metrics","text":"Metric Description Formula Interpretation TTFT Time to First Token time_at_first_token - start_time Initial response time E2E Latency End-to-End latency end_time - start_time Total request time TPOT Time Per Output Token (e2e_latency - TTFT) / (num_output_tokens - 1) Generation speed","title":"Time Metrics"},{"location":"user-guide/analysis/#throughput-metrics","text":"Metric Description Formula Unit Input Throughput Input processing rate num_input_tokens / TTFT tokens/second Output Throughput Output generation rate (num_output_tokens - 1) / output_latency tokens/second Inference Speed Overall generation speed 1 / TPOT tokens/second","title":"Throughput Metrics"},{"location":"user-guide/analysis/#token-metrics","text":"Metric Description Unit Input Tokens Number of prompt tokens tokens Output Tokens Number of generated tokens tokens Total Tokens Total tokens processed tokens","title":"Token Metrics"},{"location":"user-guide/analysis/#aggregated-metrics","text":"These metrics summarize performance across multiple requests:","title":"Aggregated Metrics"},{"location":"user-guide/analysis/#system-performance","text":"Metric Description Formula Unit Mean Input Throughput Average input processing sum(input_tokens) / run_duration tokens/second Mean Output Throughput Average output generation sum(output_tokens) / run_duration tokens/second Total Tokens Throughput Overall processing rate sum(total_tokens) / run_duration tokens/second Requests Per Minute Request processing rate num_completed_requests / duration * 60 requests/minute","title":"System Performance"},{"location":"user-guide/analysis/#quality-metrics","text":"Metric Description Formula Unit Error Rate Percentage of failed requests num_error_requests / total_requests percentage Success Rate Percentage of successful requests num_completed_requests / total_requests percentage Total Chars Per Hour Character processing capacity total_tokens_throughput * chars_per_token * 3600 characters/hour","title":"Quality Metrics"},{"location":"user-guide/analysis/#generating-reports","text":"","title":"Generating Reports"},{"location":"user-guide/analysis/#excel-reports","text":"Generate comprehensive Excel reports with all metrics: genai-bench excel \\ --experiment-dir \"./experiments/my-benchmark\" \\ --output-file \"detailed_report.xlsx\" \\ --include-raw-data \\ --include-pricing","title":"Excel Reports"},{"location":"user-guide/analysis/#excel-report-contents","text":"Summary Sheet : High-level metrics and statistics Raw Data : Individual request metrics Aggregated Metrics : System-level performance Pricing Analysis : Cost calculations Error Analysis : Failed request details","title":"Excel Report Contents"},{"location":"user-guide/analysis/#custom-plots","text":"Generate visualizations with custom configurations: genai-bench plot \\ --experiment-dirs \"./exp1,./exp2,./exp3\" \\ --output-file \"comparison.png\" \\ --plot-config \"my_config.yaml\"","title":"Custom Plots"},{"location":"user-guide/analysis/#plot-configuration","text":"Create a YAML configuration file: # plot_config.yaml layout : rows : 2 cols : 4 plots : - title : \"Throughput vs Concurrency\" type : \"line\" x_axis : \"num_users\" y_axis : \"mean_total_tokens_throughput\" color_by : \"server_engine\" - title : \"Latency vs Concurrency\" type : \"line\" x_axis : \"num_users\" y_axis : \"mean_e2e_latency\" color_by : \"server_engine\" - title : \"Error Rate vs Concurrency\" type : \"bar\" x_axis : \"num_users\" y_axis : \"error_rate\" - title : \"Token Distribution\" type : \"histogram\" x_axis : \"num_output_tokens\" bins : 20","title":"Plot Configuration"},{"location":"user-guide/analysis/#interpreting-results","text":"","title":"Interpreting Results"},{"location":"user-guide/analysis/#performance-analysis","text":"","title":"Performance Analysis"},{"location":"user-guide/analysis/#throughput-analysis","text":"Identify Peak Throughput : Find the maximum sustainable throughput Bottleneck Detection : Look for performance degradation points Scaling Efficiency : Analyze how performance scales with concurrency","title":"Throughput Analysis"},{"location":"user-guide/analysis/#latency-analysis","text":"Response Time Distribution : Understand latency percentiles TTFT vs E2E : Compare initial vs total response times Latency Stability : Check for latency spikes or inconsistencies","title":"Latency Analysis"},{"location":"user-guide/analysis/#error-analysis","text":"Error Patterns : Identify common error types Error Rate Trends : Monitor error rates under different loads Recovery Behavior : Test system recovery after errors","title":"Error Analysis"},{"location":"user-guide/analysis/#comparative-analysis","text":"","title":"Comparative Analysis"},{"location":"user-guide/analysis/#model-comparison","text":"# Run benchmarks for different models for model in \"gpt-3.5-turbo\" \"gpt-4\" \"llama-2-7b\" ; do genai-bench benchmark \\ --api-model-name \" $model \" \\ --output-dir \"./experiments/ $model \" \\ --max-time-per-run 300 done # Generate comparison genai-bench plot \\ --experiment-dirs \"./experiments/gpt-3.5-turbo,./experiments/gpt-4,./experiments/llama-2-7b\" \\ --output-file \"model_comparison.png\" \\ --group-by \"api_model_name\"","title":"Model Comparison"},{"location":"user-guide/analysis/#configuration-comparison","text":"Compare different configurations: Concurrency Levels : Test different user counts Traffic Patterns : Compare constant vs burst vs ramp Model Parameters : Test different temperature, max_tokens settings","title":"Configuration Comparison"},{"location":"user-guide/analysis/#performance-optimization","text":"","title":"Performance Optimization"},{"location":"user-guide/analysis/#identifying-bottlenecks","text":"CPU Bottlenecks : High CPU usage, low throughput Memory Bottlenecks : High memory usage, OOM errors Network Bottlenecks : High latency, connection timeouts GPU Bottlenecks : Low GPU utilization, long inference times","title":"Identifying Bottlenecks"},{"location":"user-guide/analysis/#optimization-strategies","text":"Batch Processing : Increase batch sizes for better throughput Model Optimization : Use quantized or optimized models Infrastructure Scaling : Add more resources or instances Request Optimization : Optimize prompt length and parameters","title":"Optimization Strategies"},{"location":"user-guide/analysis/#advanced-analysis","text":"","title":"Advanced Analysis"},{"location":"user-guide/analysis/#statistical-analysis","text":"","title":"Statistical Analysis"},{"location":"user-guide/analysis/#percentile-analysis","text":"import pandas as pd # Load results df = pd . read_json ( \"experiments/my-benchmark/results.json\" ) # Calculate percentiles percentiles = df [ 'e2e_latency' ] . quantile ([ 0.5 , 0.9 , 0.95 , 0.99 ]) print ( f \"P50: { percentiles [ 0.5 ] : .3f } s\" ) print ( f \"P90: { percentiles [ 0.9 ] : .3f } s\" ) print ( f \"P95: { percentiles [ 0.95 ] : .3f } s\" ) print ( f \"P99: { percentiles [ 0.99 ] : .3f } s\" )","title":"Percentile Analysis"},{"location":"user-guide/analysis/#trend-analysis","text":"# Analyze trends over time df [ 'timestamp' ] = pd . to_datetime ( df [ 'start_time' ]) hourly_stats = df . groupby ( df [ 'timestamp' ] . dt . hour )[ 'e2e_latency' ] . agg ([ 'mean' , 'std' ])","title":"Trend Analysis"},{"location":"user-guide/analysis/#custom-metrics","text":"","title":"Custom Metrics"},{"location":"user-guide/analysis/#business-metrics","text":"Calculate business-relevant metrics: # Cost per request cost_per_request = ( input_tokens * input_cost + output_tokens * output_cost ) # Revenue per hour revenue_per_hour = requests_per_hour * price_per_request # ROI calculation roi = ( revenue_per_hour - cost_per_hour ) / cost_per_hour","title":"Business Metrics"},{"location":"user-guide/analysis/#quality-metrics_1","text":"# Response quality (if available) quality_score = calculate_quality_score ( responses ) # Consistency metrics latency_consistency = df [ 'e2e_latency' ] . std () / df [ 'e2e_latency' ] . mean ()","title":"Quality Metrics"},{"location":"user-guide/analysis/#reporting-best-practices","text":"","title":"Reporting Best Practices"},{"location":"user-guide/analysis/#report-structure","text":"Executive Summary : High-level findings and recommendations Methodology : Benchmark setup and configuration Results : Detailed metrics and analysis Conclusions : Key insights and next steps","title":"Report Structure"},{"location":"user-guide/analysis/#visualization-guidelines","text":"Use Appropriate Charts : Line charts for trends, bar charts for comparisons Include Error Bars : Show confidence intervals where applicable Consistent Formatting : Use consistent colors, fonts, and scales Clear Labels : Provide descriptive titles and axis labels","title":"Visualization Guidelines"},{"location":"user-guide/analysis/#documentation","text":"Record Configuration : Document all benchmark parameters Note Environment : Record hardware, software, and network conditions Track Changes : Version control your benchmark configurations Share Context : Provide business context for technical results","title":"Documentation"},{"location":"user-guide/analysis/#troubleshooting-analysis","text":"","title":"Troubleshooting Analysis"},{"location":"user-guide/analysis/#common-issues","text":"","title":"Common Issues"},{"location":"user-guide/analysis/#missing-data","text":"Error: No results found in experiment directory Solution : Check that the benchmark completed successfully and generated results.","title":"Missing Data"},{"location":"user-guide/analysis/#inconsistent-results","text":"Warning: High variance in latency measurements Solution : Check for network issues, system load, or configuration problems.","title":"Inconsistent Results"},{"location":"user-guide/analysis/#plot-generation-errors","text":"Error: Invalid plot configuration Solution : Validate your YAML configuration file syntax and structure.","title":"Plot Generation Errors"},{"location":"user-guide/analysis/#getting-help","text":"Check the CLI Reference for command options Review Examples for analysis patterns Open an issue for bugs","title":"Getting Help"},{"location":"user-guide/analysis/#next-steps","text":"Learn about Traffic Scenarios for load testing Explore Examples for practical analysis Check out API Reference for programmatic access","title":"Next Steps"},{"location":"user-guide/cli/","text":"Command Line Interface Reference \u00b6 This guide provides a complete reference for all GenAI Bench command-line interface commands and options. Overview \u00b6 GenAI Bench provides three main commands: genai-bench [ OPTIONS ] COMMAND [ ARGS ] ... Available Commands \u00b6 benchmark - Run a benchmark based on user-defined scenarios excel - Export experiment results to an Excel file plot - Generate plots from experiment results Global Options \u00b6 genai-bench [ OPTIONS ] COMMAND [ ARGS ] ... Options: --version Show the version and exit. --help Show this message and exit. Benchmark Command \u00b6 The benchmark command is the main command for running performance benchmarks. Basic Syntax \u00b6 genai-bench benchmark [ OPTIONS ] Required Options \u00b6 Option Type Description Example --api-backend string API backend type openai , cohere , oci_cohere --api-base string API base URL http://localhost:8082 --api-key string API authentication key sk-... --api-model-name string Model name identifier gpt-3.5-turbo --task string Benchmark task type text-to-text API Configuration Options \u00b6 OpenAI-compatible Backends \u00b6 --api-backend openai --api-base \"http://localhost:8082\" --api-key \"your-api-key\" --api-model-name \"your-model\" Cohere Backend \u00b6 --api-backend cohere --api-base \"https://api.cohere.ai\" --api-key \"your-cohere-key\" --api-model-name \"command-r-plus\" OCI Cohere Backend \u00b6 --api-backend oci_cohere --api-base \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\" --api-key \"your-oci-key\" --api-model-name \"cohere.command-r-plus\" Task Configuration \u00b6 Supported Tasks \u00b6 Task Description Use Case text-to-text Generate text from text input Chat, QA, summarization text-to-embeddings Generate embeddings from text Semantic search image-text-to-text Generate text from images + text Visual QA image-to-embeddings Generate embeddings from images Image similarity Task Examples \u00b6 # Text-to-text benchmark --task text-to-text # Embeddings benchmark --task text-to-embeddings # Vision benchmark --task image-text-to-text Traffic Configuration \u00b6 Traffic Scenarios \u00b6 Scenario Description Use Case constant Steady request rate Baseline performance burst Sudden traffic spikes Stress testing ramp Gradually increasing load Capacity testing Concurrency and Load \u00b6 # Set number of concurrent users --num-users 10 # Set request rate (requests per second) --spawn-rate 5 # Set maximum time per run (seconds) --max-time-per-run 300 # Set maximum requests per run --max-requests-per-run 1000 Data Configuration \u00b6 Dataset Options \u00b6 # Use built-in dataset --dataset-name \"sonnet.txt\" # Use custom dataset file --dataset-path \"/path/to/your/dataset.txt\" # Use Hugging Face dataset --dataset-name \"your-dataset\" --dataset-split \"train\" Tokenizer Configuration \u00b6 # Use model's tokenizer --model-tokenizer \"/path/to/tokenizer\" # Use Hugging Face tokenizer --model-tokenizer \"meta-llama/Llama-2-7b-chat-hf\" Advanced Options \u00b6 Server Information \u00b6 # Server engine type --server-engine \"vLLM\" # GPU type --server-gpu-type \"H100\" # Model size --server-model-size \"7B\" Monitoring and Logging \u00b6 # Enable UI dashboard --ui # Set log level --log-level INFO # Output directory --output-dir \"./experiments\" Complete Example \u00b6 genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --num-users 5 \\ --spawn-rate 2 \\ --max-time-per-run 120 \\ --max-requests-per-run 500 \\ --dataset-name \"sonnet.txt\" \\ --model-tokenizer \"/path/to/tokenizer\" \\ --server-engine \"vLLM\" \\ --server-gpu-type \"H100\" \\ --ui \\ --output-dir \"./my-experiment\" Excel Command \u00b6 Generate Excel reports from benchmark results. Basic Syntax \u00b6 genai-bench excel [ OPTIONS ] Options \u00b6 Option Type Description Default --experiment-dir string Experiment directory ./experiments/latest --output-file string Output Excel file experiment_results.xlsx --include-raw-data flag Include raw metrics data False --include-pricing flag Include pricing calculations True Example \u00b6 genai-bench excel \\ --experiment-dir \"./experiments/my-benchmark\" \\ --output-file \"results.xlsx\" \\ --include-raw-data \\ --include-pricing Plot Command \u00b6 Generate visualizations from benchmark results. Basic Syntax \u00b6 genai-bench plot [ OPTIONS ] Options \u00b6 Option Type Description Default --experiment-dirs string Comma-separated experiment directories ./experiments/latest --output-file string Output plot file experiment_plots.png --plot-config string Plot configuration file Default 2x4 grid --filters string Filter criteria None --group-by string Grouping criteria None Plot Configuration \u00b6 Create a custom plot configuration file: # plot_config.yaml layout : rows : 2 cols : 4 plots : - title : \"Throughput vs Concurrency\" type : \"line\" x_axis : \"num_users\" y_axis : \"mean_total_tokens_throughput\" - title : \"Latency vs Concurrency\" type : \"line\" x_axis : \"num_users\" y_axis : \"mean_e2e_latency\" Example \u00b6 genai-bench plot \\ --experiment-dirs \"./exp1,./exp2,./exp3\" \\ --output-file \"comparison.png\" \\ --plot-config \"my_config.yaml\" \\ --filters \"server_engine=vLLM\" \\ --group-by \"num_users\" Environment Variables \u00b6 Set these environment variables for configuration: # Hugging Face token (for tokenizer downloads) export HF_TOKEN = \"your-huggingface-token\" # Disable torch warnings export TRANSFORMERS_VERBOSITY = error # Log level export GENAI_BENCH_LOG_LEVEL = INFO # API keys export OPENAI_API_KEY = \"your-openai-key\" export COHERE_API_KEY = \"your-cohere-key\" Getting Help \u00b6 Command Help \u00b6 # General help genai-bench --help # Benchmark command help genai-bench benchmark --help # Excel command help genai-bench excel --help # Plot command help genai-bench plot --help Interactive Mode \u00b6 For guided setup, run without options: genai-bench benchmark This will prompt you for required parameters interactively. Common Patterns \u00b6 Quick Benchmark \u00b6 genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"your-model\" \\ --task text-to-text \\ --max-time-per-run 60 Production Benchmark \u00b6 genai-bench benchmark \\ --api-backend openai \\ --api-base \"https://your-api.com\" \\ --api-key \"your-key\" \\ --api-model-name \"gpt-4\" \\ --task text-to-text \\ --num-users 20 \\ --spawn-rate 5 \\ --max-time-per-run 600 \\ --max-requests-per-run 2000 \\ --dataset-name \"production-data\" \\ --ui \\ --output-dir \"./production-benchmark\" Comparison Benchmark \u00b6 # Run multiple experiments for model in \"model1\" \"model2\" \"model3\" ; do genai-bench benchmark \\ --api-model-name \" $model \" \\ --output-dir \"./experiments/ $model \" \\ --max-time-per-run 300 done # Generate comparison plot genai-bench plot \\ --experiment-dirs \"./experiments/model1,./experiments/model2,./experiments/model3\" \\ --output-file \"model_comparison.png\"","title":"Command Line Interface"},{"location":"user-guide/cli/#command-line-interface-reference","text":"This guide provides a complete reference for all GenAI Bench command-line interface commands and options.","title":"Command Line Interface Reference"},{"location":"user-guide/cli/#overview","text":"GenAI Bench provides three main commands: genai-bench [ OPTIONS ] COMMAND [ ARGS ] ...","title":"Overview"},{"location":"user-guide/cli/#available-commands","text":"benchmark - Run a benchmark based on user-defined scenarios excel - Export experiment results to an Excel file plot - Generate plots from experiment results","title":"Available Commands"},{"location":"user-guide/cli/#global-options","text":"genai-bench [ OPTIONS ] COMMAND [ ARGS ] ... Options: --version Show the version and exit. --help Show this message and exit.","title":"Global Options"},{"location":"user-guide/cli/#benchmark-command","text":"The benchmark command is the main command for running performance benchmarks.","title":"Benchmark Command"},{"location":"user-guide/cli/#basic-syntax","text":"genai-bench benchmark [ OPTIONS ]","title":"Basic Syntax"},{"location":"user-guide/cli/#required-options","text":"Option Type Description Example --api-backend string API backend type openai , cohere , oci_cohere --api-base string API base URL http://localhost:8082 --api-key string API authentication key sk-... --api-model-name string Model name identifier gpt-3.5-turbo --task string Benchmark task type text-to-text","title":"Required Options"},{"location":"user-guide/cli/#api-configuration-options","text":"","title":"API Configuration Options"},{"location":"user-guide/cli/#openai-compatible-backends","text":"--api-backend openai --api-base \"http://localhost:8082\" --api-key \"your-api-key\" --api-model-name \"your-model\"","title":"OpenAI-compatible Backends"},{"location":"user-guide/cli/#cohere-backend","text":"--api-backend cohere --api-base \"https://api.cohere.ai\" --api-key \"your-cohere-key\" --api-model-name \"command-r-plus\"","title":"Cohere Backend"},{"location":"user-guide/cli/#oci-cohere-backend","text":"--api-backend oci_cohere --api-base \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\" --api-key \"your-oci-key\" --api-model-name \"cohere.command-r-plus\"","title":"OCI Cohere Backend"},{"location":"user-guide/cli/#task-configuration","text":"","title":"Task Configuration"},{"location":"user-guide/cli/#supported-tasks","text":"Task Description Use Case text-to-text Generate text from text input Chat, QA, summarization text-to-embeddings Generate embeddings from text Semantic search image-text-to-text Generate text from images + text Visual QA image-to-embeddings Generate embeddings from images Image similarity","title":"Supported Tasks"},{"location":"user-guide/cli/#task-examples","text":"# Text-to-text benchmark --task text-to-text # Embeddings benchmark --task text-to-embeddings # Vision benchmark --task image-text-to-text","title":"Task Examples"},{"location":"user-guide/cli/#traffic-configuration","text":"","title":"Traffic Configuration"},{"location":"user-guide/cli/#traffic-scenarios","text":"Scenario Description Use Case constant Steady request rate Baseline performance burst Sudden traffic spikes Stress testing ramp Gradually increasing load Capacity testing","title":"Traffic Scenarios"},{"location":"user-guide/cli/#concurrency-and-load","text":"# Set number of concurrent users --num-users 10 # Set request rate (requests per second) --spawn-rate 5 # Set maximum time per run (seconds) --max-time-per-run 300 # Set maximum requests per run --max-requests-per-run 1000","title":"Concurrency and Load"},{"location":"user-guide/cli/#data-configuration","text":"","title":"Data Configuration"},{"location":"user-guide/cli/#dataset-options","text":"# Use built-in dataset --dataset-name \"sonnet.txt\" # Use custom dataset file --dataset-path \"/path/to/your/dataset.txt\" # Use Hugging Face dataset --dataset-name \"your-dataset\" --dataset-split \"train\"","title":"Dataset Options"},{"location":"user-guide/cli/#tokenizer-configuration","text":"# Use model's tokenizer --model-tokenizer \"/path/to/tokenizer\" # Use Hugging Face tokenizer --model-tokenizer \"meta-llama/Llama-2-7b-chat-hf\"","title":"Tokenizer Configuration"},{"location":"user-guide/cli/#advanced-options","text":"","title":"Advanced Options"},{"location":"user-guide/cli/#server-information","text":"# Server engine type --server-engine \"vLLM\" # GPU type --server-gpu-type \"H100\" # Model size --server-model-size \"7B\"","title":"Server Information"},{"location":"user-guide/cli/#monitoring-and-logging","text":"# Enable UI dashboard --ui # Set log level --log-level INFO # Output directory --output-dir \"./experiments\"","title":"Monitoring and Logging"},{"location":"user-guide/cli/#complete-example","text":"genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --num-users 5 \\ --spawn-rate 2 \\ --max-time-per-run 120 \\ --max-requests-per-run 500 \\ --dataset-name \"sonnet.txt\" \\ --model-tokenizer \"/path/to/tokenizer\" \\ --server-engine \"vLLM\" \\ --server-gpu-type \"H100\" \\ --ui \\ --output-dir \"./my-experiment\"","title":"Complete Example"},{"location":"user-guide/cli/#excel-command","text":"Generate Excel reports from benchmark results.","title":"Excel Command"},{"location":"user-guide/cli/#basic-syntax_1","text":"genai-bench excel [ OPTIONS ]","title":"Basic Syntax"},{"location":"user-guide/cli/#options","text":"Option Type Description Default --experiment-dir string Experiment directory ./experiments/latest --output-file string Output Excel file experiment_results.xlsx --include-raw-data flag Include raw metrics data False --include-pricing flag Include pricing calculations True","title":"Options"},{"location":"user-guide/cli/#example","text":"genai-bench excel \\ --experiment-dir \"./experiments/my-benchmark\" \\ --output-file \"results.xlsx\" \\ --include-raw-data \\ --include-pricing","title":"Example"},{"location":"user-guide/cli/#plot-command","text":"Generate visualizations from benchmark results.","title":"Plot Command"},{"location":"user-guide/cli/#basic-syntax_2","text":"genai-bench plot [ OPTIONS ]","title":"Basic Syntax"},{"location":"user-guide/cli/#options_1","text":"Option Type Description Default --experiment-dirs string Comma-separated experiment directories ./experiments/latest --output-file string Output plot file experiment_plots.png --plot-config string Plot configuration file Default 2x4 grid --filters string Filter criteria None --group-by string Grouping criteria None","title":"Options"},{"location":"user-guide/cli/#plot-configuration","text":"Create a custom plot configuration file: # plot_config.yaml layout : rows : 2 cols : 4 plots : - title : \"Throughput vs Concurrency\" type : \"line\" x_axis : \"num_users\" y_axis : \"mean_total_tokens_throughput\" - title : \"Latency vs Concurrency\" type : \"line\" x_axis : \"num_users\" y_axis : \"mean_e2e_latency\"","title":"Plot Configuration"},{"location":"user-guide/cli/#example_1","text":"genai-bench plot \\ --experiment-dirs \"./exp1,./exp2,./exp3\" \\ --output-file \"comparison.png\" \\ --plot-config \"my_config.yaml\" \\ --filters \"server_engine=vLLM\" \\ --group-by \"num_users\"","title":"Example"},{"location":"user-guide/cli/#environment-variables","text":"Set these environment variables for configuration: # Hugging Face token (for tokenizer downloads) export HF_TOKEN = \"your-huggingface-token\" # Disable torch warnings export TRANSFORMERS_VERBOSITY = error # Log level export GENAI_BENCH_LOG_LEVEL = INFO # API keys export OPENAI_API_KEY = \"your-openai-key\" export COHERE_API_KEY = \"your-cohere-key\"","title":"Environment Variables"},{"location":"user-guide/cli/#getting-help","text":"","title":"Getting Help"},{"location":"user-guide/cli/#command-help","text":"# General help genai-bench --help # Benchmark command help genai-bench benchmark --help # Excel command help genai-bench excel --help # Plot command help genai-bench plot --help","title":"Command Help"},{"location":"user-guide/cli/#interactive-mode","text":"For guided setup, run without options: genai-bench benchmark This will prompt you for required parameters interactively.","title":"Interactive Mode"},{"location":"user-guide/cli/#common-patterns","text":"","title":"Common Patterns"},{"location":"user-guide/cli/#quick-benchmark","text":"genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-key\" \\ --api-model-name \"your-model\" \\ --task text-to-text \\ --max-time-per-run 60","title":"Quick Benchmark"},{"location":"user-guide/cli/#production-benchmark","text":"genai-bench benchmark \\ --api-backend openai \\ --api-base \"https://your-api.com\" \\ --api-key \"your-key\" \\ --api-model-name \"gpt-4\" \\ --task text-to-text \\ --num-users 20 \\ --spawn-rate 5 \\ --max-time-per-run 600 \\ --max-requests-per-run 2000 \\ --dataset-name \"production-data\" \\ --ui \\ --output-dir \"./production-benchmark\"","title":"Production Benchmark"},{"location":"user-guide/cli/#comparison-benchmark","text":"# Run multiple experiments for model in \"model1\" \"model2\" \"model3\" ; do genai-bench benchmark \\ --api-model-name \" $model \" \\ --output-dir \"./experiments/ $model \" \\ --max-time-per-run 300 done # Generate comparison plot genai-bench plot \\ --experiment-dirs \"./experiments/model1,./experiments/model2,./experiments/model3\" \\ --output-file \"model_comparison.png\"","title":"Comparison Benchmark"},{"location":"user-guide/overview/","text":"GenAI Bench Overview \u00b6 This guide provides a comprehensive overview of GenAI Bench, its architecture, and core concepts. What is GenAI Bench? \u00b6 GenAI Bench is a comprehensive benchmarking tool designed for evaluating the performance of Large Language Model (LLM) serving systems. It provides detailed, token-level performance analysis with both command-line and web-based interfaces. Core Architecture \u00b6 GenAI Bench follows a modular architecture with the following key components: User Input \u2192 CLI Interface \u2192 Benchmark Engine \u2193 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2193 \u2193 \u2193 Sampling Module Traffic Generator Metrics Collector \u2193 \u2193 \u2193 Data Sources API Backends Results Storage \u2193 Analysis Tools \u2193 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2193 \u2193 \u2193 \u2193 Excel Reports Plots & UI Dashboard Custom Reports Visualizations Component Flow: 1. User Input \u2192 Commands and configuration 2. CLI Interface \u2192 Validates and processes user requests 3. Benchmark Engine \u2192 Orchestrates the entire benchmarking process 4. Sampling Module \u2192 Loads and processes datasets 5. Traffic Generator \u2192 Simulates user load patterns 6. Metrics Collector \u2192 Gathers performance data 7. Analysis Tools \u2192 Processes and analyzes results Key Components \u00b6 1. CLI Interface \u00b6 The command-line interface provides a unified way to: - Configure benchmark parameters - Start and monitor benchmarks - Generate reports and visualizations - Manage experiment data 2. Benchmark Engine \u00b6 The core engine orchestrates: - Task Management : Handles different benchmark types (text-to-text, embeddings, etc.) - Traffic Generation : Creates realistic load patterns - Metrics Collection : Gathers performance data at token level - Result Processing : Aggregates and analyzes results 3. Sampling Module \u00b6 Responsible for: - Data Loading : Reads from various data sources - Tokenization : Converts text to tokens for analysis - Request Generation : Creates API requests with proper formatting 4. API Backends \u00b6 Supports multiple LLM serving systems: - OpenAI-compatible : vLLM, OpenAI API, etc. - Cohere : Native Cohere API - OCI Cohere : Oracle Cloud Infrastructure Cohere Benchmark Types \u00b6 Task Categories \u00b6 GenAI Bench supports four main task categories: Task Input Output Use Cases text-to-text Text Text Chat, QA, summarization text-to-embeddings Text Embeddings Semantic search, similarity image-text-to-text Image + Text Text Visual QA, image understanding image-to-embeddings Image Embeddings Image similarity, search Traffic Patterns \u00b6 Different traffic scenarios simulate real-world usage: Constant Load : Steady request rate for baseline performance Burst Traffic : Sudden spikes to test system resilience Ramp-up : Gradually increasing load to find capacity limits Performance Metrics \u00b6 Single-Request Metrics \u00b6 These metrics capture performance for individual requests: Metric Description Formula Unit TTFT Time to First Token time_at_first_token - start_time seconds E2E Latency End-to-End latency end_time - start_time seconds TPOT Time Per Output Token (e2e_latency - TTFT) / (num_output_tokens - 1) seconds Inference Speed Tokens per second 1 / TPOT tokens/second Input Throughput Input processing rate num_input_tokens / TTFT tokens/second Output Throughput Output generation rate (num_output_tokens - 1) / output_latency tokens/second Aggregated Metrics \u00b6 These metrics summarize performance across multiple requests: Metric Description Formula Unit Mean Input Throughput Average input processing rate sum(input_tokens) / run_duration tokens/second Mean Output Throughput Average output generation rate sum(output_tokens) / run_duration tokens/second Total Tokens Throughput Overall token processing rate sum(total_tokens) / run_duration tokens/second Requests Per Minute Request processing rate num_completed_requests / duration * 60 requests/minute Error Rate Percentage of failed requests num_error_requests / total_requests percentage Data Flow \u00b6 1. Configuration Phase \u00b6 Flow: User \u2192 CLI \u2192 Config Validator \u2192 CLI \u2192 User Steps: 1. User runs benchmark command with parameters 2. CLI receives and parses the command 3. Config Validator validates all parameters and settings 4. CLI receives validation result 5. User gets confirmation to start benchmark or error messages 2. Benchmark Execution \u00b6 Flow: Benchmark Engine \u2192 Traffic Generator \u2192 API Backend \u2192 Traffic Generator \u2192 Metrics Collector \u2192 Benchmark Engine Steps: 1. Benchmark Engine initiates traffic generation 2. Traffic Generator sends requests to API backend 3. API Backend processes requests and returns responses 4. Traffic Generator receives responses 5. Metrics Collector collects performance metrics 6. Benchmark Engine stores results for analysis 3. Analysis Phase \u00b6 Flow: User \u2192 CLI \u2192 Analysis Tools \u2192 CLI \u2192 User Steps: 1. User requests analysis (Excel reports, plots, etc.) 2. CLI processes the analysis request 3. Analysis Tools process results and generate reports/plots 4. CLI receives generated analysis output 5. User gets analysis results (files, visualizations) Key Features \u00b6 Real-time Monitoring \u00b6 Live UI Dashboard : Web-based interface showing real-time progress Rich Logging : Detailed logs with automatic file output Progress Tracking : Visual indicators of benchmark status Comprehensive Analysis \u00b6 Excel Reports : Detailed spreadsheets with all metrics Custom Plots : Flexible visualization with 2x4 grid layouts Multi-experiment Comparison : Compare results across different runs Scalability \u00b6 Distributed Benchmarking : Run benchmarks across multiple machines Configurable Load : Adjust traffic patterns and concurrency levels Resource Management : Efficient memory and CPU usage Best Practices \u00b6 Benchmark Design \u00b6 Start Simple : Begin with basic configurations Gradual Scaling : Increase load gradually to find limits Multiple Scenarios : Test different traffic patterns Consistent Environment : Use same hardware/software for comparisons Result Interpretation \u00b6 Context Matters : Consider your specific use case Look for Trends : Focus on patterns, not single data points Compare Apples to Apples : Use consistent parameters Validate Results : Run multiple times to ensure consistency Next Steps \u00b6 Learn about the Command Line Interface for detailed usage Explore Tasks and Benchmarks for specific scenarios Check out Results Analysis for understanding your data See Examples for practical use cases","title":"Overview"},{"location":"user-guide/overview/#genai-bench-overview","text":"This guide provides a comprehensive overview of GenAI Bench, its architecture, and core concepts.","title":"GenAI Bench Overview"},{"location":"user-guide/overview/#what-is-genai-bench","text":"GenAI Bench is a comprehensive benchmarking tool designed for evaluating the performance of Large Language Model (LLM) serving systems. It provides detailed, token-level performance analysis with both command-line and web-based interfaces.","title":"What is GenAI Bench?"},{"location":"user-guide/overview/#core-architecture","text":"GenAI Bench follows a modular architecture with the following key components: User Input \u2192 CLI Interface \u2192 Benchmark Engine \u2193 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2193 \u2193 \u2193 Sampling Module Traffic Generator Metrics Collector \u2193 \u2193 \u2193 Data Sources API Backends Results Storage \u2193 Analysis Tools \u2193 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2193 \u2193 \u2193 \u2193 Excel Reports Plots & UI Dashboard Custom Reports Visualizations Component Flow: 1. User Input \u2192 Commands and configuration 2. CLI Interface \u2192 Validates and processes user requests 3. Benchmark Engine \u2192 Orchestrates the entire benchmarking process 4. Sampling Module \u2192 Loads and processes datasets 5. Traffic Generator \u2192 Simulates user load patterns 6. Metrics Collector \u2192 Gathers performance data 7. Analysis Tools \u2192 Processes and analyzes results","title":"Core Architecture"},{"location":"user-guide/overview/#key-components","text":"","title":"Key Components"},{"location":"user-guide/overview/#1-cli-interface","text":"The command-line interface provides a unified way to: - Configure benchmark parameters - Start and monitor benchmarks - Generate reports and visualizations - Manage experiment data","title":"1. CLI Interface"},{"location":"user-guide/overview/#2-benchmark-engine","text":"The core engine orchestrates: - Task Management : Handles different benchmark types (text-to-text, embeddings, etc.) - Traffic Generation : Creates realistic load patterns - Metrics Collection : Gathers performance data at token level - Result Processing : Aggregates and analyzes results","title":"2. Benchmark Engine"},{"location":"user-guide/overview/#3-sampling-module","text":"Responsible for: - Data Loading : Reads from various data sources - Tokenization : Converts text to tokens for analysis - Request Generation : Creates API requests with proper formatting","title":"3. Sampling Module"},{"location":"user-guide/overview/#4-api-backends","text":"Supports multiple LLM serving systems: - OpenAI-compatible : vLLM, OpenAI API, etc. - Cohere : Native Cohere API - OCI Cohere : Oracle Cloud Infrastructure Cohere","title":"4. API Backends"},{"location":"user-guide/overview/#benchmark-types","text":"","title":"Benchmark Types"},{"location":"user-guide/overview/#task-categories","text":"GenAI Bench supports four main task categories: Task Input Output Use Cases text-to-text Text Text Chat, QA, summarization text-to-embeddings Text Embeddings Semantic search, similarity image-text-to-text Image + Text Text Visual QA, image understanding image-to-embeddings Image Embeddings Image similarity, search","title":"Task Categories"},{"location":"user-guide/overview/#traffic-patterns","text":"Different traffic scenarios simulate real-world usage: Constant Load : Steady request rate for baseline performance Burst Traffic : Sudden spikes to test system resilience Ramp-up : Gradually increasing load to find capacity limits","title":"Traffic Patterns"},{"location":"user-guide/overview/#performance-metrics","text":"","title":"Performance Metrics"},{"location":"user-guide/overview/#single-request-metrics","text":"These metrics capture performance for individual requests: Metric Description Formula Unit TTFT Time to First Token time_at_first_token - start_time seconds E2E Latency End-to-End latency end_time - start_time seconds TPOT Time Per Output Token (e2e_latency - TTFT) / (num_output_tokens - 1) seconds Inference Speed Tokens per second 1 / TPOT tokens/second Input Throughput Input processing rate num_input_tokens / TTFT tokens/second Output Throughput Output generation rate (num_output_tokens - 1) / output_latency tokens/second","title":"Single-Request Metrics"},{"location":"user-guide/overview/#aggregated-metrics","text":"These metrics summarize performance across multiple requests: Metric Description Formula Unit Mean Input Throughput Average input processing rate sum(input_tokens) / run_duration tokens/second Mean Output Throughput Average output generation rate sum(output_tokens) / run_duration tokens/second Total Tokens Throughput Overall token processing rate sum(total_tokens) / run_duration tokens/second Requests Per Minute Request processing rate num_completed_requests / duration * 60 requests/minute Error Rate Percentage of failed requests num_error_requests / total_requests percentage","title":"Aggregated Metrics"},{"location":"user-guide/overview/#data-flow","text":"","title":"Data Flow"},{"location":"user-guide/overview/#1-configuration-phase","text":"Flow: User \u2192 CLI \u2192 Config Validator \u2192 CLI \u2192 User Steps: 1. User runs benchmark command with parameters 2. CLI receives and parses the command 3. Config Validator validates all parameters and settings 4. CLI receives validation result 5. User gets confirmation to start benchmark or error messages","title":"1. Configuration Phase"},{"location":"user-guide/overview/#2-benchmark-execution","text":"Flow: Benchmark Engine \u2192 Traffic Generator \u2192 API Backend \u2192 Traffic Generator \u2192 Metrics Collector \u2192 Benchmark Engine Steps: 1. Benchmark Engine initiates traffic generation 2. Traffic Generator sends requests to API backend 3. API Backend processes requests and returns responses 4. Traffic Generator receives responses 5. Metrics Collector collects performance metrics 6. Benchmark Engine stores results for analysis","title":"2. Benchmark Execution"},{"location":"user-guide/overview/#3-analysis-phase","text":"Flow: User \u2192 CLI \u2192 Analysis Tools \u2192 CLI \u2192 User Steps: 1. User requests analysis (Excel reports, plots, etc.) 2. CLI processes the analysis request 3. Analysis Tools process results and generate reports/plots 4. CLI receives generated analysis output 5. User gets analysis results (files, visualizations)","title":"3. Analysis Phase"},{"location":"user-guide/overview/#key-features","text":"","title":"Key Features"},{"location":"user-guide/overview/#real-time-monitoring","text":"Live UI Dashboard : Web-based interface showing real-time progress Rich Logging : Detailed logs with automatic file output Progress Tracking : Visual indicators of benchmark status","title":"Real-time Monitoring"},{"location":"user-guide/overview/#comprehensive-analysis","text":"Excel Reports : Detailed spreadsheets with all metrics Custom Plots : Flexible visualization with 2x4 grid layouts Multi-experiment Comparison : Compare results across different runs","title":"Comprehensive Analysis"},{"location":"user-guide/overview/#scalability","text":"Distributed Benchmarking : Run benchmarks across multiple machines Configurable Load : Adjust traffic patterns and concurrency levels Resource Management : Efficient memory and CPU usage","title":"Scalability"},{"location":"user-guide/overview/#best-practices","text":"","title":"Best Practices"},{"location":"user-guide/overview/#benchmark-design","text":"Start Simple : Begin with basic configurations Gradual Scaling : Increase load gradually to find limits Multiple Scenarios : Test different traffic patterns Consistent Environment : Use same hardware/software for comparisons","title":"Benchmark Design"},{"location":"user-guide/overview/#result-interpretation","text":"Context Matters : Consider your specific use case Look for Trends : Focus on patterns, not single data points Compare Apples to Apples : Use consistent parameters Validate Results : Run multiple times to ensure consistency","title":"Result Interpretation"},{"location":"user-guide/overview/#next-steps","text":"Learn about the Command Line Interface for detailed usage Explore Tasks and Benchmarks for specific scenarios Check out Results Analysis for understanding your data See Examples for practical use cases","title":"Next Steps"},{"location":"user-guide/tasks/","text":"Tasks and Benchmarks \u00b6 This guide explains the different task types supported by GenAI Bench and how to configure them for various benchmarking scenarios. Task Overview \u00b6 Tasks in GenAI Bench define the type of benchmark you want to run, based on the input modality (e.g., text, image) and output modality (e.g., text, embeddings). Each task follows the pattern: <input_modality>-to-<output_modality> Supported Tasks \u00b6 Text-to-Text ( text-to-text ) \u00b6 Generate text responses from text input, such as chat completions or question answering. Use Cases \u00b6 Chat Applications : Conversational AI, customer support Question Answering : Knowledge base queries, educational content Text Generation : Content creation, summarization Code Generation : Programming assistance, code completion Configuration Example \u00b6 genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --dataset-name \"sonnet.txt\" \\ --max-time-per-run 300 Request Format \u00b6 { \"messages\" : [ { \"role\" : \"user\" , \"content\" : \"What is the capital of France?\" } ], \"max_tokens\" : 100 , \"temperature\" : 0.7 } Text-to-Embeddings ( text-to-embeddings ) \u00b6 Generate embeddings from text input for semantic search and similarity tasks. Use Cases \u00b6 Semantic Search : Document retrieval, content discovery Similarity Matching : Duplicate detection, clustering Recommendation Systems : Content recommendations Information Retrieval : Search engines, knowledge graphs Configuration Example \u00b6 genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"text-embedding-ada-002\" \\ --task text-to-embeddings \\ --dataset-name \"sonnet.txt\" \\ --max-time-per-run 300 Request Format \u00b6 { \"input\" : \"This is a sample text for embedding generation\" , \"model\" : \"text-embedding-ada-002\" } Image-Text-to-Text ( image-text-to-text ) \u00b6 Generate text responses from images combined with text prompts. Use Cases \u00b6 Visual Question Answering : Image-based Q&A Image Captioning : Descriptive text generation Visual Analysis : Object detection, scene understanding Multimodal Chat : Interactive image-based conversations Configuration Example \u00b6 genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"gpt-4-vision\" \\ --task image-text-to-text \\ --dataset-name \"vision-dataset\" \\ --max-time-per-run 300 Request Format \u00b6 { \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What do you see in this image?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : \"data:image/jpeg;base64,...\" }} ] } ], \"max_tokens\" : 150 } Image-to-Embeddings ( image-to-embeddings ) \u00b6 Generate embeddings from images for image similarity and search tasks. Use Cases \u00b6 Image Similarity : Duplicate image detection Visual Search : Find similar images Content Moderation : Inappropriate content detection Image Clustering : Organize image collections Configuration Example \u00b6 genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"clip-vit-base-patch32\" \\ --task image-to-embeddings \\ --dataset-name \"image-dataset\" \\ --max-time-per-run 300 Request Format \u00b6 { \"input\" : \"data:image/jpeg;base64,...\" , \"model\" : \"clip-vit-base-patch32\" } Task Compatibility \u00b6 API Backend Support \u00b6 Task OpenAI Cohere OCI Cohere text-to-text \u2705 \u2705 \u2705 text-to-embeddings \u2705 \u2705 \u2705 image-text-to-text \u2705 \u2705 \u2705 image-to-embeddings \u2705 \u2705 \u2705 Model Requirements \u00b6 Text-to-Text Models \u00b6 Chat Models : GPT-3.5-turbo, GPT-4, Llama-2, etc. Completion Models : text-davinci-003, etc. Embedding Models \u00b6 Text Embeddings : text-embedding-ada-002, Cohere embeddings Image Embeddings : CLIP, DALL-E embeddings Vision Models \u00b6 Multimodal : GPT-4V, Claude 3 Vision, etc. Image Understanding : Models with vision capabilities Dataset Configuration \u00b6 Built-in Datasets \u00b6 GenAI Bench provides several built-in datasets: # Text datasets --dataset-name \"sonnet.txt\" --dataset-name \"qa_dataset.txt\" # Vision datasets --dataset-name \"vision_dataset\" --dataset-name \"image_qa_dataset\" Custom Datasets \u00b6 Text Datasets \u00b6 Create a text file with one prompt per line: # prompts.txt What is the capital of France? Explain quantum computing in simple terms. Write a short poem about spring. --dataset-path \"/path/to/prompts.txt\" Vision Datasets \u00b6 For vision tasks, organize your dataset: vision_dataset/ \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 image1.jpg \u2502 \u251c\u2500\u2500 image2.png \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 prompts.txt \u2514\u2500\u2500 metadata.json --dataset-path \"/path/to/vision_dataset\" Hugging Face Datasets \u00b6 Use datasets from Hugging Face: --dataset-name \"squad\" --dataset-split \"train\" --dataset-column \"question\" Advanced Task Configuration \u00b6 Tokenizer Configuration \u00b6 For accurate token counting, specify the tokenizer: # Use model's tokenizer --model-tokenizer \"/path/to/tokenizer\" # Use Hugging Face tokenizer --model-tokenizer \"meta-llama/Llama-2-7b-chat-hf\" # Use specific tokenizer --model-tokenizer \"gpt2\" Request Parameters \u00b6 Customize request parameters for different tasks: Text-to-Text Parameters \u00b6 # Temperature (creativity) --temperature 0 .7 # Maximum tokens --max-tokens 100 # Top-p sampling --top-p 0 .9 # Frequency penalty --frequency-penalty 0 .1 # Presence penalty --presence-penalty 0 .1 Embedding Parameters \u00b6 # Embedding dimensions --embedding-dimensions 1536 # Normalize embeddings --normalize-embeddings true Task-Specific Metrics \u00b6 Different tasks collect different metrics: Text-to-Text Metrics \u00b6 TTFT : Time to first token TPOT : Time per output token Output throughput : Tokens per second Response quality : Perplexity, accuracy Embedding Metrics \u00b6 Embedding latency : Time to generate embeddings Embedding quality : Cosine similarity, clustering quality Batch processing : Throughput for multiple inputs Vision Metrics \u00b6 Image processing time : Time to process images Multimodal latency : Combined text + image processing Vision accuracy : Object detection, caption quality Best Practices \u00b6 Task Selection \u00b6 Match Your Use Case : Choose the task that matches your application Consider Model Capabilities : Ensure your model supports the task Test Compatibility : Verify API backend support Dataset Preparation \u00b6 Representative Data : Use data similar to your production workload Appropriate Size : Balance between coverage and benchmark duration Quality Control : Ensure data quality and consistency Benchmark Configuration \u00b6 Start Simple : Begin with basic configurations Gradual Scaling : Increase complexity step by step Consistent Parameters : Use same settings for comparisons Result Interpretation \u00b6 Task-Specific Metrics : Focus on relevant metrics for your task Context Matters : Consider your specific use case requirements Baseline Comparison : Compare against known good performance Troubleshooting \u00b6 Common Issues \u00b6 Task Not Supported \u00b6 Error: Task 'unsupported-task' is not supported by backend 'openai' Solution : Check task compatibility with your API backend. Model Incompatibility \u00b6 Error: Model 'text-model' does not support vision tasks Solution : Use a model that supports your task type. Dataset Issues \u00b6 Error: Dataset format not compatible with task 'image-text-to-text' Solution : Ensure your dataset matches the task requirements. Getting Help \u00b6 Check the CLI Reference for detailed command options Review Examples for practical scenarios Open an issue for bugs Next Steps \u00b6 Learn about Results Analysis for understanding metrics Explore Examples for practical use cases Check out the CLI Reference for complete command documentation","title":"Tasks and Benchmarks"},{"location":"user-guide/tasks/#tasks-and-benchmarks","text":"This guide explains the different task types supported by GenAI Bench and how to configure them for various benchmarking scenarios.","title":"Tasks and Benchmarks"},{"location":"user-guide/tasks/#task-overview","text":"Tasks in GenAI Bench define the type of benchmark you want to run, based on the input modality (e.g., text, image) and output modality (e.g., text, embeddings). Each task follows the pattern: <input_modality>-to-<output_modality>","title":"Task Overview"},{"location":"user-guide/tasks/#supported-tasks","text":"","title":"Supported Tasks"},{"location":"user-guide/tasks/#text-to-text-text-to-text","text":"Generate text responses from text input, such as chat completions or question answering.","title":"Text-to-Text (text-to-text)"},{"location":"user-guide/tasks/#use-cases","text":"Chat Applications : Conversational AI, customer support Question Answering : Knowledge base queries, educational content Text Generation : Content creation, summarization Code Generation : Programming assistance, code completion","title":"Use Cases"},{"location":"user-guide/tasks/#configuration-example","text":"genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"llama-2-7b\" \\ --task text-to-text \\ --dataset-name \"sonnet.txt\" \\ --max-time-per-run 300","title":"Configuration Example"},{"location":"user-guide/tasks/#request-format","text":"{ \"messages\" : [ { \"role\" : \"user\" , \"content\" : \"What is the capital of France?\" } ], \"max_tokens\" : 100 , \"temperature\" : 0.7 }","title":"Request Format"},{"location":"user-guide/tasks/#text-to-embeddings-text-to-embeddings","text":"Generate embeddings from text input for semantic search and similarity tasks.","title":"Text-to-Embeddings (text-to-embeddings)"},{"location":"user-guide/tasks/#use-cases_1","text":"Semantic Search : Document retrieval, content discovery Similarity Matching : Duplicate detection, clustering Recommendation Systems : Content recommendations Information Retrieval : Search engines, knowledge graphs","title":"Use Cases"},{"location":"user-guide/tasks/#configuration-example_1","text":"genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"text-embedding-ada-002\" \\ --task text-to-embeddings \\ --dataset-name \"sonnet.txt\" \\ --max-time-per-run 300","title":"Configuration Example"},{"location":"user-guide/tasks/#request-format_1","text":"{ \"input\" : \"This is a sample text for embedding generation\" , \"model\" : \"text-embedding-ada-002\" }","title":"Request Format"},{"location":"user-guide/tasks/#image-text-to-text-image-text-to-text","text":"Generate text responses from images combined with text prompts.","title":"Image-Text-to-Text (image-text-to-text)"},{"location":"user-guide/tasks/#use-cases_2","text":"Visual Question Answering : Image-based Q&A Image Captioning : Descriptive text generation Visual Analysis : Object detection, scene understanding Multimodal Chat : Interactive image-based conversations","title":"Use Cases"},{"location":"user-guide/tasks/#configuration-example_2","text":"genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"gpt-4-vision\" \\ --task image-text-to-text \\ --dataset-name \"vision-dataset\" \\ --max-time-per-run 300","title":"Configuration Example"},{"location":"user-guide/tasks/#request-format_2","text":"{ \"messages\" : [ { \"role\" : \"user\" , \"content\" : [ { \"type\" : \"text\" , \"text\" : \"What do you see in this image?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : \"data:image/jpeg;base64,...\" }} ] } ], \"max_tokens\" : 150 }","title":"Request Format"},{"location":"user-guide/tasks/#image-to-embeddings-image-to-embeddings","text":"Generate embeddings from images for image similarity and search tasks.","title":"Image-to-Embeddings (image-to-embeddings)"},{"location":"user-guide/tasks/#use-cases_3","text":"Image Similarity : Duplicate image detection Visual Search : Find similar images Content Moderation : Inappropriate content detection Image Clustering : Organize image collections","title":"Use Cases"},{"location":"user-guide/tasks/#configuration-example_3","text":"genai-bench benchmark \\ --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-api-key\" \\ --api-model-name \"clip-vit-base-patch32\" \\ --task image-to-embeddings \\ --dataset-name \"image-dataset\" \\ --max-time-per-run 300","title":"Configuration Example"},{"location":"user-guide/tasks/#request-format_3","text":"{ \"input\" : \"data:image/jpeg;base64,...\" , \"model\" : \"clip-vit-base-patch32\" }","title":"Request Format"},{"location":"user-guide/tasks/#task-compatibility","text":"","title":"Task Compatibility"},{"location":"user-guide/tasks/#api-backend-support","text":"Task OpenAI Cohere OCI Cohere text-to-text \u2705 \u2705 \u2705 text-to-embeddings \u2705 \u2705 \u2705 image-text-to-text \u2705 \u2705 \u2705 image-to-embeddings \u2705 \u2705 \u2705","title":"API Backend Support"},{"location":"user-guide/tasks/#model-requirements","text":"","title":"Model Requirements"},{"location":"user-guide/tasks/#text-to-text-models","text":"Chat Models : GPT-3.5-turbo, GPT-4, Llama-2, etc. Completion Models : text-davinci-003, etc.","title":"Text-to-Text Models"},{"location":"user-guide/tasks/#embedding-models","text":"Text Embeddings : text-embedding-ada-002, Cohere embeddings Image Embeddings : CLIP, DALL-E embeddings","title":"Embedding Models"},{"location":"user-guide/tasks/#vision-models","text":"Multimodal : GPT-4V, Claude 3 Vision, etc. Image Understanding : Models with vision capabilities","title":"Vision Models"},{"location":"user-guide/tasks/#dataset-configuration","text":"","title":"Dataset Configuration"},{"location":"user-guide/tasks/#built-in-datasets","text":"GenAI Bench provides several built-in datasets: # Text datasets --dataset-name \"sonnet.txt\" --dataset-name \"qa_dataset.txt\" # Vision datasets --dataset-name \"vision_dataset\" --dataset-name \"image_qa_dataset\"","title":"Built-in Datasets"},{"location":"user-guide/tasks/#custom-datasets","text":"","title":"Custom Datasets"},{"location":"user-guide/tasks/#text-datasets","text":"Create a text file with one prompt per line: # prompts.txt What is the capital of France? Explain quantum computing in simple terms. Write a short poem about spring. --dataset-path \"/path/to/prompts.txt\"","title":"Text Datasets"},{"location":"user-guide/tasks/#vision-datasets","text":"For vision tasks, organize your dataset: vision_dataset/ \u251c\u2500\u2500 images/ \u2502 \u251c\u2500\u2500 image1.jpg \u2502 \u251c\u2500\u2500 image2.png \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 prompts.txt \u2514\u2500\u2500 metadata.json --dataset-path \"/path/to/vision_dataset\"","title":"Vision Datasets"},{"location":"user-guide/tasks/#hugging-face-datasets","text":"Use datasets from Hugging Face: --dataset-name \"squad\" --dataset-split \"train\" --dataset-column \"question\"","title":"Hugging Face Datasets"},{"location":"user-guide/tasks/#advanced-task-configuration","text":"","title":"Advanced Task Configuration"},{"location":"user-guide/tasks/#tokenizer-configuration","text":"For accurate token counting, specify the tokenizer: # Use model's tokenizer --model-tokenizer \"/path/to/tokenizer\" # Use Hugging Face tokenizer --model-tokenizer \"meta-llama/Llama-2-7b-chat-hf\" # Use specific tokenizer --model-tokenizer \"gpt2\"","title":"Tokenizer Configuration"},{"location":"user-guide/tasks/#request-parameters","text":"Customize request parameters for different tasks:","title":"Request Parameters"},{"location":"user-guide/tasks/#text-to-text-parameters","text":"# Temperature (creativity) --temperature 0 .7 # Maximum tokens --max-tokens 100 # Top-p sampling --top-p 0 .9 # Frequency penalty --frequency-penalty 0 .1 # Presence penalty --presence-penalty 0 .1","title":"Text-to-Text Parameters"},{"location":"user-guide/tasks/#embedding-parameters","text":"# Embedding dimensions --embedding-dimensions 1536 # Normalize embeddings --normalize-embeddings true","title":"Embedding Parameters"},{"location":"user-guide/tasks/#task-specific-metrics","text":"Different tasks collect different metrics:","title":"Task-Specific Metrics"},{"location":"user-guide/tasks/#text-to-text-metrics","text":"TTFT : Time to first token TPOT : Time per output token Output throughput : Tokens per second Response quality : Perplexity, accuracy","title":"Text-to-Text Metrics"},{"location":"user-guide/tasks/#embedding-metrics","text":"Embedding latency : Time to generate embeddings Embedding quality : Cosine similarity, clustering quality Batch processing : Throughput for multiple inputs","title":"Embedding Metrics"},{"location":"user-guide/tasks/#vision-metrics","text":"Image processing time : Time to process images Multimodal latency : Combined text + image processing Vision accuracy : Object detection, caption quality","title":"Vision Metrics"},{"location":"user-guide/tasks/#best-practices","text":"","title":"Best Practices"},{"location":"user-guide/tasks/#task-selection","text":"Match Your Use Case : Choose the task that matches your application Consider Model Capabilities : Ensure your model supports the task Test Compatibility : Verify API backend support","title":"Task Selection"},{"location":"user-guide/tasks/#dataset-preparation","text":"Representative Data : Use data similar to your production workload Appropriate Size : Balance between coverage and benchmark duration Quality Control : Ensure data quality and consistency","title":"Dataset Preparation"},{"location":"user-guide/tasks/#benchmark-configuration","text":"Start Simple : Begin with basic configurations Gradual Scaling : Increase complexity step by step Consistent Parameters : Use same settings for comparisons","title":"Benchmark Configuration"},{"location":"user-guide/tasks/#result-interpretation","text":"Task-Specific Metrics : Focus on relevant metrics for your task Context Matters : Consider your specific use case requirements Baseline Comparison : Compare against known good performance","title":"Result Interpretation"},{"location":"user-guide/tasks/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"user-guide/tasks/#common-issues","text":"","title":"Common Issues"},{"location":"user-guide/tasks/#task-not-supported","text":"Error: Task 'unsupported-task' is not supported by backend 'openai' Solution : Check task compatibility with your API backend.","title":"Task Not Supported"},{"location":"user-guide/tasks/#model-incompatibility","text":"Error: Model 'text-model' does not support vision tasks Solution : Use a model that supports your task type.","title":"Model Incompatibility"},{"location":"user-guide/tasks/#dataset-issues","text":"Error: Dataset format not compatible with task 'image-text-to-text' Solution : Ensure your dataset matches the task requirements.","title":"Dataset Issues"},{"location":"user-guide/tasks/#getting-help","text":"Check the CLI Reference for detailed command options Review Examples for practical scenarios Open an issue for bugs","title":"Getting Help"},{"location":"user-guide/tasks/#next-steps","text":"Learn about Results Analysis for understanding metrics Explore Examples for practical use cases Check out the CLI Reference for complete command documentation","title":"Next Steps"}]}