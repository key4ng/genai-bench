{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"GenAI Bench \u00b6 Unified, accurate, and beautiful LLM Benchmarking What is GenAI Bench? \u00b6 Genai-bench is a powerful benchmark tool designed for comprehensive token-level performance evaluation of large language model (LLM) serving systems. It provides detailed insights into model serving performance, offering both a user-friendly CLI and a live UI for real-time progress monitoring. Live UI Dashboard \u00b6 GenAI Bench includes a real-time dashboard that provides live monitoring of your benchmarks: Key Features \u00b6 \ud83d\udee0\ufe0f CLI Tool : Validates user inputs and initiates benchmarks seamlessly. \ud83d\udcca Live UI Dashboard : Displays current progress, logs, and real-time metrics. \ud83d\udcdd Rich Logs : Automatically flushed to both terminal and file upon experiment completion. \ud83d\udcc8 Experiment Analyzer : Generates comprehensive Excel reports with pricing and raw metrics data, plus flexible plot configurations (default 2x4 grid) that visualize key performance metrics including throughput, latency (TTFT, E2E, TPOT), error rates, and RPS across different traffic scenarios and concurrency levels. Supports custom plot layouts and multi-line comparisons. Quick Start \u00b6 Get started with GenAI Bench in minutes: # Install from PyPI pip install genai-bench # Run your first benchmark genai-bench benchmark --help For detailed installation and usage instructions, see our Installation Guide . Supported Tasks \u00b6 GenAI Bench supports multiple benchmark types: Task Description Use Case text-to-text Benchmarks generating text output from text input Chat, QA text-to-embeddings Benchmarks generating embeddings from text input Semantic search image-text-to-text Benchmarks generating text from images and text prompts Visual question answering image-to-embeddings Benchmarks generating embeddings from images Image similarity Documentation Sections \u00b6 \ud83d\ude80 Getting Started \u00b6 Installation - Detailed installation guide Task Definition - Understanding different benchmark tasks Command Guidelines - Command usage guidelines Metrics Definition - Understanding benchmark metrics \ud83d\udcd6 User Guide \u00b6 Run Benchmark - How to run benchmarks Run Benchmark with Docker - Docker-based benchmarking Generate Excel Sheet - Creating Excel reports Generate Plot - Creating visualizations Upload Benchmark Results - Uploading results \ud83d\udd27 Development \u00b6 Contributing - How to contribute to GenAI Bench Support \u00b6 If you encounter any issues or have questions, please: - Check our documentation for detailed guides - Report issues on our GitHub repository - Join our community discussions License \u00b6 GenAI Bench is open source and available under the MIT License.","title":"Home"},{"location":"#genai-bench","text":"Unified, accurate, and beautiful LLM Benchmarking","title":"GenAI Bench"},{"location":"#what-is-genai-bench","text":"Genai-bench is a powerful benchmark tool designed for comprehensive token-level performance evaluation of large language model (LLM) serving systems. It provides detailed insights into model serving performance, offering both a user-friendly CLI and a live UI for real-time progress monitoring.","title":"What is GenAI Bench?"},{"location":"#live-ui-dashboard","text":"GenAI Bench includes a real-time dashboard that provides live monitoring of your benchmarks:","title":"Live UI Dashboard"},{"location":"#key-features","text":"\ud83d\udee0\ufe0f CLI Tool : Validates user inputs and initiates benchmarks seamlessly. \ud83d\udcca Live UI Dashboard : Displays current progress, logs, and real-time metrics. \ud83d\udcdd Rich Logs : Automatically flushed to both terminal and file upon experiment completion. \ud83d\udcc8 Experiment Analyzer : Generates comprehensive Excel reports with pricing and raw metrics data, plus flexible plot configurations (default 2x4 grid) that visualize key performance metrics including throughput, latency (TTFT, E2E, TPOT), error rates, and RPS across different traffic scenarios and concurrency levels. Supports custom plot layouts and multi-line comparisons.","title":"Key Features"},{"location":"#quick-start","text":"Get started with GenAI Bench in minutes: # Install from PyPI pip install genai-bench # Run your first benchmark genai-bench benchmark --help For detailed installation and usage instructions, see our Installation Guide .","title":"Quick Start"},{"location":"#supported-tasks","text":"GenAI Bench supports multiple benchmark types: Task Description Use Case text-to-text Benchmarks generating text output from text input Chat, QA text-to-embeddings Benchmarks generating embeddings from text input Semantic search image-text-to-text Benchmarks generating text from images and text prompts Visual question answering image-to-embeddings Benchmarks generating embeddings from images Image similarity","title":"Supported Tasks"},{"location":"#documentation-sections","text":"","title":"Documentation Sections"},{"location":"#getting-started","text":"Installation - Detailed installation guide Task Definition - Understanding different benchmark tasks Command Guidelines - Command usage guidelines Metrics Definition - Understanding benchmark metrics","title":"\ud83d\ude80 Getting Started"},{"location":"#user-guide","text":"Run Benchmark - How to run benchmarks Run Benchmark with Docker - Docker-based benchmarking Generate Excel Sheet - Creating Excel reports Generate Plot - Creating visualizations Upload Benchmark Results - Uploading results","title":"\ud83d\udcd6 User Guide"},{"location":"#development","text":"Contributing - How to contribute to GenAI Bench","title":"\ud83d\udd27 Development"},{"location":"#support","text":"If you encounter any issues or have questions, please: - Check our documentation for detailed guides - Report issues on our GitHub repository - Join our community discussions","title":"Support"},{"location":"#license","text":"GenAI Bench is open source and available under the MIT License.","title":"License"},{"location":"development/contributing/","text":"Contribution Guideline \u00b6 Welcome and thank you for your interest in contributing to genai-bench. Coding Style Guide \u00b6 genai-bench uses python 3.11, and we adhere to Google Python style guide . We use make format to format our code using isort and ruff . The detailed configuration can be found in pyproject.toml . Pull Requests \u00b6 When submitting a pull request, please: git pull origin main to make sure your code has been rebased on top of the latest commit on the main branch. Ensure code is properly formatted and passed every lint checks by running make check . Add new test cases to stay robust and correct. In the case of a bug fix, the tests should fail without your code changes. For new features, try to cover as many variants as reasonably possible. You can use make test to check the test coverage. Or use make test_changed to test the test coverage for your own branch. We enforce to keep a test coverage about 90%. PR Template \u00b6 It is required to classify your PR and make the commit message concise and useful. Prefix the PR title appropriately to indicate the type of change. Please use one of the following: [Bugfix] for bug fixes. [Core] for core backend changes. This includes build, version upgrade, changes in user and sampling. [Metrics] for changes made to metrics. [Frontend] for UI dashboard and CLI entrypoint related changes. [Docs] for changes related to documentation. [CI/Tests] for unittests and integration tests. [Report] for changes in generating plots and excel reports. [Misc] for PRs that do not fit the above categories. Please use this sparingly. Open source community also recommends to keep the commit message title within 52 chars and each line in message content within 72 chars. Code Reviews \u00b6 All submissions, including submissions by project members, require a code review. To make the review process as smooth as possible, please: Keep your changes as concise as possible. If your pull request involves multiple unrelated changes, consider splitting it into separate pull requests. Respond to all comments within a reasonable time frame. If a comment isn't clear, or you disagree with a suggestion, feel free to ask for clarification or discuss the suggestion. Provide constructive feedback and meaningful comments. Focus on specific improvements and suggestions that can enhance the code quality or functionality. Remember to acknowledge and respect the work the author has already put into the submission. Setup Development Environment \u00b6 make \u00b6 genai-bench utilizes make for a lot of useful commands. If your laptop doesn't have GNU make installed, (check this by typing make --version in your terminal), you can ask our GenerativeAI's chatbot about how to install it in your system. uv \u00b6 Install uv with make uv or install it from the official website . If installing from the website, create a project venv with uv venv -p python3.11 . Once you have make and uv installed, you can follow the command below to build genai-bench wheel: # check out commands genai-bench supports make help #activate virtual env managed by uv source .venv/bin/activate # install dependencies make install You can utilize wheel to install genai-bench. # build a .whl under genai-bench/dist make build # send the wheel to your remote machine if applies rsync --delete -avz ~/genai-bench/dist/<.wheel> <remote-user>@<remote-ip>:<dest-addr> On your remote machine, you can simply use the pip to install genai-bench. pip install <dest-addr>/<.wheel> Development Guide: Adding a New Task in genai-bench \u00b6 This guide explains how to add support for a new task in genai-bench . Follow the steps below to ensure consistency and compatibility with the existing codebase. 1. Define the Request and Response in protocol.py \u00b6 Steps \u00b6 Add relevant fields to the appropriate request/response data classes in protocol.py If the new task involves a new input-output modality, create a new request/response class. Use existing request/response classes ( UserChatRequest , UserEmbeddingRequest , UserImageChatRequest , etc.) if they suffice. Example \u00b6 class UserTextToImageRequest ( UserRequest ): \"\"\"Represents a request for generating images from text.\"\"\" prompt : str num_images : int = Field ( ... , description = \"Number of images to generate.\" ) image_resolution : Tuple [ int , int ] = Field ( ... , description = \"Resolution of the generated images.\" ) 2. Update or Create a Sampler \u00b6 2.1 If Input Modality Is Supported by an Existing Sampler \u00b6 Check if the current TextSampler or ImageSampler supports the input-modality. Add request creation logic in the relevant TextSampler or ImageSampler class. Refactor the sampler's _create_request method to support the new task. Tip: Avoid adding long if-else chains for new tasks. Utilize helper methods or design a request creator pattern if needed. 2.2 If Input Modality Is Not Supported \u00b6 Create a new sampler class inheriting from BaseSampler . Define the sample method to generate requests for the new task. Refer to TextSampler and ImageSampler for implementation patterns. Add utility functions for data preprocessing or validation specific to the new modality if necessary. Example for a New Sampler \u00b6 class AudioSampler ( Sampler ): input_modality = \"audio\" supported_tasks = { \"audio-to-text\" , \"audio-to-embeddings\" } def sample ( self , scenario : Scenario ) -> UserRequest : # Validate scenario self . _validate_scenario ( scenario ) if self . output_modality == \"text\" : return self . _create_audio_to_text_request ( scenario ) elif self . output_modality == \"embeddings\" : return self . _create_audio_to_embeddings_request ( scenario ) else : raise ValueError ( f \"Unsupported output_modality: { self . output_modality } \" ) 3. Add Task Support in the User Class \u00b6 Each User corresponds to one API backend, such as OpenAIUser for OpenAI. Users can have multiple tasks, each corresponding to an endpoint. Steps \u00b6 Add the new task to the supported_tasks dictionary in the relevant User class. Map the new task to its corresponding function name in the dictionary. Implement the new function in the User class for handling the task logic. If the new task uses an existing endpoint, refactor the function to support both tasks without duplicating logic. Important: Avoid creating multiple functions for tasks that use the same endpoint. Example \u00b6 class OpenAIUser ( BaseUser ): supported_tasks = { \"text-to-text\" : \"chat\" , \"image-text-to-text\" : \"chat\" , \"text-to-embeddings\" : \"embeddings\" , \"audio-to-text\" : \"audio_to_text\" , # New task added } def audio_to_text ( self ): # Implement the logic for audio-to-text task endpoint = \"/v1/audio/transcriptions\" user_request = self . sample () # Add payload and send request payload = { \"audio\" : user_request . audio_file } self . send_request ( False , endpoint , payload , self . parse_audio_response ) 4. Add Unit Tests \u00b6 Steps \u00b6 Add tests for the new task in the appropriate test files. Include tests for: Request creation in the sampler. Task validation in the User class. End-to-end workflow using the new task. 5. Update Documentation \u00b6 Steps \u00b6 Add the new task to the list of supported tasks in the Task Definition guide . Provide sample commands and explain any required configuration changes. Mention the new task in this contributing guide for future developers.","title":"Contributing"},{"location":"development/contributing/#contribution-guideline","text":"Welcome and thank you for your interest in contributing to genai-bench.","title":"Contribution Guideline"},{"location":"development/contributing/#coding-style-guide","text":"genai-bench uses python 3.11, and we adhere to Google Python style guide . We use make format to format our code using isort and ruff . The detailed configuration can be found in pyproject.toml .","title":"Coding Style Guide"},{"location":"development/contributing/#pull-requests","text":"When submitting a pull request, please: git pull origin main to make sure your code has been rebased on top of the latest commit on the main branch. Ensure code is properly formatted and passed every lint checks by running make check . Add new test cases to stay robust and correct. In the case of a bug fix, the tests should fail without your code changes. For new features, try to cover as many variants as reasonably possible. You can use make test to check the test coverage. Or use make test_changed to test the test coverage for your own branch. We enforce to keep a test coverage about 90%.","title":"Pull Requests"},{"location":"development/contributing/#pr-template","text":"It is required to classify your PR and make the commit message concise and useful. Prefix the PR title appropriately to indicate the type of change. Please use one of the following: [Bugfix] for bug fixes. [Core] for core backend changes. This includes build, version upgrade, changes in user and sampling. [Metrics] for changes made to metrics. [Frontend] for UI dashboard and CLI entrypoint related changes. [Docs] for changes related to documentation. [CI/Tests] for unittests and integration tests. [Report] for changes in generating plots and excel reports. [Misc] for PRs that do not fit the above categories. Please use this sparingly. Open source community also recommends to keep the commit message title within 52 chars and each line in message content within 72 chars.","title":"PR Template"},{"location":"development/contributing/#code-reviews","text":"All submissions, including submissions by project members, require a code review. To make the review process as smooth as possible, please: Keep your changes as concise as possible. If your pull request involves multiple unrelated changes, consider splitting it into separate pull requests. Respond to all comments within a reasonable time frame. If a comment isn't clear, or you disagree with a suggestion, feel free to ask for clarification or discuss the suggestion. Provide constructive feedback and meaningful comments. Focus on specific improvements and suggestions that can enhance the code quality or functionality. Remember to acknowledge and respect the work the author has already put into the submission.","title":"Code Reviews"},{"location":"development/contributing/#setup-development-environment","text":"","title":"Setup Development Environment"},{"location":"development/contributing/#make","text":"genai-bench utilizes make for a lot of useful commands. If your laptop doesn't have GNU make installed, (check this by typing make --version in your terminal), you can ask our GenerativeAI's chatbot about how to install it in your system.","title":"make"},{"location":"development/contributing/#uv","text":"Install uv with make uv or install it from the official website . If installing from the website, create a project venv with uv venv -p python3.11 . Once you have make and uv installed, you can follow the command below to build genai-bench wheel: # check out commands genai-bench supports make help #activate virtual env managed by uv source .venv/bin/activate # install dependencies make install You can utilize wheel to install genai-bench. # build a .whl under genai-bench/dist make build # send the wheel to your remote machine if applies rsync --delete -avz ~/genai-bench/dist/<.wheel> <remote-user>@<remote-ip>:<dest-addr> On your remote machine, you can simply use the pip to install genai-bench. pip install <dest-addr>/<.wheel>","title":"uv"},{"location":"development/contributing/#development-guide-adding-a-new-task-in-genai-bench","text":"This guide explains how to add support for a new task in genai-bench . Follow the steps below to ensure consistency and compatibility with the existing codebase.","title":"Development Guide: Adding a New Task in genai-bench"},{"location":"development/contributing/#1-define-the-request-and-response-in-protocolpy","text":"","title":"1. Define the Request and Response in protocol.py"},{"location":"development/contributing/#steps","text":"Add relevant fields to the appropriate request/response data classes in protocol.py If the new task involves a new input-output modality, create a new request/response class. Use existing request/response classes ( UserChatRequest , UserEmbeddingRequest , UserImageChatRequest , etc.) if they suffice.","title":"Steps"},{"location":"development/contributing/#example","text":"class UserTextToImageRequest ( UserRequest ): \"\"\"Represents a request for generating images from text.\"\"\" prompt : str num_images : int = Field ( ... , description = \"Number of images to generate.\" ) image_resolution : Tuple [ int , int ] = Field ( ... , description = \"Resolution of the generated images.\" )","title":"Example"},{"location":"development/contributing/#2-update-or-create-a-sampler","text":"","title":"2. Update or Create a Sampler"},{"location":"development/contributing/#21-if-input-modality-is-supported-by-an-existing-sampler","text":"Check if the current TextSampler or ImageSampler supports the input-modality. Add request creation logic in the relevant TextSampler or ImageSampler class. Refactor the sampler's _create_request method to support the new task. Tip: Avoid adding long if-else chains for new tasks. Utilize helper methods or design a request creator pattern if needed.","title":"2.1 If Input Modality Is Supported by an Existing Sampler"},{"location":"development/contributing/#22-if-input-modality-is-not-supported","text":"Create a new sampler class inheriting from BaseSampler . Define the sample method to generate requests for the new task. Refer to TextSampler and ImageSampler for implementation patterns. Add utility functions for data preprocessing or validation specific to the new modality if necessary.","title":"2.2 If Input Modality Is Not Supported"},{"location":"development/contributing/#example-for-a-new-sampler","text":"class AudioSampler ( Sampler ): input_modality = \"audio\" supported_tasks = { \"audio-to-text\" , \"audio-to-embeddings\" } def sample ( self , scenario : Scenario ) -> UserRequest : # Validate scenario self . _validate_scenario ( scenario ) if self . output_modality == \"text\" : return self . _create_audio_to_text_request ( scenario ) elif self . output_modality == \"embeddings\" : return self . _create_audio_to_embeddings_request ( scenario ) else : raise ValueError ( f \"Unsupported output_modality: { self . output_modality } \" )","title":"Example for a New Sampler"},{"location":"development/contributing/#3-add-task-support-in-the-user-class","text":"Each User corresponds to one API backend, such as OpenAIUser for OpenAI. Users can have multiple tasks, each corresponding to an endpoint.","title":"3. Add Task Support in the User Class"},{"location":"development/contributing/#steps_1","text":"Add the new task to the supported_tasks dictionary in the relevant User class. Map the new task to its corresponding function name in the dictionary. Implement the new function in the User class for handling the task logic. If the new task uses an existing endpoint, refactor the function to support both tasks without duplicating logic. Important: Avoid creating multiple functions for tasks that use the same endpoint.","title":"Steps"},{"location":"development/contributing/#example_1","text":"class OpenAIUser ( BaseUser ): supported_tasks = { \"text-to-text\" : \"chat\" , \"image-text-to-text\" : \"chat\" , \"text-to-embeddings\" : \"embeddings\" , \"audio-to-text\" : \"audio_to_text\" , # New task added } def audio_to_text ( self ): # Implement the logic for audio-to-text task endpoint = \"/v1/audio/transcriptions\" user_request = self . sample () # Add payload and send request payload = { \"audio\" : user_request . audio_file } self . send_request ( False , endpoint , payload , self . parse_audio_response )","title":"Example"},{"location":"development/contributing/#4-add-unit-tests","text":"","title":"4. Add Unit Tests"},{"location":"development/contributing/#steps_2","text":"Add tests for the new task in the appropriate test files. Include tests for: Request creation in the sampler. Task validation in the User class. End-to-end workflow using the new task.","title":"Steps"},{"location":"development/contributing/#5-update-documentation","text":"","title":"5. Update Documentation"},{"location":"development/contributing/#steps_3","text":"Add the new task to the list of supported tasks in the Task Definition guide . Provide sample commands and explain any required configuration changes. Mention the new task in this contributing guide for future developers.","title":"Steps"},{"location":"examples/%20plot-config-examples/","text":"Plot Configuration Examples \u00b6 This directory contains example plot configurations for the flexible plotting system in genai-bench. Usage \u00b6 Use these configurations with the genai-bench plot command: # Use a custom configuration file genai-bench plot --experiments-folder /path/to/experiments \\ --group-key traffic_scenario \\ --plot-config examples/plot_configs/custom_2x2.json # Use a built-in preset for multiple scenarios genai-bench plot --experiments-folder /path/to/experiments \\ --group-key traffic_scenario \\ --preset simple_2x2 # Use multi-line preset for single scenario analysis genai-bench plot --experiments-folder /path/to/experiments \\ --group-key none \\ --preset single_scenario_analysis # List available fields with actual data from your experiment genai-bench plot --experiments-folder /path/to/experiments \\ --group-key traffic_scenario \\ --list-fields # Validate a configuration without generating plots genai-bench plot --experiments-folder /path/to/experiments \\ --group-key traffic_scenario \\ --plot-config examples/plot_configs/custom_2x2.json \\ --validate-only Available Configurations \u00b6 custom_2x2.json \u00b6 A simple 2x2 grid layout focusing on key performance metrics: - Throughput vs Mean Latency - RPS vs P99 Latency - Concurrency vs TTFT - Error Rate Analysis performance_focused.json \u00b6 A comprehensive 2x3 grid for detailed performance analysis: - Token generation speed analysis - Time to first token trends - Latency percentiles - Token efficiency scatter plot - Request success rates - Throughput scaling multi_line_latency.json \u00b6 Demonstrates multi-line plotting capabilities with a 2x2 layout: - Latency Percentiles Comparison : Multiple latency percentiles (mean, P90, P99) on one plot - TTFT Performance Analysis : Mean and P95 TTFT comparison - Token Processing Speed : Output speed vs input throughput comparison - Request Success Metrics : Single-line error rate plot comprehensive_multi_line.json \u00b6 Advanced multi-line example with 1x3 layout showcasing complex comparisons: - E2E Latency Distribution : All percentiles (P25, P50, P75, P90, P99) with custom colors - Throughput Components : Input, output, and total throughput comparison - Token Statistics : Input, output, and total token counts as scatter plot Configuration Format \u00b6 Plot configurations use the following JSON schema: Single-Line Plots \u00b6 { \"layout\" : { \"rows\" : 2 , \"cols\" : 2 , \"figsize\" : [ 16 , 12 ] // Optional: [width, height] in inches }, \"plots\" : [ { \"title\" : \"Plot Title\" , \"x_field\" : \"field.path.from.AggregatedMetrics\" , \"y_field\" : \"another.field.path\" , // Single field \"x_label\" : \"Custom X Label\" , // Optional \"y_label\" : \"Custom Y Label\" , // Optional \"plot_type\" : \"line\" , // line, scatter, or bar \"position\" : [ 0 , 0 ] // [row, col] in grid } ] } Multi-Line Plots \u00b6 { \"plots\" : [ { \"title\" : \"Multi-Line Comparison\" , \"x_field\" : \"requests_per_second\" , \"y_fields\" : [ // Multiple fields on same plot { \"field\" : \"stats.e2e_latency.mean\" , \"label\" : \"Mean Latency\" , // Optional custom label \"color\" : \"blue\" , // Optional custom color \"linestyle\" : \"-\" // Optional: '-', '--', '-.', ':' }, { \"field\" : \"stats.e2e_latency.p90\" , \"label\" : \"P90 Latency\" , \"color\" : \"red\" , \"linestyle\" : \"--\" } ], \"x_label\" : \"RPS\" , \"y_label\" : \"Latency (s)\" , \"plot_type\" : \"line\" , \"position\" : [ 0 , 0 ] } ] } Key Features \u00b6 Single vs Multi-Line : Use y_field for single line, y_fields for multiple lines Custom Styling : Each line can have custom color, linestyle, and label Flexible Layout : Any NxM grid layout from 1x1 to 5x6 Plot Types : line , scatter , bar (multi-line bar creates grouped bars) Automatic Legends : Multi-line plots automatically generate legends When to Use Multi-Line Plots \u00b6 \u2705 GOOD Use Cases: - Single scenario analysis : Use --group-key \"\" (empty string) for one traffic scenario - Deep metric comparison : Comparing mean, P90, P99 latency on same plot - Performance analysis : Related metrics on the same scale \u274c AVOID Multi-Line Plots When: - Multiple scenarios : --group-key traffic_scenario with multiple scenarios - Multiple server versions : --group-key server_version - Any grouping : Multi-line + grouping creates cluttered, hard-to-read plots The system will automatically convert multi-line plots to single-line plots when it detects multiple groups/scenarios for better visualization. Usage Patterns \u00b6 # \u2705 GOOD: Multi-line for single scenario analysis genai-bench plot --preset single_scenario_analysis --group-key \"\" # \u2705 GOOD: Single-line for multiple scenarios genai-bench plot --preset 2x4_default --group-key traffic_scenario # \u26a0\ufe0f AUTO-CONVERTED: Multi-line + grouping \u2192 single-line genai-bench plot --preset multi_line_latency --group-key traffic_scenario Available Fields \u00b6 Run genai-bench plot --experiments-folder /path/to/experiments --group-key traffic_scenario --list-fields to see all available field paths with actual data from your experiments. Common field paths include: Direct Metrics \u00b6 num_concurrency - Concurrency level requests_per_second - RPS error_rate - Error rate mean_output_throughput_tokens_per_s - Server output throughput mean_total_tokens_throughput_tokens_per_s - Total throughput run_duration - Duration of the run Statistical Fields \u00b6 Access statistics using stats.{metric}.{statistic} : Metrics: ttft, tpot, e2e_latency, output_latency, output_inference_speed, num_input_tokens, num_output_tokens, total_tokens, input_throughput, output_throughput Statistics: min, max, mean, stddev, sum, p25, p50, p75, p90, p95, p99 Examples: - stats.ttft.mean - Mean time to first token - stats.e2e_latency.p99 - 99th percentile end-to-end latency - stats.output_inference_speed.mean - Mean output inference speed Built-in Presets \u00b6 2x4_default \u00b6 The original 2x4 layout with all 8 standard plots. This maintains backwards compatibility with the existing plotting system. simple_2x2 \u00b6 A simplified 2x2 layout with the most important metrics for quick analysis. Creating Custom Configurations \u00b6 Start with an example configuration Modify the layout dimensions and plot specifications Use --list-fields to find available metrics Use --validate-only to test your configuration Generate plots with your custom config Tips \u00b6 Use descriptive titles for your plots Choose appropriate plot types (line for trends, scatter for relationships, bar for comparisons) Ensure field paths are valid using --validate-only Consider your audience when selecting metrics to display Use figsize to adjust the output image dimensions","title":"Plot Config Examples"},{"location":"examples/%20plot-config-examples/#plot-configuration-examples","text":"This directory contains example plot configurations for the flexible plotting system in genai-bench.","title":"Plot Configuration Examples"},{"location":"examples/%20plot-config-examples/#usage","text":"Use these configurations with the genai-bench plot command: # Use a custom configuration file genai-bench plot --experiments-folder /path/to/experiments \\ --group-key traffic_scenario \\ --plot-config examples/plot_configs/custom_2x2.json # Use a built-in preset for multiple scenarios genai-bench plot --experiments-folder /path/to/experiments \\ --group-key traffic_scenario \\ --preset simple_2x2 # Use multi-line preset for single scenario analysis genai-bench plot --experiments-folder /path/to/experiments \\ --group-key none \\ --preset single_scenario_analysis # List available fields with actual data from your experiment genai-bench plot --experiments-folder /path/to/experiments \\ --group-key traffic_scenario \\ --list-fields # Validate a configuration without generating plots genai-bench plot --experiments-folder /path/to/experiments \\ --group-key traffic_scenario \\ --plot-config examples/plot_configs/custom_2x2.json \\ --validate-only","title":"Usage"},{"location":"examples/%20plot-config-examples/#available-configurations","text":"","title":"Available Configurations"},{"location":"examples/%20plot-config-examples/#custom_2x2json","text":"A simple 2x2 grid layout focusing on key performance metrics: - Throughput vs Mean Latency - RPS vs P99 Latency - Concurrency vs TTFT - Error Rate Analysis","title":"custom_2x2.json"},{"location":"examples/%20plot-config-examples/#performance_focusedjson","text":"A comprehensive 2x3 grid for detailed performance analysis: - Token generation speed analysis - Time to first token trends - Latency percentiles - Token efficiency scatter plot - Request success rates - Throughput scaling","title":"performance_focused.json"},{"location":"examples/%20plot-config-examples/#multi_line_latencyjson","text":"Demonstrates multi-line plotting capabilities with a 2x2 layout: - Latency Percentiles Comparison : Multiple latency percentiles (mean, P90, P99) on one plot - TTFT Performance Analysis : Mean and P95 TTFT comparison - Token Processing Speed : Output speed vs input throughput comparison - Request Success Metrics : Single-line error rate plot","title":"multi_line_latency.json"},{"location":"examples/%20plot-config-examples/#comprehensive_multi_linejson","text":"Advanced multi-line example with 1x3 layout showcasing complex comparisons: - E2E Latency Distribution : All percentiles (P25, P50, P75, P90, P99) with custom colors - Throughput Components : Input, output, and total throughput comparison - Token Statistics : Input, output, and total token counts as scatter plot","title":"comprehensive_multi_line.json"},{"location":"examples/%20plot-config-examples/#configuration-format","text":"Plot configurations use the following JSON schema:","title":"Configuration Format"},{"location":"examples/%20plot-config-examples/#single-line-plots","text":"{ \"layout\" : { \"rows\" : 2 , \"cols\" : 2 , \"figsize\" : [ 16 , 12 ] // Optional: [width, height] in inches }, \"plots\" : [ { \"title\" : \"Plot Title\" , \"x_field\" : \"field.path.from.AggregatedMetrics\" , \"y_field\" : \"another.field.path\" , // Single field \"x_label\" : \"Custom X Label\" , // Optional \"y_label\" : \"Custom Y Label\" , // Optional \"plot_type\" : \"line\" , // line, scatter, or bar \"position\" : [ 0 , 0 ] // [row, col] in grid } ] }","title":"Single-Line Plots"},{"location":"examples/%20plot-config-examples/#multi-line-plots","text":"{ \"plots\" : [ { \"title\" : \"Multi-Line Comparison\" , \"x_field\" : \"requests_per_second\" , \"y_fields\" : [ // Multiple fields on same plot { \"field\" : \"stats.e2e_latency.mean\" , \"label\" : \"Mean Latency\" , // Optional custom label \"color\" : \"blue\" , // Optional custom color \"linestyle\" : \"-\" // Optional: '-', '--', '-.', ':' }, { \"field\" : \"stats.e2e_latency.p90\" , \"label\" : \"P90 Latency\" , \"color\" : \"red\" , \"linestyle\" : \"--\" } ], \"x_label\" : \"RPS\" , \"y_label\" : \"Latency (s)\" , \"plot_type\" : \"line\" , \"position\" : [ 0 , 0 ] } ] }","title":"Multi-Line Plots"},{"location":"examples/%20plot-config-examples/#key-features","text":"Single vs Multi-Line : Use y_field for single line, y_fields for multiple lines Custom Styling : Each line can have custom color, linestyle, and label Flexible Layout : Any NxM grid layout from 1x1 to 5x6 Plot Types : line , scatter , bar (multi-line bar creates grouped bars) Automatic Legends : Multi-line plots automatically generate legends","title":"Key Features"},{"location":"examples/%20plot-config-examples/#when-to-use-multi-line-plots","text":"\u2705 GOOD Use Cases: - Single scenario analysis : Use --group-key \"\" (empty string) for one traffic scenario - Deep metric comparison : Comparing mean, P90, P99 latency on same plot - Performance analysis : Related metrics on the same scale \u274c AVOID Multi-Line Plots When: - Multiple scenarios : --group-key traffic_scenario with multiple scenarios - Multiple server versions : --group-key server_version - Any grouping : Multi-line + grouping creates cluttered, hard-to-read plots The system will automatically convert multi-line plots to single-line plots when it detects multiple groups/scenarios for better visualization.","title":"When to Use Multi-Line Plots"},{"location":"examples/%20plot-config-examples/#usage-patterns","text":"# \u2705 GOOD: Multi-line for single scenario analysis genai-bench plot --preset single_scenario_analysis --group-key \"\" # \u2705 GOOD: Single-line for multiple scenarios genai-bench plot --preset 2x4_default --group-key traffic_scenario # \u26a0\ufe0f AUTO-CONVERTED: Multi-line + grouping \u2192 single-line genai-bench plot --preset multi_line_latency --group-key traffic_scenario","title":"Usage Patterns"},{"location":"examples/%20plot-config-examples/#available-fields","text":"Run genai-bench plot --experiments-folder /path/to/experiments --group-key traffic_scenario --list-fields to see all available field paths with actual data from your experiments. Common field paths include:","title":"Available Fields"},{"location":"examples/%20plot-config-examples/#direct-metrics","text":"num_concurrency - Concurrency level requests_per_second - RPS error_rate - Error rate mean_output_throughput_tokens_per_s - Server output throughput mean_total_tokens_throughput_tokens_per_s - Total throughput run_duration - Duration of the run","title":"Direct Metrics"},{"location":"examples/%20plot-config-examples/#statistical-fields","text":"Access statistics using stats.{metric}.{statistic} : Metrics: ttft, tpot, e2e_latency, output_latency, output_inference_speed, num_input_tokens, num_output_tokens, total_tokens, input_throughput, output_throughput Statistics: min, max, mean, stddev, sum, p25, p50, p75, p90, p95, p99 Examples: - stats.ttft.mean - Mean time to first token - stats.e2e_latency.p99 - 99th percentile end-to-end latency - stats.output_inference_speed.mean - Mean output inference speed","title":"Statistical Fields"},{"location":"examples/%20plot-config-examples/#built-in-presets","text":"","title":"Built-in Presets"},{"location":"examples/%20plot-config-examples/#2x4_default","text":"The original 2x4 layout with all 8 standard plots. This maintains backwards compatibility with the existing plotting system.","title":"2x4_default"},{"location":"examples/%20plot-config-examples/#simple_2x2","text":"A simplified 2x2 layout with the most important metrics for quick analysis.","title":"simple_2x2"},{"location":"examples/%20plot-config-examples/#creating-custom-configurations","text":"Start with an example configuration Modify the layout dimensions and plot specifications Use --list-fields to find available metrics Use --validate-only to test your configuration Generate plots with your custom config","title":"Creating Custom Configurations"},{"location":"examples/%20plot-config-examples/#tips","text":"Use descriptive titles for your plots Choose appropriate plot types (line for trends, scatter for relationships, bar for comparisons) Ensure field paths are valid using --validate-only Consider your audience when selecting metrics to display Use figsize to adjust the output image dimensions","title":"Tips"},{"location":"getting-started/command-guidelines/","text":"Command Guidelines \u00b6 Once you install it in your local environment, you can use --help to read about what command options it supports. genai-bench --help genai-bench supports three commands: Commands: benchmark Run a benchmark based on user defined scenarios. excel Exports the experiment results to an Excel file. plot Plots the experiment ( s ) results based on filters and group... You can also refer to option_groups.py .","title":"Command Guidelines"},{"location":"getting-started/command-guidelines/#command-guidelines","text":"Once you install it in your local environment, you can use --help to read about what command options it supports. genai-bench --help genai-bench supports three commands: Commands: benchmark Run a benchmark based on user defined scenarios. excel Exports the experiment results to an Excel file. plot Plots the experiment ( s ) results based on filters and group... You can also refer to option_groups.py .","title":"Command Guidelines"},{"location":"getting-started/installation/","text":"Installation Guide \u00b6 This guide covers all the ways to install GenAI Bench, from simple PyPI installation to full development setup. Installation Methods \u00b6 Method 1: PyPI Installation (Recommended) \u00b6 The simplest way to install GenAI Bench: pip install genai-bench For a specific version: pip install genai-bench == 0 .1.75 Method 2: Development Installation \u00b6 For development or to use the latest features: Please make sure you have Python3.11 installed. You can check out online how to set it up. Use the virtual environment from uv Activate the virtual environment to ensure the dev environment is correctly set up: make uv source .venv/bin/activate Install the Project in Editable Mode If not already done, install your project in editable mode using make. This ensures that any changes you make are immediately reflected: make install Method 3: Docker Installation \u00b6 For containerized environments: Pull the latest docker image: docker pull ghcr.io/moirai-internal/genai-bench:v0.0.1 Building from Source \u00b6 Alternatively, you can build the image locally from the Dockerfile : docker build . -f Dockerfile -t genai-bench:dev Verification \u00b6 After installation, verify that GenAI Bench is working: # Check version genai-bench --version # Check help genai-bench --help # Check benchmark command genai-bench benchmark --help Environment Setup \u00b6 Environment Variables \u00b6 Set these environment variables for optimal performance: # For Hugging Face tokenizer downloads export HF_TOKEN = \"your-huggingface-token\" # Disable torch warnings (not needed for benchmarking) export TRANSFORMERS_VERBOSITY = error # Optional: Set log level export GENAI_BENCH_LOG_LEVEL = INFO API Keys \u00b6 Depending on your backend, you may need API keys: # OpenAI-compatible APIs export OPENAI_API_KEY = \"your-api-key\" # Cohere API export COHERE_API_KEY = \"your-cohere-key\" # OCI Cohere export OCI_CONFIG_FILE = \"~/.oci/config\" Troubleshooting \u00b6 Common Issues \u00b6 Python Version Issues \u00b6 # Check Python version python3 --version # If you have multiple Python versions, use specific version python3.11 -m pip install genai-bench Permission Issues \u00b6 # Use user installation pip install --user genai-bench # Or use virtual environment python3 -m venv genai-bench-env source genai-bench-env/bin/activate pip install genai-bench Missing Dependencies \u00b6 # Update pip pip install --upgrade pip # Install with all dependencies pip install genai-bench [ dev ] Getting Help \u00b6 If you encounter issues: Check the GitHub Issues Search for similar problems Create a new issue with: Your operating system and Python version Installation method used Full error message Steps to reproduce Next Steps \u00b6 After successful installation: Read the Task Definition Guide to understand different benchmark tasks Explore the User Guide for detailed usage Check out Command Guidelines for practical scenarios","title":"Installation"},{"location":"getting-started/installation/#installation-guide","text":"This guide covers all the ways to install GenAI Bench, from simple PyPI installation to full development setup.","title":"Installation Guide"},{"location":"getting-started/installation/#installation-methods","text":"","title":"Installation Methods"},{"location":"getting-started/installation/#method-1-pypi-installation-recommended","text":"The simplest way to install GenAI Bench: pip install genai-bench For a specific version: pip install genai-bench == 0 .1.75","title":"Method 1: PyPI Installation (Recommended)"},{"location":"getting-started/installation/#method-2-development-installation","text":"For development or to use the latest features: Please make sure you have Python3.11 installed. You can check out online how to set it up. Use the virtual environment from uv Activate the virtual environment to ensure the dev environment is correctly set up: make uv source .venv/bin/activate Install the Project in Editable Mode If not already done, install your project in editable mode using make. This ensures that any changes you make are immediately reflected: make install","title":"Method 2: Development Installation"},{"location":"getting-started/installation/#method-3-docker-installation","text":"For containerized environments: Pull the latest docker image: docker pull ghcr.io/moirai-internal/genai-bench:v0.0.1","title":"Method 3: Docker Installation"},{"location":"getting-started/installation/#building-from-source","text":"Alternatively, you can build the image locally from the Dockerfile : docker build . -f Dockerfile -t genai-bench:dev","title":"Building from Source"},{"location":"getting-started/installation/#verification","text":"After installation, verify that GenAI Bench is working: # Check version genai-bench --version # Check help genai-bench --help # Check benchmark command genai-bench benchmark --help","title":"Verification"},{"location":"getting-started/installation/#environment-setup","text":"","title":"Environment Setup"},{"location":"getting-started/installation/#environment-variables","text":"Set these environment variables for optimal performance: # For Hugging Face tokenizer downloads export HF_TOKEN = \"your-huggingface-token\" # Disable torch warnings (not needed for benchmarking) export TRANSFORMERS_VERBOSITY = error # Optional: Set log level export GENAI_BENCH_LOG_LEVEL = INFO","title":"Environment Variables"},{"location":"getting-started/installation/#api-keys","text":"Depending on your backend, you may need API keys: # OpenAI-compatible APIs export OPENAI_API_KEY = \"your-api-key\" # Cohere API export COHERE_API_KEY = \"your-cohere-key\" # OCI Cohere export OCI_CONFIG_FILE = \"~/.oci/config\"","title":"API Keys"},{"location":"getting-started/installation/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"getting-started/installation/#common-issues","text":"","title":"Common Issues"},{"location":"getting-started/installation/#python-version-issues","text":"# Check Python version python3 --version # If you have multiple Python versions, use specific version python3.11 -m pip install genai-bench","title":"Python Version Issues"},{"location":"getting-started/installation/#permission-issues","text":"# Use user installation pip install --user genai-bench # Or use virtual environment python3 -m venv genai-bench-env source genai-bench-env/bin/activate pip install genai-bench","title":"Permission Issues"},{"location":"getting-started/installation/#missing-dependencies","text":"# Update pip pip install --upgrade pip # Install with all dependencies pip install genai-bench [ dev ]","title":"Missing Dependencies"},{"location":"getting-started/installation/#getting-help","text":"If you encounter issues: Check the GitHub Issues Search for similar problems Create a new issue with: Your operating system and Python version Installation method used Full error message Steps to reproduce","title":"Getting Help"},{"location":"getting-started/installation/#next-steps","text":"After successful installation: Read the Task Definition Guide to understand different benchmark tasks Explore the User Guide for detailed usage Check out Command Guidelines for practical scenarios","title":"Next Steps"},{"location":"getting-started/metrics-definition/","text":"Metrics Definition \u00b6 This section puts together the standard metrics required for LLM serving performance analysis. We classify metrics to two types: single-request level metrics , representing the metrics collected from one request. And aggregated level metrics , summarizing the single-request metrics from one run (with specific traffic scenario and num concurrency). NOTE : Each single-request metric includes standard statistics: percentile , min , max , stddev , and mean . The following metrics cover input , output , and end-to-end (e2e) stages. For chat tasks, all stages are relevant for evaluation. For embedding tasks, where there is no output stage, output metrics will be set to 0. For details about output metrics collection, please check out OUTPUT_METRICS_FIELDS in metrics.py . Single Request Level Metrics \u00b6 The following metrics capture token-level performance for a single request, providing insights into server efficiency for each individual request. Glossary Meaning Calculation Formula Units TTFT Time to First Token. Initial response time when the first output token is generated. This is also known as the latency for the input (input) stage. ttft = time_at_first_token - start_time seconds End-to-End Latency End-to-End latency. This metric indicates how long it takes from submitting a query to receiving the full response, including network latencies. e2e_latency = end_time - start_time seconds TPOT Time Per Output Token. The average time between two subsequent generated tokens. TPOT = (e2e_latency - TTFT) / (num_output_tokens - 1) seconds Output Inference Speed The rate of how many tokens the model can generate per second for a single request. inference_speed = 1 / TPOT tokens/second Num of Input Tokens Number of prompt tokens. num_input_tokens = tokenizer.encode(prompt) tokens Num of Output Tokens Number of output tokens. num_output_tokens = num_completion_tokens tokens Num of Request Tokens Total number of tokens processed in one request. num_request_tokens = num_input_tokens + num_output_tokens tokens Input Throughput The overall throughput of input (input process). input_throughput = num_input_tokens / TTFT tokens/second Output Throughput The throughput of output (output generation) for a single request. output_throughput = (num_output_tokens - 1) / output_latency tokens/second Aggregated Metrics \u00b6 This metrics collection summarizes the metrics relevant to a specific traffic load pattern, defined by the traffic scenario and the num of concurrency. It provides insights into server capacity and performance under pressure. Glossary Meaning Calculation Formula Units Mean Input Throughput The average throughput of how many input tokens can be processed by the model in one run with multiple concurrent requests. mean_input_throughput = sum(input_tokens_for_all_requests) / run_duration tokens/second Mean Output Throughput The average throughput of how many output tokens can be processed by the model in one run with multiple concurrent requests. mean_output_throughput = sum(output_tokens_for_all_requests) / run_duration tokens/second Total Tokens Throughput The average throughput of how many tokens can be processed by the model, including both input and output tokens. mean_total_tokens_throughput = all_requests[\"total_tokens\"][\"sum\"] / run_duration tokens/second Total Chars Per Hour 1 The average total characters can be processed by the model per hour. total_chars_per_hour = total_tokens_throughput * dataset_chars_to_token_ratio * 3600 Characters Requests Per Minute The number of requests processed by the model per minute. num_completed_requests_per_min = num_completed_requests / (end_time - start_time) * 60 Requests Error Codes to Frequency A map that shows the returned error status code to its frequency. Error Rate The rate of error requests over total requests. error_rate = num_error_requests / num_requests Num of Error Requests The number of error requests in one load. if requests.status_code != '200': num_error_requests += 1 Num of Completed Requests The number of completed requests in one load. if requests.status_code == '200': num_completed_requests += 1 Num of Requests The total number of requests processed for one load. total_requests = num_completed_requests + num_error_requests Total Chars Per Hour is derived from a character-to-token ratio based on sonnet.txt and the model's tokenizer. This metric aids in pricing decisions for an LLM serving solution. For tasks with multi-modal inputs, non-text tokens are converted to an equivalent character count using the same character-to-token ratio. \u21a9","title":"Metrics Definition"},{"location":"getting-started/metrics-definition/#metrics-definition","text":"This section puts together the standard metrics required for LLM serving performance analysis. We classify metrics to two types: single-request level metrics , representing the metrics collected from one request. And aggregated level metrics , summarizing the single-request metrics from one run (with specific traffic scenario and num concurrency). NOTE : Each single-request metric includes standard statistics: percentile , min , max , stddev , and mean . The following metrics cover input , output , and end-to-end (e2e) stages. For chat tasks, all stages are relevant for evaluation. For embedding tasks, where there is no output stage, output metrics will be set to 0. For details about output metrics collection, please check out OUTPUT_METRICS_FIELDS in metrics.py .","title":"Metrics Definition"},{"location":"getting-started/metrics-definition/#single-request-level-metrics","text":"The following metrics capture token-level performance for a single request, providing insights into server efficiency for each individual request. Glossary Meaning Calculation Formula Units TTFT Time to First Token. Initial response time when the first output token is generated. This is also known as the latency for the input (input) stage. ttft = time_at_first_token - start_time seconds End-to-End Latency End-to-End latency. This metric indicates how long it takes from submitting a query to receiving the full response, including network latencies. e2e_latency = end_time - start_time seconds TPOT Time Per Output Token. The average time between two subsequent generated tokens. TPOT = (e2e_latency - TTFT) / (num_output_tokens - 1) seconds Output Inference Speed The rate of how many tokens the model can generate per second for a single request. inference_speed = 1 / TPOT tokens/second Num of Input Tokens Number of prompt tokens. num_input_tokens = tokenizer.encode(prompt) tokens Num of Output Tokens Number of output tokens. num_output_tokens = num_completion_tokens tokens Num of Request Tokens Total number of tokens processed in one request. num_request_tokens = num_input_tokens + num_output_tokens tokens Input Throughput The overall throughput of input (input process). input_throughput = num_input_tokens / TTFT tokens/second Output Throughput The throughput of output (output generation) for a single request. output_throughput = (num_output_tokens - 1) / output_latency tokens/second","title":"Single Request Level Metrics"},{"location":"getting-started/metrics-definition/#aggregated-metrics","text":"This metrics collection summarizes the metrics relevant to a specific traffic load pattern, defined by the traffic scenario and the num of concurrency. It provides insights into server capacity and performance under pressure. Glossary Meaning Calculation Formula Units Mean Input Throughput The average throughput of how many input tokens can be processed by the model in one run with multiple concurrent requests. mean_input_throughput = sum(input_tokens_for_all_requests) / run_duration tokens/second Mean Output Throughput The average throughput of how many output tokens can be processed by the model in one run with multiple concurrent requests. mean_output_throughput = sum(output_tokens_for_all_requests) / run_duration tokens/second Total Tokens Throughput The average throughput of how many tokens can be processed by the model, including both input and output tokens. mean_total_tokens_throughput = all_requests[\"total_tokens\"][\"sum\"] / run_duration tokens/second Total Chars Per Hour 1 The average total characters can be processed by the model per hour. total_chars_per_hour = total_tokens_throughput * dataset_chars_to_token_ratio * 3600 Characters Requests Per Minute The number of requests processed by the model per minute. num_completed_requests_per_min = num_completed_requests / (end_time - start_time) * 60 Requests Error Codes to Frequency A map that shows the returned error status code to its frequency. Error Rate The rate of error requests over total requests. error_rate = num_error_requests / num_requests Num of Error Requests The number of error requests in one load. if requests.status_code != '200': num_error_requests += 1 Num of Completed Requests The number of completed requests in one load. if requests.status_code == '200': num_completed_requests += 1 Num of Requests The total number of requests processed for one load. total_requests = num_completed_requests + num_error_requests Total Chars Per Hour is derived from a character-to-token ratio based on sonnet.txt and the model's tokenizer. This metric aids in pricing decisions for an LLM serving solution. For tasks with multi-modal inputs, non-text tokens are converted to an equivalent character count using the same character-to-token ratio. \u21a9","title":"Aggregated Metrics"},{"location":"getting-started/task-definition/","text":"Task Definition \u00b6 Tasks in genai-bench define the type of benchmark you want to run, based on the input modality (e.g., text, image) and output modality (e.g., text, embeddings). Tasks are specified using the --task option in the genai-bench benchmark command. Each task follows the pattern: <input_modality>-to-<output_modality> Here are the currently supported tasks: NOTE : Task compatibility may vary depending on the API format. Task Name Description text-to-text Benchmarks generating text output from text input, such as chat or QA tasks. text-to-embeddings Benchmarks generating embeddings from text input, often for semantic search. image-text-to-text Benchmarks generating text from images and text prompts, such as visual question answering. image-to-embeddings Benchmarks generating embeddings from images, often for image similarity. How Tasks Work \u00b6 Input Modality: Defines the type of input data the task operates on, such as text or images. Output Modality: Defines the type of output the task generates, such as text or embeddings. When you specify a task, the appropriate sampler ( TextSampler or ImageSampler ) and request type ( UserChatRequest , UserEmbeddingRequest , etc.) are automatically selected based on the input and output modalities. Example Task Usage \u00b6 For a text-to-text task (e.g., generating a response to a text prompt, typical chat completions): bash genai-bench benchmark --task text-to-text ... For an image-to-text task (e.g., generating a response for an image and text interleave message): bash genai-bench benchmark --task image-to-text ... For an image-to-embeddings task (e.g., generating embeddings for similarity search): bash genai-bench benchmark --task text-to-embeddings ...","title":"Task Definition"},{"location":"getting-started/task-definition/#task-definition","text":"Tasks in genai-bench define the type of benchmark you want to run, based on the input modality (e.g., text, image) and output modality (e.g., text, embeddings). Tasks are specified using the --task option in the genai-bench benchmark command. Each task follows the pattern: <input_modality>-to-<output_modality> Here are the currently supported tasks: NOTE : Task compatibility may vary depending on the API format. Task Name Description text-to-text Benchmarks generating text output from text input, such as chat or QA tasks. text-to-embeddings Benchmarks generating embeddings from text input, often for semantic search. image-text-to-text Benchmarks generating text from images and text prompts, such as visual question answering. image-to-embeddings Benchmarks generating embeddings from images, often for image similarity.","title":"Task Definition"},{"location":"getting-started/task-definition/#how-tasks-work","text":"Input Modality: Defines the type of input data the task operates on, such as text or images. Output Modality: Defines the type of output the task generates, such as text or embeddings. When you specify a task, the appropriate sampler ( TextSampler or ImageSampler ) and request type ( UserChatRequest , UserEmbeddingRequest , etc.) are automatically selected based on the input and output modalities.","title":"How Tasks Work"},{"location":"getting-started/task-definition/#example-task-usage","text":"For a text-to-text task (e.g., generating a response to a text prompt, typical chat completions): bash genai-bench benchmark --task text-to-text ... For an image-to-text task (e.g., generating a response for an image and text interleave message): bash genai-bench benchmark --task image-to-text ... For an image-to-embeddings task (e.g., generating embeddings for similarity search): bash genai-bench benchmark --task text-to-embeddings ...","title":"Example Task Usage"},{"location":"user-guide/generate-excel-sheet/","text":"Generate an Excel sheet \u00b6 genai-bench also provides the feature to analyze a finished benchmark. You can check out genai-bench excel --help to find how you can generate an .xlsx sheet containing a summary of your benchmark experiments. Sample command \u00b6 genai-bench excel --experiment-folder <path-to-experiment-folder> --excel-name <name-of-the-sheet>","title":"Generate Excel Sheet"},{"location":"user-guide/generate-excel-sheet/#generate-an-excel-sheet","text":"genai-bench also provides the feature to analyze a finished benchmark. You can check out genai-bench excel --help to find how you can generate an .xlsx sheet containing a summary of your benchmark experiments.","title":"Generate an Excel sheet"},{"location":"user-guide/generate-excel-sheet/#sample-command","text":"genai-bench excel --experiment-folder <path-to-experiment-folder> --excel-name <name-of-the-sheet>","title":"Sample command"},{"location":"user-guide/generate-plot/","text":"Generate a 2x4 Plot \u00b6 You can check out genai-bench plot --help to find how to generate a 2x4 Plot containing: Output Inference Speed (tokens/s) vs Output Throughput of Server (tokens/s) TTFT (s) vs Output Throughput of Server (tokens/s) Mean E2E Latency (s) per Request vs RPS Error Rates by HTTP Status vs Concurrency Output Inference Speed per Request (tokens/s) vs Total Throughput (Input + Output) of Server (tokens/s) TTFT (s) vs Total Throughput (Input + Output) of Server (tokens/s) P90 E2E Latency (s) per Request vs RPS P99 E2E Latency (s) per Request vs RPS Note : TTFT plots automatically use logarithmic scale for better visualization of the wide range of values. You can override this by specifying \"y_scale\": \"linear\" in custom plot configurations. genai-bench plot --experiments-folder <path-to-experiment-folder> --group-key traffic_scenario","title":"Generate Plot"},{"location":"user-guide/generate-plot/#generate-a-2x4-plot","text":"You can check out genai-bench plot --help to find how to generate a 2x4 Plot containing: Output Inference Speed (tokens/s) vs Output Throughput of Server (tokens/s) TTFT (s) vs Output Throughput of Server (tokens/s) Mean E2E Latency (s) per Request vs RPS Error Rates by HTTP Status vs Concurrency Output Inference Speed per Request (tokens/s) vs Total Throughput (Input + Output) of Server (tokens/s) TTFT (s) vs Total Throughput (Input + Output) of Server (tokens/s) P90 E2E Latency (s) per Request vs RPS P99 E2E Latency (s) per Request vs RPS Note : TTFT plots automatically use logarithmic scale for better visualization of the wide range of values. You can override this by specifying \"y_scale\": \"linear\" in custom plot configurations. genai-bench plot --experiments-folder <path-to-experiment-folder> --group-key traffic_scenario","title":"Generate a 2x4 Plot"},{"location":"user-guide/run-benchmark-using-docker/","text":"Running Benchmark Using genai-bench Container \u00b6 Using Pre-built Docker Image \u00b6 Pull the latest docker image: docker pull ghcr.io/moirai-internal/genai-bench:v0.0.1 Building from Source \u00b6 Alternatively, you can build the image locally from the Dockerfile : docker build . -f Dockerfile -t genai-bench:dev To avoid internet disruptions and network latency, it's recommended to run the benchmarking within the same network as the target inference server. You can always choose to use --network host if you prefer. To create a bridge network in docker: docker network create benchmark-network -d bridge Then, start the inference server using the standard Docker command with the additional flag --network benchmark-network . Example: docker run -itd \\ --gpus \\\" device = 0 ,1,2,3 \\\" \\ --shm-size 10g -v /raid/models:/models \\ --ulimit nofile = 65535 :65535 --network benchmark-network \\ --name sglang-v0.4.7.post1-llama4-scout-tp4 \\ lmsysorg/sglang:v0.4.7.post1-cu124 \\ python3 -m sglang.launch_server \\ --model-path = /models/meta-llama/Llama-4-Scout-17B-16E-Instruct \\ --tp 4 \\ --port = 8080 \\ --host 0 .0.0.0 \\ --context-length = 131072 Next, start the genai-bench container with the same network flag. Example: First, create a dataset configuration file to properly specify the split: llava-config.json: { \"source\" : { \"type\" : \"huggingface\" , \"path\" : \"lmms-lab/llava-bench-in-the-wild\" , \"huggingface_kwargs\" : { \"split\" : \"train\" } }, \"prompt_column\" : \"question\" , \"image_column\" : \"image\" } Then run the benchmark with the configuration file: docker run \\ -tid \\ --shm-size 5g \\ --ulimit nofile = 65535 :65535 \\ --env HF_TOKEN = \"your_HF_TOKEN\" \\ --network benchmark-network \\ -v /mnt/data/models:/models \\ -v $( pwd ) /llava-config.json:/genai-bench/llava-config.json \\ --name llama-4-scout-benchmark \\ genai-bench:dev \\ benchmark \\ --api-backend openai \\ --api-base http://localhost:8080 \\ --api-key your_api_key \\ --api-model-name /models/meta-llama/Llama-4-Scout-17B-16E-Instruct \\ --model-tokenizer /models/meta-llama/Llama-4-Scout-17B-16E-Instruct \\ --task image-to-text \\ --max-time-per-run 10 \\ --max-requests-per-run 100 \\ --server-engine \"SGLang\" \\ --server-gpu-type \"H100\" \\ --server-version \"v0.4.7.post1\" \\ --server-gpu-count 4 \\ --traffic-scenario \"I(512,512)\" \\ --traffic-scenario \"I(2048,2048)\" \\ --num-concurrency 1 \\ --num-concurrency 2 \\ --num-concurrency 4 \\ --dataset-config /genai-bench/llava-config.json Note that genai-bench is already the entrypoint of the container, so you only need to provide the command arguments afterward. The genai-bench runtime UI should be available through: docker logs --follow <CONTAINER_ID> You can also utilize tmux for additional parallelism and session control. Monitor benchmark using volume mount \u00b6 To monitor benchmark interim results using the genai-bench container, you can leverage volume mounts along with the --experiment-base-dir option. HOST_OUTPUT_DIR = $HOME /benchmark_results CONTAINER_OUTPUT_DIR = /genai-bench/benchmark_results docker run \\ -tid \\ --shm-size 5g \\ --ulimit nofile = 65535 :65535 \\ --env HF_TOKEN = \"your_HF_TOKEN\" \\ --network benchmark-network \\ -v /mnt/data/models:/models \\ -v $HOST_OUTPUT_DIR : $CONTAINER_OUTPUT_DIR \\ -v $( pwd ) /llava-config.json:/genai-bench/llava-config.json \\ --name llama-3.2-11b-benchmark \\ genai-bench:dev \\ benchmark \\ --api-backend openai \\ --api-base http://localhost:8080 \\ --api-key your_api_key \\ --api-model-name /models/meta-llama/Llama-4-Scout-17B-16E-Instruct \\ --model-tokenizer /models/meta-llama/Llama-4-Scout-17B-16E-Instruct \\ --task image-to-text \\ --max-time-per-run 10 \\ --max-requests-per-run 100 \\ --server-engine \"SGLang\" \\ --server-gpu-type \"H100\" \\ --server-version \"v0.4.7.post1\" \\ --server-gpu-count 4 \\ --traffic-scenario \"I(512,512)\" \\ --traffic-scenario \"I(2048,2048)\" \\ --num-concurrency 1 \\ --num-concurrency 2 \\ --num-concurrency 4 \\ --dataset-config /genai-bench/llava-config.json \\ --experiment-base-dir $CONTAINER_OUTPUT_DIR","title":"Run Benchmark with Docker"},{"location":"user-guide/run-benchmark-using-docker/#running-benchmark-using-genai-bench-container","text":"","title":"Running Benchmark Using genai-bench Container"},{"location":"user-guide/run-benchmark-using-docker/#using-pre-built-docker-image","text":"Pull the latest docker image: docker pull ghcr.io/moirai-internal/genai-bench:v0.0.1","title":"Using Pre-built Docker Image"},{"location":"user-guide/run-benchmark-using-docker/#building-from-source","text":"Alternatively, you can build the image locally from the Dockerfile : docker build . -f Dockerfile -t genai-bench:dev To avoid internet disruptions and network latency, it's recommended to run the benchmarking within the same network as the target inference server. You can always choose to use --network host if you prefer. To create a bridge network in docker: docker network create benchmark-network -d bridge Then, start the inference server using the standard Docker command with the additional flag --network benchmark-network . Example: docker run -itd \\ --gpus \\\" device = 0 ,1,2,3 \\\" \\ --shm-size 10g -v /raid/models:/models \\ --ulimit nofile = 65535 :65535 --network benchmark-network \\ --name sglang-v0.4.7.post1-llama4-scout-tp4 \\ lmsysorg/sglang:v0.4.7.post1-cu124 \\ python3 -m sglang.launch_server \\ --model-path = /models/meta-llama/Llama-4-Scout-17B-16E-Instruct \\ --tp 4 \\ --port = 8080 \\ --host 0 .0.0.0 \\ --context-length = 131072 Next, start the genai-bench container with the same network flag. Example: First, create a dataset configuration file to properly specify the split: llava-config.json: { \"source\" : { \"type\" : \"huggingface\" , \"path\" : \"lmms-lab/llava-bench-in-the-wild\" , \"huggingface_kwargs\" : { \"split\" : \"train\" } }, \"prompt_column\" : \"question\" , \"image_column\" : \"image\" } Then run the benchmark with the configuration file: docker run \\ -tid \\ --shm-size 5g \\ --ulimit nofile = 65535 :65535 \\ --env HF_TOKEN = \"your_HF_TOKEN\" \\ --network benchmark-network \\ -v /mnt/data/models:/models \\ -v $( pwd ) /llava-config.json:/genai-bench/llava-config.json \\ --name llama-4-scout-benchmark \\ genai-bench:dev \\ benchmark \\ --api-backend openai \\ --api-base http://localhost:8080 \\ --api-key your_api_key \\ --api-model-name /models/meta-llama/Llama-4-Scout-17B-16E-Instruct \\ --model-tokenizer /models/meta-llama/Llama-4-Scout-17B-16E-Instruct \\ --task image-to-text \\ --max-time-per-run 10 \\ --max-requests-per-run 100 \\ --server-engine \"SGLang\" \\ --server-gpu-type \"H100\" \\ --server-version \"v0.4.7.post1\" \\ --server-gpu-count 4 \\ --traffic-scenario \"I(512,512)\" \\ --traffic-scenario \"I(2048,2048)\" \\ --num-concurrency 1 \\ --num-concurrency 2 \\ --num-concurrency 4 \\ --dataset-config /genai-bench/llava-config.json Note that genai-bench is already the entrypoint of the container, so you only need to provide the command arguments afterward. The genai-bench runtime UI should be available through: docker logs --follow <CONTAINER_ID> You can also utilize tmux for additional parallelism and session control.","title":"Building from Source"},{"location":"user-guide/run-benchmark-using-docker/#monitor-benchmark-using-volume-mount","text":"To monitor benchmark interim results using the genai-bench container, you can leverage volume mounts along with the --experiment-base-dir option. HOST_OUTPUT_DIR = $HOME /benchmark_results CONTAINER_OUTPUT_DIR = /genai-bench/benchmark_results docker run \\ -tid \\ --shm-size 5g \\ --ulimit nofile = 65535 :65535 \\ --env HF_TOKEN = \"your_HF_TOKEN\" \\ --network benchmark-network \\ -v /mnt/data/models:/models \\ -v $HOST_OUTPUT_DIR : $CONTAINER_OUTPUT_DIR \\ -v $( pwd ) /llava-config.json:/genai-bench/llava-config.json \\ --name llama-3.2-11b-benchmark \\ genai-bench:dev \\ benchmark \\ --api-backend openai \\ --api-base http://localhost:8080 \\ --api-key your_api_key \\ --api-model-name /models/meta-llama/Llama-4-Scout-17B-16E-Instruct \\ --model-tokenizer /models/meta-llama/Llama-4-Scout-17B-16E-Instruct \\ --task image-to-text \\ --max-time-per-run 10 \\ --max-requests-per-run 100 \\ --server-engine \"SGLang\" \\ --server-gpu-type \"H100\" \\ --server-version \"v0.4.7.post1\" \\ --server-gpu-count 4 \\ --traffic-scenario \"I(512,512)\" \\ --traffic-scenario \"I(2048,2048)\" \\ --num-concurrency 1 \\ --num-concurrency 2 \\ --num-concurrency 4 \\ --dataset-config /genai-bench/llava-config.json \\ --experiment-base-dir $CONTAINER_OUTPUT_DIR","title":"Monitor benchmark using volume mount"},{"location":"user-guide/run-benchmark/","text":"Run Benchmark \u00b6 Start a chat benchmark \u00b6 IMPORTANT : Use genai-bench benchmark --help to check out each command option and how to use it. For starter, you can try to type genai-bench benchmark , it will prompt the list of options you need to specify. Below is a sample command you can use to start a benchmark. The command will connect with a server running on address http://localhost:8082 , using the default traffic scenario and num concurrency, and run each combination 1 minute. # Optional. This is required when you load the tokenizer from huggingface.co with a model-id export HF_TOKEN = \"<your-key>\" # HF transformers will log a warning about torch not installed, since benchmark doesn't really need torch # and cuda, we use this env to disable the warning export TRANSFORMERS_VERBOSITY = error genai-bench benchmark --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-openai-api-key\" \\ --api-model-name \"vllm-model\" \\ --model-tokenizer \"/mnt/data/models/Meta-Llama-3.1-70B-Instruct\" \\ --task text-to-text \\ --max-time-per-run 15 \\ --max-requests-per-run 300 \\ --server-engine \"vLLM\" \\ --server-gpu-type \"H100\" \\ --server-version \"v0.6.0\" \\ --server-gpu-count 4 Start a vision based chat benchmark \u00b6 IMPORTANT : Image auto-generation pipeline is not yet implemented in this repository, hence we will be using a huggingface dataset instead. Image Datasets : Huggingface Llava Benchmark Images Below is a sample command to trigger a vision benchmark task. genai-bench benchmark \\ --api-backend openai \\ --api-key \"your-openai-api-key\" \\ --api-base \"http://localhost:8180\" \\ --api-model-name \"/models/Phi-3-vision-128k-instruct\" \\ --model-tokenizer \"/models/Phi-3-vision-128k-instruct\" \\ --task image-to-text \\ --max-time-per-run 15 \\ --max-requests-per-run 300 \\ --server-engine vLLM \\ --server-gpu-type A100-80G \\ --server-version \"v0.6.0\" \\ --server-gpu-count 4 \\ --traffic-scenario \"I(256,256)\" \\ --traffic-scenario \"I(1024,1024)\" \\ --num-concurrency 1 \\ --num-concurrency 8 \\ --dataset-config ./examples/dataset_configs/config_llava-bench-in-the-wild.json Start an embedding benchmark \u00b6 Below is a sample command to trigger an embedding benchmark task. Note: when running an embedding benchmark, it is recommended to set --num-concurrency to 1. genai-bench benchmark --api-backend openai \\ --api-base \"http://172.18.0.3:8000\" \\ --api-key \"xxx\" \\ --api-model-name \"/models/e5-mistral-7b-instruct\" \\ --model-tokenizer \"/mnt/data/models/e5-mistral-7b-instruct\" \\ --task text-to-embeddings \\ --server-engine \"SGLang\" \\ --max-time-per-run 15 \\ --max-requests-per-run 1500 \\ --traffic-scenario \"E(64)\" \\ --traffic-scenario \"E(128)\" \\ --traffic-scenario \"E(512)\" \\ --traffic-scenario \"E(1024)\" \\ --server-gpu-type \"H100\" \\ --server-version \"v0.4.2\" \\ --server-gpu-count 1 Start a rerank benchmark against OCI Cohere \u00b6 Below is a sample command to trigger a benchmark against cohere chat API. genai-bench benchmark --api-backend oci-cohere \\ --config-file /home/ubuntu/.oci/config \\ --api-base \"https://ppe.inference.generativeai.us-chicago-1.oci.oraclecloud.com\" \\ --api-model-name \"rerank-v3.5\" \\ --model-tokenizer \"Cohere/rerank-v3.5\" \\ --server-engine \"cohere-TensorRT\" \\ --task text-to-rerank \\ --num-concurrency 1 \\ --server-gpu-type A100-80G \\ --server-version \"1.7.0\" \\ --server-gpu-count 4 \\ --max-time-per-run 15 \\ --max-requests-per-run 3 \\ --additional-request-params '{\"compartmentId\": \"COMPARTMENTID\", \"endpointId\": \"ENDPOINTID\", \"servingType\": \"DEDICATED\"}' \\ --num-workers 4 Start a benchmark against OCI Cohere \u00b6 Below is a sample command to trigger a benchmark against cohere chat API. genai-bench benchmark --api-backend oci-cohere \\ --config-file /home/ubuntu/.oci/config \\ --api-base \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\" \\ --api-model-name \"c4ai-command-r-08-2024\" \\ --model-tokenizer \"/home/ubuntu/c4ai-command-r-08-2024\" \\ --server-engine \"vLLM\" \\ --task text-to-text \\ --num-concurrency 1 \\ --server-gpu-type A100-80G \\ --server-version \"command_r_082024_v1_7\" \\ --server-gpu-count 4 \\ --max-time-per-run 15 \\ --max-requests-per-run 300 \\ --additional-request-params '{\"compartmentId\": \"COMPARTMENTID\", \"endpointId\": \"ENDPOINTID\", \"servingType\": \"DEDICATED\"}' \\ --num-workers 4 Monitor a benchmark \u00b6 IMPORTANT : logs in genai-bench are all useful. Please keep an eye on WARNING logs when you finish one benchmark. Specify --traffic-scenario and --num-concurrency \u00b6 IMPORTANT : Please use genai-bench benchmark --help to check out the latest default value of --num-concurrency and --traffic-scenario . Both options are defined as multi-value options in click. Meaning you can pass this command multiple times. If you want to define your own --num-concurrency or --traffic-scenario , you can use genai-bench benchmark \\ --api-backend openai \\ --task text-to-text \\ --max-time-per-run 10 \\ --max-requests-per-run 300 \\ --num-concurrency 1 --num-concurrency 2 --num-concurrency 4 \\ --num-concurrency 8 --num-concurrency 16 --num-concurrency 32 \\ --traffic-scenario \"N(480,240)/(300,150)\" --traffic-scenario \"D(100,100)\" Notes on specific options \u00b6 To manage each run or iteration in an experiment, genai-bench uses two parameters to control the exit logic. You can find more details in the manage_run_time function located in utils.py . Combination of --max-time-per-run and --max-requests-per-run should save overall time of one benchmark. For light traffic scenarios, such as D(7800,200) or lighter, we recommend the following settings: --max-time-per-run 10 \\ --max-requests-per-run 300 \\ For heavier traffic scenarios, like D(16000,200) or D(128000,200) , use the following configuration: --max-time-per-run 30 \\ --max-requests-per-run 100 \\ --traffic-scenario \"D(16000,200)\" \\ --traffic-scenario \"D(32000,200)\" \\ --traffic-scenario \"D(128000,200)\" \\ --num-concurrency 1 \\ --num-concurrency 2 \\ --num-concurrency 4 \\ --num-concurrency 8 \\ --num-concurrency 16 \\ --num-concurrency 32 \\ Distributed Benchmark \u00b6 If you see the message below in the genai-bench logs, it indicates that a single process is insufficient to generate the desired load. CPU usage above 90%! This may constrain your throughput and may even give inconsistent response time measurements! To address this, you can increase the number of worker processes using the --num-workers option. For example, to spin up 4 worker processes, use: --num-workers 4 --master-port 5577 This distributes the load across multiple processes on a single machine, improving performance and ensuring your benchmark runs smoothly. Notes on Usage \u00b6 This feature is experimental, so monitor the system's behavior when enabling multiple workers. Recommended Limit: Do not set the number of workers to more than 16, as excessive worker processes can lead to resource contention and diminished performance. Ensure your system has sufficient CPU and memory resources to support the desired number of workers. Adjust the number of workers based on your target load and system capacity to achieve optimal results. Using Dataset Configurations \u00b6 Genai-bench supports flexible dataset configurations through two approaches: Simple CLI Usage (for basic datasets) \u00b6 # Local CSV file --dataset-path /path/to/data.csv \\ --dataset-prompt-column \"prompt\" # HuggingFace dataset with simple options --dataset-path squad \\ --dataset-prompt-column \"question\" # Local text file (default) --dataset-path /path/to/prompts.txt Advanced Configuration Files (for complex setups) \u00b6 For advanced HuggingFace configurations, create a JSON config file: Important Note for HuggingFace Datasets: When using HuggingFace datasets, you should always check if you need a split , subset parameter to avoid errors. If you don't specify, HuggingFace's load_dataset may return a DatasetDict object instead of a Dataset , which will cause the benchmark to fail. config.json: { \"source\" : { \"type\" : \"huggingface\" , \"path\" : \"ccdv/govreport-summarization\" , \"huggingface_kwargs\" : { \"split\" : \"train\" , \"revision\" : \"main\" , \"streaming\" : true } }, \"prompt_column\" : \"report\" } Vision dataset config: { \"source\" : { \"type\" : \"huggingface\" , \"path\" : \"BLINK-Benchmark/BLINK\" , \"huggingface_kwargs\" : { \"split\" : \"test\" , \"name\" : \"Jigsaw\" } }, \"prompt_column\" : \"question\" , \"image_column\" : \"image_1\" } Example for the llava-bench-in-the-wild dataset: { \"source\" : { \"type\" : \"huggingface\" , \"path\" : \"lmms-lab/llava-bench-in-the-wild\" , \"huggingface_kwargs\" : { \"split\" : \"train\" } }, \"prompt_column\" : \"question\" , \"image_column\" : \"image\" } Then use: --dataset-config config.json Benefits of config files: - Access to ALL HuggingFace load_dataset parameters - Reusable and version-controllable - Support for complex configurations - Future-proof (no CLI updates needed for new HuggingFace features)","title":"Run Benchmark"},{"location":"user-guide/run-benchmark/#run-benchmark","text":"","title":"Run Benchmark"},{"location":"user-guide/run-benchmark/#start-a-chat-benchmark","text":"IMPORTANT : Use genai-bench benchmark --help to check out each command option and how to use it. For starter, you can try to type genai-bench benchmark , it will prompt the list of options you need to specify. Below is a sample command you can use to start a benchmark. The command will connect with a server running on address http://localhost:8082 , using the default traffic scenario and num concurrency, and run each combination 1 minute. # Optional. This is required when you load the tokenizer from huggingface.co with a model-id export HF_TOKEN = \"<your-key>\" # HF transformers will log a warning about torch not installed, since benchmark doesn't really need torch # and cuda, we use this env to disable the warning export TRANSFORMERS_VERBOSITY = error genai-bench benchmark --api-backend openai \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-openai-api-key\" \\ --api-model-name \"vllm-model\" \\ --model-tokenizer \"/mnt/data/models/Meta-Llama-3.1-70B-Instruct\" \\ --task text-to-text \\ --max-time-per-run 15 \\ --max-requests-per-run 300 \\ --server-engine \"vLLM\" \\ --server-gpu-type \"H100\" \\ --server-version \"v0.6.0\" \\ --server-gpu-count 4","title":"Start a chat benchmark"},{"location":"user-guide/run-benchmark/#start-a-vision-based-chat-benchmark","text":"IMPORTANT : Image auto-generation pipeline is not yet implemented in this repository, hence we will be using a huggingface dataset instead. Image Datasets : Huggingface Llava Benchmark Images Below is a sample command to trigger a vision benchmark task. genai-bench benchmark \\ --api-backend openai \\ --api-key \"your-openai-api-key\" \\ --api-base \"http://localhost:8180\" \\ --api-model-name \"/models/Phi-3-vision-128k-instruct\" \\ --model-tokenizer \"/models/Phi-3-vision-128k-instruct\" \\ --task image-to-text \\ --max-time-per-run 15 \\ --max-requests-per-run 300 \\ --server-engine vLLM \\ --server-gpu-type A100-80G \\ --server-version \"v0.6.0\" \\ --server-gpu-count 4 \\ --traffic-scenario \"I(256,256)\" \\ --traffic-scenario \"I(1024,1024)\" \\ --num-concurrency 1 \\ --num-concurrency 8 \\ --dataset-config ./examples/dataset_configs/config_llava-bench-in-the-wild.json","title":"Start a vision based chat benchmark"},{"location":"user-guide/run-benchmark/#start-an-embedding-benchmark","text":"Below is a sample command to trigger an embedding benchmark task. Note: when running an embedding benchmark, it is recommended to set --num-concurrency to 1. genai-bench benchmark --api-backend openai \\ --api-base \"http://172.18.0.3:8000\" \\ --api-key \"xxx\" \\ --api-model-name \"/models/e5-mistral-7b-instruct\" \\ --model-tokenizer \"/mnt/data/models/e5-mistral-7b-instruct\" \\ --task text-to-embeddings \\ --server-engine \"SGLang\" \\ --max-time-per-run 15 \\ --max-requests-per-run 1500 \\ --traffic-scenario \"E(64)\" \\ --traffic-scenario \"E(128)\" \\ --traffic-scenario \"E(512)\" \\ --traffic-scenario \"E(1024)\" \\ --server-gpu-type \"H100\" \\ --server-version \"v0.4.2\" \\ --server-gpu-count 1","title":"Start an embedding benchmark"},{"location":"user-guide/run-benchmark/#start-a-rerank-benchmark-against-oci-cohere","text":"Below is a sample command to trigger a benchmark against cohere chat API. genai-bench benchmark --api-backend oci-cohere \\ --config-file /home/ubuntu/.oci/config \\ --api-base \"https://ppe.inference.generativeai.us-chicago-1.oci.oraclecloud.com\" \\ --api-model-name \"rerank-v3.5\" \\ --model-tokenizer \"Cohere/rerank-v3.5\" \\ --server-engine \"cohere-TensorRT\" \\ --task text-to-rerank \\ --num-concurrency 1 \\ --server-gpu-type A100-80G \\ --server-version \"1.7.0\" \\ --server-gpu-count 4 \\ --max-time-per-run 15 \\ --max-requests-per-run 3 \\ --additional-request-params '{\"compartmentId\": \"COMPARTMENTID\", \"endpointId\": \"ENDPOINTID\", \"servingType\": \"DEDICATED\"}' \\ --num-workers 4","title":"Start a rerank benchmark against OCI Cohere"},{"location":"user-guide/run-benchmark/#start-a-benchmark-against-oci-cohere","text":"Below is a sample command to trigger a benchmark against cohere chat API. genai-bench benchmark --api-backend oci-cohere \\ --config-file /home/ubuntu/.oci/config \\ --api-base \"https://inference.generativeai.us-chicago-1.oci.oraclecloud.com\" \\ --api-model-name \"c4ai-command-r-08-2024\" \\ --model-tokenizer \"/home/ubuntu/c4ai-command-r-08-2024\" \\ --server-engine \"vLLM\" \\ --task text-to-text \\ --num-concurrency 1 \\ --server-gpu-type A100-80G \\ --server-version \"command_r_082024_v1_7\" \\ --server-gpu-count 4 \\ --max-time-per-run 15 \\ --max-requests-per-run 300 \\ --additional-request-params '{\"compartmentId\": \"COMPARTMENTID\", \"endpointId\": \"ENDPOINTID\", \"servingType\": \"DEDICATED\"}' \\ --num-workers 4","title":"Start a benchmark against OCI Cohere"},{"location":"user-guide/run-benchmark/#monitor-a-benchmark","text":"IMPORTANT : logs in genai-bench are all useful. Please keep an eye on WARNING logs when you finish one benchmark.","title":"Monitor a benchmark"},{"location":"user-guide/run-benchmark/#specify-traffic-scenario-and-num-concurrency","text":"IMPORTANT : Please use genai-bench benchmark --help to check out the latest default value of --num-concurrency and --traffic-scenario . Both options are defined as multi-value options in click. Meaning you can pass this command multiple times. If you want to define your own --num-concurrency or --traffic-scenario , you can use genai-bench benchmark \\ --api-backend openai \\ --task text-to-text \\ --max-time-per-run 10 \\ --max-requests-per-run 300 \\ --num-concurrency 1 --num-concurrency 2 --num-concurrency 4 \\ --num-concurrency 8 --num-concurrency 16 --num-concurrency 32 \\ --traffic-scenario \"N(480,240)/(300,150)\" --traffic-scenario \"D(100,100)\"","title":"Specify --traffic-scenario and --num-concurrency"},{"location":"user-guide/run-benchmark/#notes-on-specific-options","text":"To manage each run or iteration in an experiment, genai-bench uses two parameters to control the exit logic. You can find more details in the manage_run_time function located in utils.py . Combination of --max-time-per-run and --max-requests-per-run should save overall time of one benchmark. For light traffic scenarios, such as D(7800,200) or lighter, we recommend the following settings: --max-time-per-run 10 \\ --max-requests-per-run 300 \\ For heavier traffic scenarios, like D(16000,200) or D(128000,200) , use the following configuration: --max-time-per-run 30 \\ --max-requests-per-run 100 \\ --traffic-scenario \"D(16000,200)\" \\ --traffic-scenario \"D(32000,200)\" \\ --traffic-scenario \"D(128000,200)\" \\ --num-concurrency 1 \\ --num-concurrency 2 \\ --num-concurrency 4 \\ --num-concurrency 8 \\ --num-concurrency 16 \\ --num-concurrency 32 \\","title":"Notes on specific options"},{"location":"user-guide/run-benchmark/#distributed-benchmark","text":"If you see the message below in the genai-bench logs, it indicates that a single process is insufficient to generate the desired load. CPU usage above 90%! This may constrain your throughput and may even give inconsistent response time measurements! To address this, you can increase the number of worker processes using the --num-workers option. For example, to spin up 4 worker processes, use: --num-workers 4 --master-port 5577 This distributes the load across multiple processes on a single machine, improving performance and ensuring your benchmark runs smoothly.","title":"Distributed Benchmark"},{"location":"user-guide/run-benchmark/#notes-on-usage","text":"This feature is experimental, so monitor the system's behavior when enabling multiple workers. Recommended Limit: Do not set the number of workers to more than 16, as excessive worker processes can lead to resource contention and diminished performance. Ensure your system has sufficient CPU and memory resources to support the desired number of workers. Adjust the number of workers based on your target load and system capacity to achieve optimal results.","title":"Notes on Usage"},{"location":"user-guide/run-benchmark/#using-dataset-configurations","text":"Genai-bench supports flexible dataset configurations through two approaches:","title":"Using Dataset Configurations"},{"location":"user-guide/run-benchmark/#simple-cli-usage-for-basic-datasets","text":"# Local CSV file --dataset-path /path/to/data.csv \\ --dataset-prompt-column \"prompt\" # HuggingFace dataset with simple options --dataset-path squad \\ --dataset-prompt-column \"question\" # Local text file (default) --dataset-path /path/to/prompts.txt","title":"Simple CLI Usage (for basic datasets)"},{"location":"user-guide/run-benchmark/#advanced-configuration-files-for-complex-setups","text":"For advanced HuggingFace configurations, create a JSON config file: Important Note for HuggingFace Datasets: When using HuggingFace datasets, you should always check if you need a split , subset parameter to avoid errors. If you don't specify, HuggingFace's load_dataset may return a DatasetDict object instead of a Dataset , which will cause the benchmark to fail. config.json: { \"source\" : { \"type\" : \"huggingface\" , \"path\" : \"ccdv/govreport-summarization\" , \"huggingface_kwargs\" : { \"split\" : \"train\" , \"revision\" : \"main\" , \"streaming\" : true } }, \"prompt_column\" : \"report\" } Vision dataset config: { \"source\" : { \"type\" : \"huggingface\" , \"path\" : \"BLINK-Benchmark/BLINK\" , \"huggingface_kwargs\" : { \"split\" : \"test\" , \"name\" : \"Jigsaw\" } }, \"prompt_column\" : \"question\" , \"image_column\" : \"image_1\" } Example for the llava-bench-in-the-wild dataset: { \"source\" : { \"type\" : \"huggingface\" , \"path\" : \"lmms-lab/llava-bench-in-the-wild\" , \"huggingface_kwargs\" : { \"split\" : \"train\" } }, \"prompt_column\" : \"question\" , \"image_column\" : \"image\" } Then use: --dataset-config config.json Benefits of config files: - Access to ALL HuggingFace load_dataset parameters - Reusable and version-controllable - Support for complex configurations - Future-proof (no CLI updates needed for new HuggingFace features)","title":"Advanced Configuration Files (for complex setups)"},{"location":"user-guide/upload-benchmark-result/","text":"Uploading Benchmark Results to OCI Object Storage \u00b6 GenAI Bench supports uploading benchmark results directly to OCI Object Storage. This feature is useful for: - Storing benchmark results in a centralized location - Sharing results with team members - Maintaining a historical record of benchmarks - Analyzing results across different runs To enable result uploading, use the following options with the benchmark command: genai-bench benchmark \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-openai-api-key\" \\ --api-model-name \"vllm-model\" \\ --model-tokenizer \"/mnt/data/models/Meta-Llama-3.1-70B-Instruct\" \\ --task text-to-text \\ --max-time-per-run 15 \\ --max-requests-per-run 300 \\ --server-engine \"vLLM\" \\ --server-gpu-type \"H100\" \\ --server-version \"v0.6.0\" \\ --server-gpu-count 4 \\ --upload-results \\ --bucket \"your-bucket-name\" By default, GenAI Bench uses OCI User Principal for authentication and authorization. The default namespace is the current tenancy, and the default region is the current region in which the client is positioned. You can override the namespace and region using the --namespace and --region options, respectively. Alternatively, you can change the authentication and authorization mechanism using the --auth option. The default object prefix is empty, but you can specify a prefix using the --prefix option.","title":"Upload Benchmark Results"},{"location":"user-guide/upload-benchmark-result/#uploading-benchmark-results-to-oci-object-storage","text":"GenAI Bench supports uploading benchmark results directly to OCI Object Storage. This feature is useful for: - Storing benchmark results in a centralized location - Sharing results with team members - Maintaining a historical record of benchmarks - Analyzing results across different runs To enable result uploading, use the following options with the benchmark command: genai-bench benchmark \\ --api-base \"http://localhost:8082\" \\ --api-key \"your-openai-api-key\" \\ --api-model-name \"vllm-model\" \\ --model-tokenizer \"/mnt/data/models/Meta-Llama-3.1-70B-Instruct\" \\ --task text-to-text \\ --max-time-per-run 15 \\ --max-requests-per-run 300 \\ --server-engine \"vLLM\" \\ --server-gpu-type \"H100\" \\ --server-version \"v0.6.0\" \\ --server-gpu-count 4 \\ --upload-results \\ --bucket \"your-bucket-name\" By default, GenAI Bench uses OCI User Principal for authentication and authorization. The default namespace is the current tenancy, and the default region is the current region in which the client is positioned. You can override the namespace and region using the --namespace and --region options, respectively. Alternatively, you can change the authentication and authorization mechanism using the --auth option. The default object prefix is empty, but you can specify a prefix using the --prefix option.","title":"Uploading Benchmark Results to OCI Object Storage"}]}